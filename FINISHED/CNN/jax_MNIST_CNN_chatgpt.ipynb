{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 21:52:49.831918: W external/xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 352.46MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-11-25 21:52:50.217083: W external/xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 348.03MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Accuracy: 0.1795\n",
      "Epoch 2, Test Accuracy: 0.2028\n",
      "Epoch 3, Test Accuracy: 0.2521\n",
      "Epoch 4, Test Accuracy: 0.3597\n",
      "Epoch 5, Test Accuracy: 0.5163\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit\n",
    "from jax.nn import relu, log_softmax\n",
    "from jax.nn.initializers import glorot_uniform\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "train_data = MNIST(root=\"./\", train=True, download=True, transform=ToTensor())\n",
    "test_data = MNIST(root=\"./\", train=False, download=True, transform=ToTensor())\n",
    "\n",
    "x_train, y_train = zip(*(list(train_data)[:40]))\n",
    "x_test, y_test = zip(*test_data)\n",
    "\n",
    "x_train, y_train = jnp.array(np.stack(x_train)[:, 0, :, :]), jnp.array(y_train)\n",
    "x_test, y_test = jnp.array(np.stack(x_test)[:, 0, :, :]), jnp.array(y_test)\n",
    "\n",
    "classes = len(set(y_train.tolist()))\n",
    "y_train = jax.nn.one_hot(y_train, classes)\n",
    "y_test = jax.nn.one_hot(y_test, classes)\n",
    "\n",
    "# Helper to compute output size after conv and pooling layers\n",
    "def compute_output_shape(input_shape, kernel_size, stride, padding):\n",
    "    return (input_shape - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "# Define CNN model\n",
    "def init_cnn_params(rng, input_shape):\n",
    "    k1, k2, k3 = random.split(rng, 3)\n",
    "\n",
    "    conv1_out_channels = 4\n",
    "    conv2_out_channels = 2\n",
    "\n",
    "    # Calculate spatial size after Conv1 and pooling\n",
    "    conv1_out_size = compute_output_shape(28, 3, 1, 1)  # SAME padding\n",
    "    pool1_out_size = compute_output_shape(conv1_out_size, 2, 2, 0)  # VALID pooling\n",
    "\n",
    "    # Calculate spatial size after Conv2 and pooling\n",
    "    conv2_out_size = compute_output_shape(pool1_out_size, 3, 1, 1)  # SAME padding\n",
    "    pool2_out_size = compute_output_shape(conv2_out_size, 2, 2, 0)  # VALID pooling\n",
    "\n",
    "    # Final flattened size\n",
    "    flattened_size = pool2_out_size * pool2_out_size * conv2_out_channels\n",
    "\n",
    "    params = {\n",
    "        \"conv1\": glorot_uniform()(k1, (conv1_out_channels, 1, 3, 3)),  # 4 filters, 3x3 kernel\n",
    "        \"conv2\": glorot_uniform()(k2, (conv2_out_channels, conv1_out_channels, 3, 3)),  # 2 filters, 3x3 kernel\n",
    "        \"fc\": glorot_uniform()(k3, (flattened_size, classes)),  # Fully connected\n",
    "    }\n",
    "    return params, pool2_out_size\n",
    "\n",
    "def cnn_forward(params, x):\n",
    "    # Conv Layer 1\n",
    "    x = jax.lax.conv_general_dilated(\n",
    "        x, params[\"conv1\"], window_strides=(1, 1), padding=\"SAME\", dimension_numbers=(\"NCHW\", \"OIHW\", \"NCHW\")\n",
    "    )\n",
    "    x = relu(x)\n",
    "    x = jax.lax.reduce_window(\n",
    "        x, \n",
    "        -jnp.inf, \n",
    "        jax.lax.max, \n",
    "        window_dimensions=(1, 1, 2, 2),  # Pooling over spatial dimensions only\n",
    "        window_strides=(1, 1, 2, 2), \n",
    "        padding=\"VALID\"\n",
    "    )\n",
    "    \n",
    "    # Conv Layer 2\n",
    "    x = jax.lax.conv_general_dilated(\n",
    "        x, params[\"conv2\"], window_strides=(1, 1), padding=\"SAME\", dimension_numbers=(\"NCHW\", \"OIHW\", \"NCHW\")\n",
    "    )\n",
    "    x = relu(x)\n",
    "    x = jax.lax.reduce_window(\n",
    "        x, \n",
    "        -jnp.inf, \n",
    "        jax.lax.max, \n",
    "        window_dimensions=(1, 1, 2, 2),  # Pooling over spatial dimensions only\n",
    "        window_strides=(1, 1, 2, 2), \n",
    "        padding=\"VALID\"\n",
    "    )\n",
    "    \n",
    "    # Flatten\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    logits = jnp.dot(x, params[\"fc\"])\n",
    "    return log_softmax(logits)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = cnn_forward(params, x)\n",
    "    return -jnp.mean(jnp.sum(logits * y, axis=-1))\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(params, x, y):\n",
    "    preds = jnp.argmax(cnn_forward(params, x), axis=-1)\n",
    "    labels = jnp.argmax(y, axis=-1)\n",
    "    return jnp.mean(preds == labels)\n",
    "\n",
    "# Training function\n",
    "@jit\n",
    "def train_step(params, x, y, opt_state):\n",
    "    grads = grad(cross_entropy_loss)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n",
    "\n",
    "# Shuffle function\n",
    "def shuffle_data(x, y, rng):\n",
    "    indices = jax.random.permutation(rng, len(x))\n",
    "    return x[indices], y[indices]\n",
    "\n",
    "# Initialize parameters and optimizer\n",
    "rng = random.PRNGKey(0)\n",
    "input_shape = (batch_size, 1, 28, 28)\n",
    "params, pool2_out_size = init_cnn_params(rng, input_shape)\n",
    "\n",
    "import optax\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    rng, subkey = random.split(rng)\n",
    "    x_train, y_train = shuffle_data(x_train, y_train, subkey)\n",
    "    \n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train[i:i+batch_size][:, None, :, :]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        params, opt_state = train_step(params, x_batch, y_batch, opt_state)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_acc = accuracy(params, x_test[:, None, :, :], y_test)\n",
    "    print(f\"Epoch {epoch + 1}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
