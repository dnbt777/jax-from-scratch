{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from functools import partial\n",
    "import random as rand\n",
    "import time\n",
    "\n",
    "gpu_device = jax.devices('gpu')[0]\n",
    "cpu_device = jax.devices('cpu')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_log = print\n",
    "debug_log = lambda *x: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "sequence_length = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment:\n",
    "get my posts\n",
    "input: nothing\n",
    "output: a post\n",
    "\n",
    "experiment:\n",
    "video generator\n",
    "input: blank transparent canvas\n",
    "output: the next frame of the video\n",
    "do a several convolution skip connections on each step (adds changes to each frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replies:\n",
      "@VictorTaelin ive found sonnet 3.5v2 to be surprisingly good at coding. upgraded my tools from v1 to v2 and all of a sudden i have to reprompt it like 1/3rd the time\n",
      "@dgant &lt;script src=\"gifeditor.mp3\" type=\"application/json\"&gt;\n",
      "@bozo10n ðŸ’ª\n",
      "@sunsettler experimentation games are the best\n",
      "\n",
      "Posts:\n",
      "compression always feels so satisfying https://t.co/mM5acJxydL\n",
      "it only takes one line of code to make a gifboard btw https://t.co/hFlPyNTvRm\n",
      "RT @calbch: @kuberdenis entrepreneurship is the ultimative vehicle for personal development\n",
      "just two idiots playing a game of chess https://t.co/USjWySv3W9\n"
     ]
    }
   ],
   "source": [
    "# process tweet data\n",
    "import json\n",
    "\n",
    "# Load the tweets.js file\n",
    "with open('./tweets.js', 'r', encoding='utf-8') as file:\n",
    "    # Skip the JavaScript assignment and load only the JSON part\n",
    "    content = file.read()\n",
    "    json_data = content.split('=', 1)[1].strip()  # Extract the JSON part after `=`\n",
    "    json_data = json_data.rstrip(';')  # Remove trailing semicolon if present\n",
    "    tweets_data = json.loads(json_data)\n",
    "\n",
    "# Initialize lists for replies and posts\n",
    "replies = []\n",
    "posts = []\n",
    "\n",
    "# Process each tweet in the dataset\n",
    "for tweet_obj in tweets_data:\n",
    "    tweet = tweet_obj[\"tweet\"]\n",
    "    \n",
    "    if \"in_reply_to_status_id_str\" in tweet and tweet[\"in_reply_to_status_id_str\"]:\n",
    "        # It's a reply\n",
    "        replies.append({\n",
    "            \"id\": tweet[\"id_str\"],\n",
    "            \"text\": tweet[\"full_text\"],\n",
    "            \"in_reply_to\": tweet[\"in_reply_to_status_id_str\"],\n",
    "            \"user\": tweet.get(\"in_reply_to_screen_name\", None),\n",
    "            \"created_at\": tweet[\"created_at\"]\n",
    "        })\n",
    "    else:\n",
    "        # It's a standalone post\n",
    "        posts.append({\n",
    "            \"id\": tweet[\"id_str\"],\n",
    "            \"text\": tweet[\"full_text\"],\n",
    "            \"created_at\": tweet[\"created_at\"]\n",
    "        })\n",
    "\n",
    "# Output the results\n",
    "print(\"Replies:\")\n",
    "for reply in replies[:4]:\n",
    "    print(reply[\"text\"])\n",
    "\n",
    "print(\"\\nPosts:\")\n",
    "for post in posts[:4]:\n",
    "    print(post[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathered initial posts\n",
      "vocab size: 1978\n",
      "converted posts to tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 208.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 4274 sequences of len 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set up and process data\n",
    "post_limit = 100\n",
    "\n",
    "stop_char = \"ðŸ›‘\"\n",
    "all_posts = [\n",
    "  \"reply: \" + reply[\"text\"] + stop_char for reply in replies[:post_limit]\n",
    "]\n",
    "all_posts.extend(\n",
    "  [\n",
    "    \"post: \" + post[\"text\"] + stop_char for post in posts[:post_limit]\n",
    "  ]\n",
    ")\n",
    "rand.shuffle(all_posts)\n",
    "print('gathered initial posts')\n",
    "\n",
    "token_type = \"word\"\n",
    "if token_type == \"char\":\n",
    "  chars = \"\".join(all_posts)\n",
    "  vocab = sorted(list(set(chars)))\n",
    "  vocab_size = len(vocab)\n",
    "  print(\"vocab size:\", vocab_size)\n",
    "\n",
    "  translations = {\n",
    "    \"encode\" : dict([(c, t) for c, t in zip(vocab, range(len(vocab)))]),\n",
    "    \"decode\" : dict([(t, c) for c, t in zip(vocab, range(len(vocab)))])\n",
    "  }\n",
    "\n",
    "  token = lambda c: translations[\"encode\"][c] # char to token\n",
    "  char = lambda t: translations[\"decode\"][int(t)] # token to char\n",
    "  encode = lambda cs: jnp.array([token(c) for c in cs])\n",
    "  decode = lambda ts: \"\".join([char(t) for t in ts])\n",
    "\n",
    "elif token_type == \"word\":\n",
    "  text = \" \".join(all_posts)\n",
    "  words = text.split(' ')\n",
    "  vocab = sorted(list(set(words)))\n",
    "  vocab_size = len(vocab)\n",
    "  print(\"vocab size:\", vocab_size)\n",
    "\n",
    "  translations = {\n",
    "    \"encode\" : dict([(w, t) for w, t in zip(vocab, range(len(vocab)))]),\n",
    "    \"decode\" : dict([(t, w) for w, t in zip(vocab, range(len(vocab)))])\n",
    "  }\n",
    "\n",
    "  token = lambda w: translations[\"encode\"][w] # word to token\n",
    "  word = lambda t: translations[\"decode\"][int(t)] # token => word\n",
    "  encode = lambda cs: jnp.array([token(w) for w in cs.split(\" \")])\n",
    "  decode = lambda ts: \" \".join([word(t) for t in ts])\n",
    "\n",
    "\n",
    "post_token_sets = [encode(post) for post in all_posts]\n",
    "print('converted posts to tokens')\n",
    "\n",
    "from tqdm import tqdm\n",
    "sequence_token_sets = []\n",
    "for post in tqdm(post_token_sets):\n",
    "  for i in range(max(0, len(post) - sequence_length + 1)):\n",
    "    sequence_token_sets.append(post[i:i+sequence_length])\n",
    "\n",
    "\n",
    "print(f\"loaded {len(sequence_token_sets)} sequences of len {sequence_length}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train functions\n",
    "# init rnn params\n",
    "# inputs ()\n",
    "def init_rnn_params(key, input_shape, hidden_shape, conveyor_shape, output_shape):\n",
    "  keys = random.split(key, 4*sequence_length)\n",
    "  hx_concat_length = hidden_shape[0] + input_shape[0] # todo incorporate batching\n",
    "  conveyor_shape = conveyor_shape[0]\n",
    "  output_size = output_shape[0]\n",
    "  rnn_params = [\n",
    "    # xW means shape of W is (input, output)\n",
    "    # hW and xW should output the shape of h.\n",
    "    {\n",
    "      \"Whxc\" : random.normal(keys[3*n + 0], shape=(hx_concat_length, output_size)) * jnp.sqrt(2 / hx_concat_length),\n",
    "      \"Bhxc\" : jnp.zeros(shape=(output_size,)),\n",
    "      \"Whxi\" : random.normal(keys[3*n + 1], shape=(hx_concat_length, output_size)) * jnp.sqrt(2 / hx_concat_length),\n",
    "      \"Bhxi\" : jnp.zeros(shape=(output_size,)),\n",
    "      \"Whxcupd\" : random.normal(keys[3*n + 2], shape=(hx_concat_length, output_size)) * jnp.sqrt(2 / hx_concat_length),\n",
    "      \"Bhxcupd\" : jnp.zeros(shape=(output_size,)),\n",
    "      \"Whxo\" : random.normal(keys[3*n + 3], shape=(hx_concat_length, output_size)) * jnp.sqrt(2 / hx_concat_length),\n",
    "      \"Bhxo\" : jnp.zeros(shape=(output_size,)),\n",
    "    }\n",
    "    for n in range(sequence_length + 1) # +1 for the empty sequence at the start\n",
    "  ]\n",
    "  return rnn_params\n",
    "\n",
    "@partial(jax.jit, device=gpu_device)\n",
    "def step_start(rnn_params, x):\n",
    "  # todo incorporate batching\n",
    "  hx_concat_length, conveyor_shape = rnn_params[0][\"Whxc\"].shape\n",
    "  # x is B, C (T == 1 so its not in here)\n",
    "  batch_size = x.shape[0]\n",
    "  input_shape = x.shape[1]\n",
    "  h_shape = hx_concat_length - input_shape\n",
    "  h = jnp.zeros((batch_size, h_shape)) # batch size 1 for now\n",
    "  c = jnp.zeros((batch_size, conveyor_shape))\n",
    "  y, c, h = step(rnn_params, c, h, x, n=0)\n",
    "  debug_log('y: ', y.shape, f\"should be {('Batch', 'C(logits=vocab_size)')}\")\n",
    "  return y, c, h\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"n\"], device=gpu_device) # n is static (i think calculated at jit comptime each time a new val is passed)\n",
    "def step(rnn_params, c, h, x, n):\n",
    "  # dim=1 so that for batching when its (n, h+x) it concats h+x\n",
    "  hxconcat = jax.lax.concatenate([h, x], dimension=1) # (batch, h+x)\n",
    "\n",
    "  debug_log('x: ', x.shape, f\"should be {('Batch', 'C(one hot encodings)')}\")\n",
    "\n",
    "  # input gate\n",
    "  # (B, h+x) @ (h+x,c) => (B, c) (B = batch dimension, right now its just 1)\n",
    "  wxc = rnn_params[n][\"Whxc\"]\n",
    "  xc = hxconcat @ wxc\n",
    "  b = rnn_params[n][\"Bhxc\"]\n",
    "  f = jax.nn.sigmoid(xc + b)\n",
    "  c = c * f # (B, c) * (B, c) => (B, c)\n",
    "\n",
    "  # update gate\n",
    "  # (B, h+x) @ (h+x, c) => (B, c)\n",
    "  i = jax.nn.sigmoid(hxconcat @ rnn_params[n][\"Whxi\"] + rnn_params[n][\"Bhxi\"])\n",
    "  # (B, h+x) @ (h+x, c) => (B, c)\n",
    "  c_upd = jax.nn.tanh(hxconcat @ rnn_params[n][\"Whxcupd\"] + rnn_params[n][\"Bhxcupd\"])\n",
    "  c = c + i*c_upd # (B, c) + (B, c) * (B, c) => (B, c)\n",
    "\n",
    "  # output gate\n",
    "  o = jax.nn.sigmoid(hxconcat @ rnn_params[n][\"Whxo\"] + rnn_params[n][\"Bhxo\"])\n",
    "  h = o * jax.nn.tanh(c) # delete gate\n",
    "\n",
    "  y = o\n",
    "  return y, c, h\n",
    "\n",
    "@partial(jax.jit, device=gpu_device)\n",
    "def forward(rnn_params, xbow):\n",
    "  ys = []\n",
    "  yi, c, h = step_start(rnn_params, xbow[:, 0]) # try to predict the first token of each x in the batch\n",
    "  ys.append(yi)\n",
    "  for n in range(1, xbow.shape[1]):\n",
    "    y, c, h = step(rnn_params, c, h, xbow[:, n], n=n) # try to predict i+1th token for each batch\n",
    "    ys.append(y)\n",
    "  \n",
    "  ys_out = jnp.transpose(jnp.array(ys), (1, 0, 2)) # turn it from (T, B, C) to (B, T, C)\n",
    "  debug_log(\"ys\", ys_out.shape, f\"should be {('batches', 3, 'C(logits=vocab_size)')}\")\n",
    "\n",
    "  return ys_out\n",
    "\n",
    "\n",
    "# learned embeddings\n",
    "embedding_type = \"learned\"\n",
    "if embedding_type == \"learned\":\n",
    "  @jax.jit\n",
    "  def embed_tokens(embedding_params, tokens):\n",
    "    # ts[:, None] turns it from [t, t, t, t] to [[t], [t], [t], [t]]. as it should be, a row vector. transpose but for 1d vec.\n",
    "    # x = one-hot(x) -> embed(x)\n",
    "    oh = jax.nn.one_hot(tokens[:, None], vocab_size, axis=1) # T, 1 => T, vocab_size\n",
    "    debug_log(oh.shape, embedding_params[\"layer_1\"][\"w\"].shape)\n",
    "    x = oh @ embedding_params[\"layer_1\"][\"w\"] + embedding_params[\"layer_1\"][\"b\"] # (T, vocab_size) # (vocab_size, model_dim) => (T, model_dim)\n",
    "    return x\n",
    "  \n",
    "  def init_embedding_params(key, vocab_size, model_dim):\n",
    "    keys = random.split(key, 10)\n",
    "    embedding_params = {\n",
    "      \"layer_1\" : {\n",
    "        \"w\" : random.normal(keys[0], shape=(vocab_size, model_dim)), # T, vocab_size => T, model_dim\n",
    "        \"b\" : jnp.zeros((model_dim,)),\n",
    "        }\n",
    "    }\n",
    "    return embedding_params\n",
    "elif embedding_type == \"one-hot\":\n",
    "  # one hot embeddings\n",
    "  # EXPENSIVE when vocab size is high\n",
    "  @partial(jax.jit, device=gpu_device)\n",
    "  def embed_tokens(tokens):\n",
    "    return jax.nn.one_hot(tokens, vocab_size, axis=-1)\n",
    "\n",
    "\n",
    "def embed_chars(chars):\n",
    "  tokens = encode(chars)\n",
    "  return embed_tokens(tokens)\n",
    "\n",
    "\n",
    "@partial(jax.jit, device=gpu_device)\n",
    "def get_loss(rnn_params, xtokens, ytokens):\n",
    "  xbow = embed_tokens(rnn_params[-1], xtokens)\n",
    "  debug_log(\"embeddings: \", xbow.shape, \"should be\", \"(B, T, C(one hot)) =\", (\"b\", 3, \"vocab_size\"))\n",
    "  logits = forward(rnn_params, xbow)\n",
    "  debug_log(\"logits:\", logits.shape, \"should be\", \"(B, T, C) =\", (\"b\", 3, \"vocab_size\"))\n",
    "  vocab_size = logits.shape[-1] # channel dimension\n",
    "  ytokens_one_hot = jax.nn.one_hot(ytokens, vocab_size, axis=-1)\n",
    "  debug_log(\"ytokens_one_hot\", ytokens_one_hot.shape, \"should be\", \"(B, T, C(one hot)) =\", (\"b\", 3, \"vocab_size\"))\n",
    "  cross_entropies = -jnp.sum(jax.nn.log_softmax(logits, axis=-1) * ytokens_one_hot, axis=-1) # axis=-1 is along C in a (B,T,C)\n",
    "  debug_log(\"cross_entropies\", cross_entropies.shape, \"should be\", \"(B, T, 1) =\", (\"b\", 3, 1))\n",
    "  net_cross_entropy_loss = jnp.mean(cross_entropies)\n",
    "  return net_cross_entropy_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Token IDs: [2, 5, 5, 4, 3]\n",
      "Tokens: ['u', 'mmmm', 'mmmm', 'mm', 'm</w>']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "# Your text\n",
    "text = \"ummmmmmmmmmm\"\n",
    "\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"Token IDs:\", output.ids)\n",
    "print(\"Tokens:\", output.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 13:19:43.870415: W external/xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_0_bfc) ran out of memory trying to allocate 15.01MiB (rounded to 15737344)requested by op \n",
      "2024-12-09 13:19:43.873072: W external/xla/xla/tsl/framework/bfc_allocator.cc:508] ****************************************************************************************************\n",
      "E1209 13:19:43.873101  246714 pjrt_stream_executor_client.cc:3085] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 15737220 bytes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 15737220 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m output_shape \u001b[38;5;66;03m# small for now\u001b[39;00m\n\u001b[1;32m     11\u001b[0m conveyor_shape \u001b[38;5;241m=\u001b[39m output_shape \u001b[38;5;66;03m# these HAVE TO BE THE SAME\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m rnn_params \u001b[38;5;241m=\u001b[39m \u001b[43minit_rnn_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconveyor_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearned\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     17\u001b[0m   embedding_params \u001b[38;5;241m=\u001b[39m init_embedding_params(keys[\u001b[38;5;241m0\u001b[39m], vocab_size, model_dim)\n",
      "Cell \u001b[0;32mIn[40], line 9\u001b[0m, in \u001b[0;36minit_rnn_params\u001b[0;34m(key, input_shape, hidden_shape, conveyor_shape, output_shape)\u001b[0m\n\u001b[1;32m      7\u001b[0m conveyor_shape \u001b[38;5;241m=\u001b[39m conveyor_shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m output_size \u001b[38;5;241m=\u001b[39m output_shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m rnn_params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m# xW means shape of W is (input, output)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m# hW and xW should output the shape of h.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m   {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhxc\u001b[39m\u001b[38;5;124m\"\u001b[39m : random\u001b[38;5;241m.\u001b[39mnormal(keys[\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0\u001b[39m], shape\u001b[38;5;241m=\u001b[39m(hx_concat_length, output_size)) \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m hx_concat_length),\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBhxc\u001b[39m\u001b[38;5;124m\"\u001b[39m : jnp\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(output_size,)),\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhxi\u001b[39m\u001b[38;5;124m\"\u001b[39m : random\u001b[38;5;241m.\u001b[39mnormal(keys[\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], shape\u001b[38;5;241m=\u001b[39m(hx_concat_length, output_size)) \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m hx_concat_length),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBhxi\u001b[39m\u001b[38;5;124m\"\u001b[39m : jnp\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(output_size,)),\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhxcupd\u001b[39m\u001b[38;5;124m\"\u001b[39m : random\u001b[38;5;241m.\u001b[39mnormal(keys[\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m], shape\u001b[38;5;241m=\u001b[39m(hx_concat_length, output_size)) \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m hx_concat_length),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBhxcupd\u001b[39m\u001b[38;5;124m\"\u001b[39m : jnp\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(output_size,)),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhxo\u001b[39m\u001b[38;5;124m\"\u001b[39m : random\u001b[38;5;241m.\u001b[39mnormal(keys[\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m], shape\u001b[38;5;241m=\u001b[39m(hx_concat_length, output_size)) \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m hx_concat_length),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBhxo\u001b[39m\u001b[38;5;124m\"\u001b[39m : jnp\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(output_size,)),\n\u001b[1;32m     21\u001b[0m   }\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# +1 for the empty sequence at the start\u001b[39;00m\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rnn_params\n",
      "Cell \u001b[0;32mIn[40], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m conveyor_shape \u001b[38;5;241m=\u001b[39m conveyor_shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m output_size \u001b[38;5;241m=\u001b[39m output_shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m rnn_params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m# xW means shape of W is (input, output)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m# hW and xW should output the shape of h.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m   {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhxc\u001b[39m\u001b[38;5;124m\"\u001b[39m : random\u001b[38;5;241m.\u001b[39mnormal(keys[\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0\u001b[39m], shape\u001b[38;5;241m=\u001b[39m(hx_concat_length, output_size)) \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m hx_concat_length),\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBhxc\u001b[39m\u001b[38;5;124m\"\u001b[39m : jnp\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(output_size,)),\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhxi\u001b[39m\u001b[38;5;124m\"\u001b[39m : random\u001b[38;5;241m.\u001b[39mnormal(keys[\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], shape\u001b[38;5;241m=\u001b[39m(hx_concat_length, output_size)) \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m hx_concat_length),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBhxi\u001b[39m\u001b[38;5;124m\"\u001b[39m : jnp\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(output_size,)),\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhxcupd\u001b[39m\u001b[38;5;124m\"\u001b[39m : random\u001b[38;5;241m.\u001b[39mnormal(keys[\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m], shape\u001b[38;5;241m=\u001b[39m(hx_concat_length, output_size)) \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m hx_concat_length),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBhxcupd\u001b[39m\u001b[38;5;124m\"\u001b[39m : jnp\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(output_size,)),\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhxo\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhx_concat_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m hx_concat_length),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBhxo\u001b[39m\u001b[38;5;124m\"\u001b[39m : jnp\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(output_size,)),\n\u001b[1;32m     21\u001b[0m   }\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# +1 for the empty sequence at the start\u001b[39;00m\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rnn_params\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/random.py:700\u001b[0m, in \u001b[0;36mnormal\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype argument to `normal` must be a float or complex dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    698\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    699\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype)\n\u001b[0;32m--> 700\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 15737220 bytes."
     ]
    }
   ],
   "source": [
    "# train\n",
    "keys = random.split(random.PRNGKey(198123), 10)\n",
    "\n",
    "if embedding_type == \"learned\":\n",
    "  model_dim = 11 # C\n",
    "elif embedding_type == \"one-hot\":\n",
    "  model_dim = vocab_size\n",
    "input_shape = (model_dim,) # channel size\n",
    "output_shape = (vocab_size,) # logits\n",
    "hidden_shape = output_shape # small for now\n",
    "conveyor_shape = output_shape # these HAVE TO BE THE SAME\n",
    "\n",
    "\n",
    "\n",
    "rnn_params = init_rnn_params(keys[1], input_shape, hidden_shape, conveyor_shape, output_shape)\n",
    "if embedding_type == \"learned\":\n",
    "  embedding_params = init_embedding_params(keys[0], vocab_size, model_dim)\n",
    "  rnn_params.append(embedding_params)\n",
    "jax.device_put(rnn_params, device=gpu_device)\n",
    "\n",
    "\n",
    "\n",
    "lr = 2e-3\n",
    "#scheduler = optax.schedules.linear_onecycle_schedule(\n",
    "#  transition_steps=300000,\n",
    "#  peak_value=lr,\n",
    "#  pct_start = 0.3,\n",
    "#  pct_final = 0.9,\n",
    "#  div_factor = 25,\n",
    "#  final_div_factor=100000,\n",
    "#)\n",
    "#optimizer = optax.chain(\n",
    "#  optax.scale_by_adam(),\n",
    "#  optax.scale_by_schedule(scheduler),\n",
    "#  optax.scale(-1), # params += -learning_rate x grads\n",
    "#)\n",
    "\n",
    "# https://stackoverflow.com/a/53046624\n",
    "minibatch_size = 500\n",
    "minibatch_lr_scaling = jnp.sqrt(minibatch_size)\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(rnn_params)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(rnn_params, xtokens, ytokens, opt_state):\n",
    "  loss, grads = jax.value_and_grad(get_loss)(rnn_params, xtokens, ytokens)\n",
    "  param_updates, updated_opt_state = optimizer.update(grads, opt_state, rnn_params)\n",
    "  updated_params = optax.apply_updates(rnn_params, param_updates)\n",
    "  return loss, updated_params, updated_opt_state\n",
    "\n",
    "\n",
    "sample_size = 300000 # num of post samples to train on\n",
    "\n",
    "sequences = jnp.array(sequence_token_sets[:sample_size])\n",
    "print_every = 1\n",
    "print(f'Training on {sequences.shape[0]} posts')\n",
    "last_time = time.time()\n",
    "#with jax.profiler.trace('./tmp/run'):\n",
    "for train_sample_idx in range(sample_size - 1):\n",
    "  # train step on batch\n",
    "  xtokens_minibatch = sequences[train_sample_idx:train_sample_idx+minibatch_size]\n",
    "  ytokens_minibatch = sequences[train_sample_idx+1:train_sample_idx+minibatch_size+1]\n",
    "  debug_log(\"xtokens_minibatch: \", xtokens_minibatch.shape)\n",
    "  loss, rnn_params, opt_state = train_step(rnn_params, xtokens_minibatch, ytokens_minibatch, opt_state)\n",
    "  \n",
    "  if train_sample_idx % print_every == 0:\n",
    "    batch_time = time.time() - last_time\n",
    "    last_time = time.time()\n",
    "    \n",
    "    show_output = True\n",
    "    if show_output:\n",
    "      logits = forward(rnn_params, embed_tokens(xtokens_minibatch)) # B, T, C\n",
    "      yhattokens = jnp.argmax(logits, axis=-1) # B, T\n",
    "      debug_log(\"logits:\", logits.shape)\n",
    "      yhat_minibatch = jnp.argmax(logits, axis=-1)\n",
    "      debug_log(\"yhat_minibatch:\", yhat_minibatch.shape)\n",
    "      prediction = decode(xtokens_minibatch[-1]).replace('\\n', '')\n",
    "      prediction_out = decode(yhat_minibatch[-1]).replace('\\n', '')\n",
    "      print(f'minibatch {train_sample_idx:4.0f}  loss/seq={loss}  samples/s={minibatch_size/batch_time:0.2f}  prediction: \"{prediction}\" => \"{prediction_out}\"')\n",
    "    else:\n",
    "      print(f'minibatch {train_sample_idx:4.0f}  loss/seq={loss/xtokens_minibatch.shape[0]:3.4f}  tsteps/s={xtokens_minibatch.shape[0]/batch_time:0.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post: @ante ante ante ante ante ante ante ante ante ante ante ante ante ante ante ante ante ante ante ant"
     ]
    }
   ],
   "source": [
    "def inference(xchars):\n",
    "  xbow = embed_chars(xchars)[None, :] # from T,C to B,T,C where B=1\n",
    "  logits = forward(rnn_params, xbow) # 1, T, C\n",
    "  yhatbow = jnp.argmax(logits, axis=-1)[0] #1, T => T\n",
    "  yhatbow_chars = decode(yhatbow) # T\n",
    "  return yhatbow_chars # T\n",
    "\n",
    "text = 'post:'\n",
    "print(text, end='')\n",
    "for i in range(100):\n",
    "  current_input = text[-sequence_length:] # final $seq_length chars\n",
    "  next_char = inference(current_input)[-1]\n",
    "  text += next_char\n",
    "  print(next_char, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 16:14:12.800525: E external/xla/xla/python/profiler/internal/python_hooks.cc:400] Can't import tensorflow.python.profiler.trace\n",
      "/home/dan/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:2953: RuntimeWarning: invalid value encountered in cast\n",
      "  out = np.array(c).astype(eqn.params['new_dtype'])\n",
      "2024-12-02 16:14:22.047804: E external/xla/xla/python/profiler/internal/python_hooks.cc:400] Can't import tensorflow.python.profiler.trace\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "with jax.profiler.trace('./tmp/trace'):\n",
    "  for i in range(1000):\n",
    "    x = jax.nn.log_softmax(jnp.arange(100000000))\n",
    "  y = x + 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
