{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from functools import partial\n",
    "import random as rand\n",
    "import time\n",
    "\n",
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "gpu_device = jax.devices('gpu')[0]\n",
    "cpu_device = jax.devices('cpu')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_log = print\n",
    "debug_log = lambda *x: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "sequence_length = 2\n",
    "max_vocab_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment:\n",
    "get my posts\n",
    "input: nothing\n",
    "output: a post\n",
    "\n",
    "experiment:\n",
    "video generator\n",
    "input: blank transparent canvas\n",
    "output: the next frame of the video\n",
    "do a several convolution skip connections on each step (adds changes to each frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replies:\n",
      "@VictorTaelin ive found sonnet 3.5v2 to be surprisingly good at coding. upgraded my tools from v1 to v2 and all of a sudden i have to reprompt it like 1/3rd the time\n",
      "@dgant &lt;script src=\"gifeditor.mp3\" type=\"application/json\"&gt;\n",
      "@bozo10n ðŸ’ª\n",
      "@sunsettler experimentation games are the best\n",
      "\n",
      "Posts:\n",
      "compression always feels so satisfying https://t.co/mM5acJxydL\n",
      "it only takes one line of code to make a gifboard btw https://t.co/hFlPyNTvRm\n",
      "RT @calbch: @kuberdenis entrepreneurship is the ultimative vehicle for personal development\n",
      "just two idiots playing a game of chess https://t.co/USjWySv3W9\n"
     ]
    }
   ],
   "source": [
    "# process tweet data\n",
    "import json\n",
    "\n",
    "# Load the tweets.js file\n",
    "with open('./tweets.js', 'r', encoding='utf-8') as file:\n",
    "    # Skip the JavaScript assignment and load only the JSON part\n",
    "    content = file.read()\n",
    "    json_data = content.split('=', 1)[1].strip()  # Extract the JSON part after `=`\n",
    "    json_data = json_data.rstrip(';')  # Remove trailing semicolon if present\n",
    "    tweets_data = json.loads(json_data)\n",
    "\n",
    "# Initialize lists for replies and posts\n",
    "replies = []\n",
    "posts = []\n",
    "\n",
    "# Process each tweet in the dataset\n",
    "for tweet_obj in tweets_data:\n",
    "    tweet = tweet_obj[\"tweet\"]\n",
    "    \n",
    "    if \"in_reply_to_status_id_str\" in tweet and tweet[\"in_reply_to_status_id_str\"]:\n",
    "        # It's a reply\n",
    "        replies.append({\n",
    "            \"id\": tweet[\"id_str\"],\n",
    "            \"text\": tweet[\"full_text\"],\n",
    "            \"in_reply_to\": tweet[\"in_reply_to_status_id_str\"],\n",
    "            \"user\": tweet.get(\"in_reply_to_screen_name\", None),\n",
    "            \"created_at\": tweet[\"created_at\"]\n",
    "        })\n",
    "    else:\n",
    "        # It's a standalone post\n",
    "        posts.append({\n",
    "            \"id\": tweet[\"id_str\"],\n",
    "            \"text\": tweet[\"full_text\"],\n",
    "            \"created_at\": tweet[\"created_at\"]\n",
    "        })\n",
    "\n",
    "# Output the results\n",
    "print(\"Replies:\")\n",
    "for reply in replies[:4]:\n",
    "    print(reply[\"text\"])\n",
    "\n",
    "print(\"\\nPosts:\")\n",
    "for post in posts[:4]:\n",
    "    print(post[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathered initial posts\n",
      "\n",
      "\n",
      "\n",
      "vocab size: 1000\n",
      "converted posts to tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 43683.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 11089 sequences of len 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set up and process data\n",
    "post_limit = 100\n",
    "\n",
    "stop_char = \"ðŸ›‘\"\n",
    "all_posts = [\n",
    "  \"reply: \" + reply[\"text\"] + stop_char for reply in replies[:post_limit]\n",
    "]\n",
    "all_posts.extend(\n",
    "  [\n",
    "    \"post: \" + post[\"text\"] + stop_char for post in posts[:post_limit]\n",
    "  ]\n",
    ")\n",
    "rand.shuffle(all_posts)\n",
    "print('gathered initial posts')\n",
    "\n",
    "token_type = \"char\"\n",
    "if token_type == \"char\":\n",
    "  tokenizer = CharBPETokenizer()\n",
    "\n",
    "  text = \"\".join(all_posts)\n",
    "  # Initialize and train the tokenizer\n",
    "  \n",
    "  tokenizer.train_from_iterator([text], vocab_size = max_vocab_size)\n",
    "\n",
    "  # Tokenize the text\n",
    "  vocab_size = len(tokenizer.get_vocab())\n",
    "  print(\"vocab size:\", vocab_size)\n",
    "\n",
    "  encode = lambda characters: tokenizer.encode(characters).ids\n",
    "  decode = lambda token_ids: \"\".join(tokenizer.decode(token_ids))\n",
    "\n",
    "  translation = dict([(v, k.replace(\"</w>\", \" \")) for k, v in tokenizer.get_vocab().items()])\n",
    "  decode_to_tokens = lambda token_ids: [translation[int(token_id)] for token_id in token_ids]\n",
    "\n",
    "\n",
    "post_token_sets = [encode(post) for post in all_posts]\n",
    "print('converted posts to tokens')\n",
    "\n",
    "from tqdm import tqdm\n",
    "sequence_token_sets = []\n",
    "for post in tqdm(post_token_sets):\n",
    "  for i in range(max(0, len(post) - sequence_length + 1)):\n",
    "    sequence_token_sets.append(post[i:i+sequence_length])\n",
    "\n",
    "\n",
    "print(f\"loaded {len(sequence_token_sets)} sequences of len {sequence_length}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train functions\n",
    "# init rnn params\n",
    "# inputs ()\n",
    "def init_rnn_params(key, input_shape, hidden_shape, conveyor_shape, output_shape):\n",
    "  keys = random.split(key, 4*sequence_length)\n",
    "  hx_concat_length = hidden_shape[0] + input_shape[0] # todo incorporate batching\n",
    "  conveyor_shape = conveyor_shape[0]\n",
    "  output_size = output_shape[0]\n",
    "  rnn_params = [\n",
    "    # xW means shape of W is (input, output)\n",
    "    # hW and xW should output the shape of h.\n",
    "    {\n",
    "      \"Whxc\" : random.normal(keys[3*n + 0], shape=(hx_concat_length, output_size)) * jnp.sqrt(2 / hx_concat_length),\n",
    "      \"Bhxc\" : jnp.zeros(shape=(output_size,)),\n",
    "      \"Whxi\" : random.normal(keys[3*n + 1], shape=(hx_concat_length, output_size)) * jnp.sqrt(2 / hx_concat_length),\n",
    "      \"Bhxi\" : jnp.zeros(shape=(output_size,)),\n",
    "      \"Whxcupd\" : random.normal(keys[3*n + 2], shape=(hx_concat_length, output_size)) * jnp.sqrt(2 / hx_concat_length),\n",
    "      \"Bhxcupd\" : jnp.zeros(shape=(output_size,)),\n",
    "      \"Whxo\" : random.normal(keys[3*n + 3], shape=(hx_concat_length, output_size)) * jnp.sqrt(2 / hx_concat_length),\n",
    "      \"Bhxo\" : jnp.zeros(shape=(output_size,)),\n",
    "    }\n",
    "    for n in range(sequence_length) # +1 for the empty sequence at the start # WRONG\n",
    "  ]\n",
    "  return rnn_params\n",
    "\n",
    "@partial(jax.jit, device=gpu_device)\n",
    "def step_start(rnn_params, x):\n",
    "  # todo incorporate batching\n",
    "  hx_concat_length, conveyor_shape = rnn_params[0][\"Whxc\"].shape\n",
    "  # x is B, C (T == 1 so its not in here)\n",
    "  batch_size = x.shape[0]\n",
    "  input_shape = x.shape[1]\n",
    "  h_shape = hx_concat_length - input_shape\n",
    "  h = jnp.ones((batch_size, h_shape)) # batch size 1 for now\n",
    "  c = jnp.ones((batch_size, conveyor_shape))\n",
    "  y, c, h = step(rnn_params, c, h, x, n=0)\n",
    "  debug_log('y: ', y.shape, f\"should be {('Batch', 'C(logits=vocab_size)')}\")\n",
    "  return y, c, h\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"n\"], device=gpu_device) # n is static (i think calculated at jit comptime each time a new val is passed)\n",
    "def step(rnn_params, c, h, x, n):\n",
    "  # dim=1 so that for batching when its (n, h+x) it concats h+x\n",
    "  hxconcat = jax.lax.concatenate([h, x], dimension=1) # (batch, h+x)\n",
    "\n",
    "  debug_log('x: ', x.shape, f\"should be {('Batch', 'C(one hot encodings)')}\")\n",
    "\n",
    "  # input gate\n",
    "  # (B, h+x) @ (h+x,c) => (B, c) (B = batch dimension, right now its just 1)\n",
    "  debug_log(n)\n",
    "  wxc = rnn_params[n][\"Whxc\"]\n",
    "  xc = hxconcat @ wxc\n",
    "  b = rnn_params[n][\"Bhxc\"]\n",
    "  f = jax.nn.sigmoid(xc + b)\n",
    "  c = c * f # (B, c) * (B, c) => (B, c)\n",
    "\n",
    "  # update gate\n",
    "  # (B, h+x) @ (h+x, c) => (B, c)\n",
    "  i = jax.nn.sigmoid(hxconcat @ rnn_params[n][\"Whxi\"] + rnn_params[n][\"Bhxi\"])\n",
    "  # (B, h+x) @ (h+x, c) => (B, c)\n",
    "  c_upd = jax.nn.tanh(hxconcat @ rnn_params[n][\"Whxcupd\"] + rnn_params[n][\"Bhxcupd\"])\n",
    "  c = c + i*c_upd # (B, c) + (B, c) * (B, c) => (B, c)\n",
    "\n",
    "  # output gate\n",
    "  o = jax.nn.sigmoid(hxconcat @ rnn_params[n][\"Whxo\"] + rnn_params[n][\"Bhxo\"])\n",
    "  h = o * jax.nn.tanh(c) # delete gate\n",
    "\n",
    "  y = h\n",
    "  return y, c, h\n",
    "\n",
    "@partial(jax.jit, device=gpu_device)\n",
    "def forward(rnn_params, xbow):\n",
    "  debug_log(\"forward() called\")\n",
    "  ys = []\n",
    "  yi, c, h = step_start(rnn_params, xbow[:, 0]) # try to predict the first token of each x in the batch\n",
    "  ys.append(yi)\n",
    "  debug_log(\"steps_total:\", xbow.shape[1], list(range(1, xbow.shape[1])))\n",
    "  for n in range(1, xbow.shape[1]):\n",
    "    debug_log(\"step n:\", n)\n",
    "    y, c, h = step(rnn_params, c, h, xbow[:, n], n=n) # try to predict i+1th token for each batch\n",
    "    ys.append(y)\n",
    "  \n",
    "  ys_out = jnp.transpose(jnp.array(ys), (1, 0, 2)) # turn it from (T, B, C) to (B, T, C)\n",
    "  debug_log(\"ys\", ys_out.shape, f\"should be {('batches', 3, 'C(logits=vocab_size)')}\")\n",
    "\n",
    "  return ys_out\n",
    "\n",
    "\n",
    "# learned embeddings\n",
    "embedding_type = \"learned\"\n",
    "if embedding_type == \"learned\":\n",
    "  @jax.jit\n",
    "  def embed_tokens(embedding_params, tokens):\n",
    "    # ts[:, None] turns it from [t, t, t, t] to [[t], [t], [t], [t]]. as it should be, a row vector. transpose but for 1d vec.\n",
    "    # x = one-hot(x) -> embed(x)\n",
    "    oh = jax.nn.one_hot(tokens, vocab_size, axis=-1) # B, T, => B, T, 1 => T, vocab_size\n",
    "    debug_log(tokens.shape, oh.shape, embedding_params[\"layer_1\"][\"w\"].shape)\n",
    "    x = oh @ embedding_params[\"layer_1\"][\"w\"] + embedding_params[\"layer_1\"][\"b\"] # (T, vocab_size) # (vocab_size, model_dim) => (T, model_dim)\n",
    "    return x\n",
    "  \n",
    "  def init_embedding_params(key, vocab_size, model_dim):\n",
    "    keys = random.split(key, 10)\n",
    "    embedding_params = {\n",
    "      \"layer_1\" : {\n",
    "        \"w\" : random.normal(keys[0], shape=(vocab_size, model_dim)), # T, vocab_size => T, model_dim\n",
    "        \"b\" : jnp.zeros((model_dim,)),\n",
    "        }\n",
    "    }\n",
    "    return embedding_params\n",
    "elif embedding_type == \"one-hot\":\n",
    "  # one hot embeddings\n",
    "  # EXPENSIVE when vocab size is high\n",
    "  @partial(jax.jit, device=gpu_device)\n",
    "  def embed_tokens(tokens):\n",
    "    # create channel dim and one hot. (B, T) => (B, T, 1) => (B, T, C)\n",
    "    return jax.nn.one_hot(tokens, vocab_size, axis=-1)\n",
    "\n",
    "\n",
    "def embed_chars(chars):\n",
    "  tokens = encode(chars)\n",
    "  return embed_tokens(tokens)\n",
    "\n",
    "\n",
    "@partial(jax.jit, device=gpu_device)\n",
    "def get_loss(rnn_params, xtokens, ytokens):\n",
    "  debug_log(\"xtokens: \", xtokens.shape, \"should be (B, T)\")\n",
    "  xbow = embed_tokens(rnn_params[-1], xtokens)\n",
    "  debug_log(\"embeddings: \", xbow.shape, \"should be\", \"(B, T, C(model_size)) =\", (\"b\", 2, \"model_size\"))\n",
    "  logits = forward(rnn_params, xbow)\n",
    "  debug_log(\"logits:\", logits.shape, \"should be\", \"(B, T, C) =\", (\"b\", 2, \"vocab_size\"))\n",
    "  vocab_size = logits.shape[-1] # channel dimension\n",
    "  ytokens_one_hot = jax.nn.one_hot(ytokens, vocab_size, axis=-1)\n",
    "  debug_log(\"ytokens_one_hot\", ytokens_one_hot.shape, \"should be\", \"(B, T, C(one hot)) =\", (\"b\", 2, \"vocab_size\"))\n",
    "  cross_entropies = -jnp.sum(ytokens_one_hot * jax.nn.log_softmax(logits, axis=-1), axis=-1) # axis=-1 is along C in a (B,T,C)\n",
    "  debug_log(\"cross_entropies\", cross_entropies.shape, \"should be\", \"(B, T, 1) =\", (\"b\", 3, 1))\n",
    "  net_cross_entropy_loss = jnp.mean(cross_entropies)\n",
    "  return net_cross_entropy_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_setup\n",
    "keys = random.split(random.PRNGKey(198123), 10)\n",
    "\n",
    "if embedding_type == \"learned\":\n",
    "  model_dim = 1024*8 # C\n",
    "elif embedding_type == \"one-hot\":\n",
    "  model_dim = vocab_size\n",
    "input_shape = (model_dim,) # channel size\n",
    "output_shape = (vocab_size,) # logits\n",
    "hidden_shape = output_shape # small for now\n",
    "conveyor_shape = output_shape # these HAVE TO BE THE SAME\n",
    "\n",
    "\n",
    "\n",
    "rnn_params = init_rnn_params(keys[1], input_shape, hidden_shape, conveyor_shape, output_shape)\n",
    "if embedding_type == \"learned\":\n",
    "  embedding_params = init_embedding_params(keys[0], vocab_size, model_dim)\n",
    "  rnn_params.append(embedding_params)\n",
    "jax.device_put(rnn_params, device=gpu_device)\n",
    "\n",
    "\n",
    "\n",
    "lr = 2e-1\n",
    "minibatch_size = 1\n",
    "minibatch_lr_scaling = jnp.sqrt(minibatch_size)\n",
    "lr *= minibatch_lr_scaling\n",
    "scheduler = optax.schedules.linear_onecycle_schedule(\n",
    "  transition_steps=10000,\n",
    "  peak_value=lr,\n",
    "  pct_start = 0.1,\n",
    "  pct_final = 0.9,\n",
    "  div_factor = 25,\n",
    "  final_div_factor=100000,\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "  optax.scale_by_adam(),\n",
    "  optax.scale_by_schedule(scheduler),\n",
    "  optax.scale(-1), # params += -learning_rate x grads\n",
    ")\n",
    "\n",
    "# https://stackoverflow.com/a/53046624\n",
    "\n",
    "#optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(rnn_params)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(rnn_params, xtokens, ytokens, opt_state):\n",
    "  loss, grads = jax.value_and_grad(get_loss)(rnn_params, xtokens, ytokens)\n",
    "  param_updates, updated_opt_state = optimizer.update(grads, opt_state, rnn_params)\n",
    "  updated_params = optax.apply_updates(rnn_params, param_updates)\n",
    "  return loss, updated_params, updated_opt_state, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 5 posts\n",
      "hxconcat (1, 9192)\n",
      "hxconcat (1, 9192)\n",
      "minibatch    0  loss/seq=7.154355049133301  samples/s=0.37  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch    1  loss/seq=6.786767959594727  samples/s=0.58  prediction: \": Im\" => \"UP Im\" || \"Im so\"\n",
      "minibatch    2  loss/seq=7.171241760253906  samples/s=26.54  prediction: \"Im so\" => \"* so\" || \"so so\"\n",
      "minibatch    3  loss/seq=6.8018388748168945  samples/s=26.33  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch    4  loss/seq=6.071624755859375  samples/s=26.67  prediction: \": Im\" => \"CCIm\" || \"Im so\"\n",
      "minibatch    5  loss/seq=6.530707359313965  samples/s=26.82  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch    6  loss/seq=6.801838397979736  samples/s=27.77  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch    7  loss/seq=6.057343482971191  samples/s=26.48  prediction: \": Im\" => \"very Im\" || \"Im so\"\n",
      "minibatch    8  loss/seq=6.521933078765869  samples/s=26.90  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch    9  loss/seq=6.801787376403809  samples/s=27.92  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   10  loss/seq=6.055840015411377  samples/s=27.92  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   11  loss/seq=6.519951820373535  samples/s=26.91  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   12  loss/seq=6.8015923500061035  samples/s=28.34  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   13  loss/seq=6.052706718444824  samples/s=27.64  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   14  loss/seq=6.517600059509277  samples/s=21.69  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   15  loss/seq=6.8015923500061035  samples/s=27.90  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   16  loss/seq=6.052349090576172  samples/s=27.92  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   17  loss/seq=6.516843795776367  samples/s=27.27  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   18  loss/seq=6.8015923500061035  samples/s=26.26  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   19  loss/seq=6.052297592163086  samples/s=27.15  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   20  loss/seq=6.5167741775512695  samples/s=25.75  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   21  loss/seq=6.8015923500061035  samples/s=27.96  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   22  loss/seq=6.051450252532959  samples/s=27.72  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   23  loss/seq=6.515312671661377  samples/s=28.38  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   24  loss/seq=6.8015923500061035  samples/s=28.67  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   25  loss/seq=6.050643444061279  samples/s=28.16  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   26  loss/seq=6.515006065368652  samples/s=28.15  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   27  loss/seq=6.8015923500061035  samples/s=27.45  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   28  loss/seq=6.0501298904418945  samples/s=27.34  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   29  loss/seq=6.514762878417969  samples/s=27.22  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   30  loss/seq=6.8015923500061035  samples/s=27.90  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   31  loss/seq=6.049178600311279  samples/s=26.65  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   32  loss/seq=6.5147624015808105  samples/s=26.75  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   33  loss/seq=6.8015923500061035  samples/s=26.00  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   34  loss/seq=6.047998905181885  samples/s=25.88  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   35  loss/seq=6.5147552490234375  samples/s=28.40  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   36  loss/seq=6.8015923500061035  samples/s=28.42  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   37  loss/seq=6.04781436920166  samples/s=27.47  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   38  loss/seq=6.514394760131836  samples/s=22.66  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   39  loss/seq=6.8015923500061035  samples/s=27.04  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   40  loss/seq=6.047519207000732  samples/s=28.63  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   41  loss/seq=6.514382362365723  samples/s=28.45  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   42  loss/seq=6.8015923500061035  samples/s=26.84  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   43  loss/seq=6.047519207000732  samples/s=28.27  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   44  loss/seq=6.5138349533081055  samples/s=27.19  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   45  loss/seq=6.8015923500061035  samples/s=28.81  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   46  loss/seq=6.047518253326416  samples/s=28.60  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   47  loss/seq=6.51383113861084  samples/s=28.45  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   48  loss/seq=6.8015923500061035  samples/s=28.46  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   49  loss/seq=6.046656608581543  samples/s=28.61  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   50  loss/seq=6.513775825500488  samples/s=27.96  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   51  loss/seq=6.8015923500061035  samples/s=27.47  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   52  loss/seq=6.046564102172852  samples/s=28.11  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   53  loss/seq=6.513205528259277  samples/s=27.91  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   54  loss/seq=6.8015923500061035  samples/s=27.89  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   55  loss/seq=6.046292304992676  samples/s=28.83  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   56  loss/seq=6.512402057647705  samples/s=27.92  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   57  loss/seq=6.8015923500061035  samples/s=28.60  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   58  loss/seq=6.045952796936035  samples/s=27.45  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   59  loss/seq=6.511761665344238  samples/s=27.94  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   60  loss/seq=6.8015923500061035  samples/s=27.40  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   61  loss/seq=6.044853210449219  samples/s=28.07  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   62  loss/seq=6.511749267578125  samples/s=28.06  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   63  loss/seq=6.8015923500061035  samples/s=28.73  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   64  loss/seq=6.044132232666016  samples/s=27.02  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   65  loss/seq=6.511748790740967  samples/s=26.67  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   66  loss/seq=6.8015923500061035  samples/s=27.69  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   67  loss/seq=6.043834209442139  samples/s=28.16  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   68  loss/seq=6.5112104415893555  samples/s=27.91  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   69  loss/seq=6.8015923500061035  samples/s=27.91  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   70  loss/seq=6.04353666305542  samples/s=27.74  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   71  loss/seq=6.510543346405029  samples/s=27.65  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   72  loss/seq=6.8015923500061035  samples/s=27.81  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   73  loss/seq=6.043234825134277  samples/s=28.60  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   74  loss/seq=6.510538101196289  samples/s=27.85  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   75  loss/seq=6.8015923500061035  samples/s=28.22  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   76  loss/seq=6.042851448059082  samples/s=27.82  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   77  loss/seq=6.510538101196289  samples/s=28.04  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   78  loss/seq=6.8015923500061035  samples/s=27.94  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   79  loss/seq=6.042136192321777  samples/s=28.61  prediction: \": Im\" => \"very ys\" || \"Im so\"\n",
      "minibatch   80  loss/seq=6.510497093200684  samples/s=28.36  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   81  loss/seq=6.8015923500061035  samples/s=28.79  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   82  loss/seq=6.041539192199707  samples/s=28.66  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch   83  loss/seq=6.509653568267822  samples/s=28.90  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   84  loss/seq=6.8015923500061035  samples/s=28.02  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   85  loss/seq=6.041536331176758  samples/s=28.55  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch   86  loss/seq=6.509605407714844  samples/s=28.63  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   87  loss/seq=6.8015923500061035  samples/s=28.31  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   88  loss/seq=6.040855884552002  samples/s=28.34  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch   89  loss/seq=6.509579658508301  samples/s=27.46  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   90  loss/seq=6.8015923500061035  samples/s=27.93  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   91  loss/seq=6.040475845336914  samples/s=28.00  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch   92  loss/seq=6.509324073791504  samples/s=27.55  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   93  loss/seq=6.8015923500061035  samples/s=28.19  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   94  loss/seq=6.040475845336914  samples/s=27.01  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch   95  loss/seq=6.509113311767578  samples/s=27.44  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   96  loss/seq=6.8015923500061035  samples/s=28.50  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch   97  loss/seq=6.040474891662598  samples/s=27.33  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch   98  loss/seq=6.509041786193848  samples/s=26.12  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch   99  loss/seq=6.8015923500061035  samples/s=27.15  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch  100  loss/seq=6.03983211517334  samples/s=28.16  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch  101  loss/seq=6.508761405944824  samples/s=27.54  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch  102  loss/seq=6.8015923500061035  samples/s=27.43  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch  103  loss/seq=6.038933753967285  samples/s=27.42  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch  104  loss/seq=6.5086822509765625  samples/s=27.05  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch  105  loss/seq=6.8015923500061035  samples/s=28.71  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch  106  loss/seq=6.038933277130127  samples/s=28.82  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch  107  loss/seq=6.508671760559082  samples/s=28.86  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch  108  loss/seq=6.8015923500061035  samples/s=28.54  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch  109  loss/seq=6.038932800292969  samples/s=28.79  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch  110  loss/seq=6.508671760559082  samples/s=27.06  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch  111  loss/seq=6.8015923500061035  samples/s=27.78  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch  112  loss/seq=6.038668632507324  samples/s=27.81  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch  113  loss/seq=6.508671760559082  samples/s=28.67  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch  114  loss/seq=6.8015923500061035  samples/s=28.33  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch  115  loss/seq=6.03855037689209  samples/s=28.81  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch  116  loss/seq=6.50832462310791  samples/s=28.33  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch  117  loss/seq=6.8015923500061035  samples/s=28.24  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch  118  loss/seq=6.038541316986084  samples/s=28.93  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch  119  loss/seq=6.508018493652344  samples/s=28.80  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch  120  loss/seq=6.8015923500061035  samples/s=28.83  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch  121  loss/seq=6.037771224975586  samples/s=28.67  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n",
      "minibatch  122  loss/seq=6.507736682891846  samples/s=28.71  prediction: \"Im so\" => \"? so\" || \"so so\"\n",
      "minibatch  123  loss/seq=6.8015923500061035  samples/s=28.86  prediction: \"post :\" => \"? ti\" || \": Im\"\n",
      "minibatch  124  loss/seq=6.03656005859375  samples/s=28.72  prediction: \": Im\" => \"Im ys\" || \"Im so\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m yhat_minibatch \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m debug_log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myhat_minibatch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, yhat_minibatch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 29\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtokens_minibatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m prediction_out \u001b[38;5;241m=\u001b[39m decode(yhat_minibatch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m correct \u001b[38;5;241m=\u001b[39m decode(ytokens_minibatch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(token_ids)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, vocab_size)\n\u001b[1;32m     29\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m characters: tokenizer\u001b[38;5;241m.\u001b[39mencode(characters)\u001b[38;5;241m.\u001b[39mids\n\u001b[0;32m---> 30\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m token_ids: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m translation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(v, k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</w>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mget_vocab()\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m     33\u001b[0m decode_to_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m token_ids: [translation[\u001b[38;5;28mint\u001b[39m(token_id)] \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m token_ids]\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/tokenizers/implementations/base_tokenizer.py:278\u001b[0m, in \u001b[0;36mBaseTokenizer.decode\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone input is not valid. Should be a list of integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/array.py:318\u001b[0m, in \u001b[0;36mArrayImpl.__index__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__index__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    317\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_integer_conversion(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 318\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/profiler.py:333\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/array.py:628\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_steps = 10000\n",
    "base_sample_size = 3 # num of post samples to train on\n",
    "sequences = jnp.array(sequence_token_sets[:base_sample_size + minibatch_size + 1])\n",
    "sample_size = base_sample_size + minibatch_size\n",
    "print_every = 1\n",
    "print(f'Training on {sequences.shape[0]} posts')\n",
    "last_time = time.time()\n",
    "#with jax.profiler.trace('./tmp/run'):\n",
    "for train_step_num in range(train_steps - 1):\n",
    "  # train step on batch\n",
    "  train_sample_idx = train_step_num % base_sample_size\n",
    "  xtokens_minibatch = sequences[train_sample_idx:train_sample_idx+minibatch_size]\n",
    "  ytokens_minibatch = sequences[train_sample_idx+1:train_sample_idx+minibatch_size+1]\n",
    "  debug_log(\"xtokens_minibatch: \", xtokens_minibatch.shape)\n",
    "  loss, rnn_params, opt_state, grads = train_step(rnn_params, xtokens_minibatch, ytokens_minibatch, opt_state)\n",
    "  \n",
    "  if train_sample_idx % print_every == 0:\n",
    "    batch_time = time.time() - last_time\n",
    "    last_time = time.time()\n",
    "    \n",
    "    show_output = True\n",
    "    if show_output:\n",
    "      logits = forward(rnn_params, embed_tokens(rnn_params[-1], xtokens_minibatch)) # B, T, C\n",
    "      yhattokens = jnp.argmax(logits, axis=-1) # B, T\n",
    "      debug_log(\"logits:\", logits.shape)\n",
    "      yhat_minibatch = jnp.argmax(logits, axis=-1)\n",
    "      debug_log(\"yhat_minibatch:\", yhat_minibatch.shape)\n",
    "      prediction = decode(xtokens_minibatch[-1]).replace('\\n', '')\n",
    "      prediction_out = decode(yhat_minibatch[-1]).replace('\\n', '')\n",
    "      correct = decode(ytokens_minibatch[-1]).replace('\\n', '')\n",
    "      print(f'minibatch {train_step_num:4.0f}  loss/seq={loss}  samples/s={minibatch_size/batch_time:0.2f}  prediction: \"{prediction}\" => \"{prediction_out}\" || \"{correct}\"')\n",
    "    else:\n",
    "      print(f'minibatch {train_step_num:4.0f}  loss/seq={loss/xtokens_minibatch.shape[0]:3.4f}  tsteps/s={xtokens_minibatch.shape[0]/batch_time:0.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post : 5AM 5555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555"
     ]
    }
   ],
   "source": [
    "def inference(xchars):\n",
    "  xtokens = jnp.array(encode(xchars))\n",
    "  debug_log(xtokens)\n",
    "  xbow = embed_tokens(rnn_params[-1], xtokens)[None, :] # from T,C to B,T,C where B=1\n",
    "  logits = forward(rnn_params, xbow) # 1, T, C\n",
    "  yhatbow = jnp.argmax(logits, axis=-1)[0] #1, T => T\n",
    "  yhatbow_chunks = decode_to_tokens(yhatbow) # T\n",
    "  return yhatbow_chunks # T\n",
    "\n",
    "text = 'post : '\n",
    "print(text, end='')\n",
    "for i in range(100):\n",
    "  current_input = decode(encode(text)[-sequence_length:]) # final $seq_length chars\n",
    "  debug_log(\"current input\", len(encode(current_input)), f\"|{current_input}|\")\n",
    "  next_chunk = inference(current_input)[-1]\n",
    "  text += next_chunk\n",
    "  print(next_chunk, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Bhxc': Array(0., dtype=float32),\n",
      "  'Bhxcupd': Array(2.323032e-07, dtype=float32),\n",
      "  'Bhxi': Array(3.941133e-07, dtype=float32),\n",
      "  'Bhxo': Array(0.00017044, dtype=float32),\n",
      "  'Whxc': Array(0., dtype=float32),\n",
      "  'Whxcupd': Array(5.434752e-06, dtype=float32),\n",
      "  'Whxi': Array(9.7011325e-06, dtype=float32),\n",
      "  'Whxo': Array(0.00866771, dtype=float32)},\n",
      " {'Bhxc': Array(0., dtype=float32),\n",
      "  'Bhxcupd': Array(0., dtype=float32),\n",
      "  'Bhxi': Array(0., dtype=float32),\n",
      "  'Bhxo': Array(0.00013062, dtype=float32),\n",
      "  'Whxc': Array(0., dtype=float32),\n",
      "  'Whxcupd': Array(0., dtype=float32),\n",
      "  'Whxi': Array(0., dtype=float32),\n",
      "  'Whxo': Array(0.00522402, dtype=float32)},\n",
      " {'layer_1': {'b': Array(0.00042082, dtype=float32),\n",
      "              'w': Array(0.00042985, dtype=float32)}}]\n"
     ]
    }
   ],
   "source": [
    "# print update norms\n",
    "from pprint import pprint\n",
    "norms = jax.tree_util.tree_map(lambda x: jnp.linalg.norm(x), grads)\n",
    "pprint(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Bhxc': 1000,\n",
      "  'Bhxcupd': 1000,\n",
      "  'Bhxi': 1000,\n",
      "  'Bhxo': 1000,\n",
      "  'Whxc': 2024000,\n",
      "  'Whxcupd': 2024000,\n",
      "  'Whxi': 2024000,\n",
      "  'Whxo': 2024000},\n",
      " {'Bhxc': 1000,\n",
      "  'Bhxcupd': 1000,\n",
      "  'Bhxi': 1000,\n",
      "  'Bhxo': 1000,\n",
      "  'Whxc': 2024000,\n",
      "  'Whxcupd': 2024000,\n",
      "  'Whxi': 2024000,\n",
      "  'Whxo': 2024000},\n",
      " {'layer_1': {'b': 1024, 'w': 1024000}}]\n"
     ]
    }
   ],
   "source": [
    "# print param count\n",
    "from pprint import pprint\n",
    "norms = jax.tree_util.tree_map(lambda x: x.size, grads)\n",
    "pprint(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 16:14:12.800525: E external/xla/xla/python/profiler/internal/python_hooks.cc:400] Can't import tensorflow.python.profiler.trace\n",
      "/home/dan/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:2953: RuntimeWarning: invalid value encountered in cast\n",
      "  out = np.array(c).astype(eqn.params['new_dtype'])\n",
      "2024-12-02 16:14:22.047804: E external/xla/xla/python/profiler/internal/python_hooks.cc:400] Can't import tensorflow.python.profiler.trace\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "with jax.profiler.trace('./tmp/trace'):\n",
    "  for i in range(1000):\n",
    "    x = jax.nn.log_softmax(jnp.arange(100000000))\n",
    "  y = x + 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "debug_log = lambda *x: print(\"[DEBUG]:\", *x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_log = lambda *x: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wn</w>': 413,\n",
       " 'TI': 967,\n",
       " 'ter': 443,\n",
       " 'd': 62,\n",
       " 'Sh': 645,\n",
       " 'reme': 837,\n",
       " 'imo</w>': 892,\n",
       " 'dys': 918,\n",
       " '\\U0001fae1</w>': 163,\n",
       " 'fee': 880,\n",
       " 'that</w>': 309,\n",
       " 'ol': 304,\n",
       " 'ch': 317,\n",
       " 'ive</w>': 440,\n",
       " 'min': 395,\n",
       " 'bi': 649,\n",
       " 'ec': 356,\n",
       " 'ku': 594,\n",
       " 'has</w>': 430,\n",
       " 'into</w>': 692,\n",
       " '`</w>': 141,\n",
       " 'poten': 845,\n",
       " 'potential</w>': 936,\n",
       " 'OO</w>': 961,\n",
       " 'sett': 617,\n",
       " 'me': 259,\n",
       " 'getting</w>': 716,\n",
       " 'No': 779,\n",
       " 'resources</w>': 934,\n",
       " 'cad': 985,\n",
       " 'ðŸ›‘</w>': 127,\n",
       " 'all': 699,\n",
       " 'bt': 388,\n",
       " 'pt</w>': 466,\n",
       " 'ck</w>': 791,\n",
       " 'ful</w>': 343,\n",
       " 'dn': 986,\n",
       " 'sel': 705,\n",
       " 'loo': 618,\n",
       " 'NI': 778,\n",
       " 'fa': 804,\n",
       " 'S</w>': 137,\n",
       " 'buil': 429,\n",
       " 'ges</w>': 661,\n",
       " 'you': 267,\n",
       " 'ML</w>': 434,\n",
       " 'g': 65,\n",
       " 'ssi': 315,\n",
       " 'A': 31,\n",
       " 'LETS</w>': 751,\n",
       " 'der': 798,\n",
       " 'now</w>': 353,\n",
       " 'sually</w>': 862,\n",
       " 'tter</w>': 416,\n",
       " '/': 15,\n",
       " 'ho': 292,\n",
       " 'lo': 229,\n",
       " 'his</w>': 541,\n",
       " 'times</w>': 851,\n",
       " 'Pe': 963,\n",
       " 'zo': 832,\n",
       " 'ME</w>': 777,\n",
       " 'I</w>': 152,\n",
       " 'ni': 361,\n",
       " 'glo': 999,\n",
       " 'bo': 324,\n",
       " 'thats</w>': 916,\n",
       " 'mon</w>': 598,\n",
       " 'gr': 509,\n",
       " 'ed': 405,\n",
       " '#</w>': 147,\n",
       " 'te</w>': 307,\n",
       " 'pe</w>': 815,\n",
       " '%': 5,\n",
       " 'so': 251,\n",
       " 'RT</w>': 345,\n",
       " 'ir</w>': 462,\n",
       " 'ol</w>': 349,\n",
       " 'startu': 578,\n",
       " 'tt': 184,\n",
       " 'giz': 736,\n",
       " '9</w>': 171,\n",
       " 'rac': 306,\n",
       " 'f': 64,\n",
       " 'tool</w>': 625,\n",
       " 'ge</w>': 392,\n",
       " 'ci': 327,\n",
       " 'RE': 643,\n",
       " 'pro</w>': 816,\n",
       " 'sp': 682,\n",
       " 'de</w>': 312,\n",
       " 'â€¦</w>': 126,\n",
       " 'ust</w>': 300,\n",
       " 'ght</w>': 575,\n",
       " '!': 1,\n",
       " ')': 9,\n",
       " 'making</w>': 703,\n",
       " 'ide': 463,\n",
       " 'mend</w>': 875,\n",
       " 'men': 281,\n",
       " 'just</w>': 372,\n",
       " 'avor': 901,\n",
       " 'se</w>': 338,\n",
       " 've': 604,\n",
       " 'good</w>': 384,\n",
       " 'then</w>': 712,\n",
       " 'going</w>': 865,\n",
       " 'impro': 903,\n",
       " 'sc': 376,\n",
       " 'ge': 253,\n",
       " 'fu': 243,\n",
       " 'life</w>': 580,\n",
       " 'star': 417,\n",
       " '_</w>': 133,\n",
       " 'es': 587,\n",
       " 'pa': 314,\n",
       " 'wit': 320,\n",
       " 'SI': 644,\n",
       " 'ation</w>': 648,\n",
       " 'even</w>': 741,\n",
       " 'Y': 55,\n",
       " 'SE': 965,\n",
       " 'gh</w>': 407,\n",
       " 'very</w>': 290,\n",
       " '10': 762,\n",
       " 'tic': 521,\n",
       " 'upda': 824,\n",
       " 'st': 207,\n",
       " 'onn': 565,\n",
       " 'nt</w>': 409,\n",
       " 'I': 39,\n",
       " 'to': 252,\n",
       " 'coun': 655,\n",
       " 'DðŸ›‘reply</w>': 770,\n",
       " 'MT': 641,\n",
       " 'reality</w>': 897,\n",
       " 'with</w>': 323,\n",
       " '3</w>': 145,\n",
       " 'ds</w>': 297,\n",
       " 'ders</w>': 800,\n",
       " 'OO': 455,\n",
       " 'using</w>': 877,\n",
       " 'cise</w>': 883,\n",
       " 'recommend</w>': 933,\n",
       " 'on': 189,\n",
       " 'ms</w>': 360,\n",
       " '16hr</w>': 496,\n",
       " 'cy': 981,\n",
       " '&</w>': 130,\n",
       " 'RY</w>': 781,\n",
       " '*</w>': 173,\n",
       " 'sion</w>': 620,\n",
       " 'F</w>': 169,\n",
       " 'use': 351,\n",
       " '5': 21,\n",
       " '87': 948,\n",
       " 'mi': 596,\n",
       " 'gan': 660,\n",
       " 'ice</w>': 870,\n",
       " 'k': 69,\n",
       " 'io</w>': 666,\n",
       " 'att': 647,\n",
       " 'learning</w>': 493,\n",
       " 'dre': 797,\n",
       " 'DQ': 955,\n",
       " 'B': 32,\n",
       " 'realiz': 898,\n",
       " 'im</w>': 591,\n",
       " 'ore</w>': 303,\n",
       " 'ingðŸ›‘reply</w>': 876,\n",
       " 'em</w>': 802,\n",
       " 'b': 60,\n",
       " 'et': 990,\n",
       " 'other</w>': 513,\n",
       " 'ople</w>': 734,\n",
       " 'od': 546,\n",
       " '(': 8,\n",
       " 'RL': 964,\n",
       " 'added</w>': 630,\n",
       " 's</w>': 95,\n",
       " 'an': 181,\n",
       " 'up</w>': 424,\n",
       " 'op': 337,\n",
       " 'time': 700,\n",
       " 'CC': 953,\n",
       " 'tw': 473,\n",
       " 'ke': 668,\n",
       " 'j': 68,\n",
       " 'this</w>': 260,\n",
       " 'do': 261,\n",
       " 'oltard</w>': 529,\n",
       " 'htt': 197,\n",
       " '14': 942,\n",
       " 'them</w>': 526,\n",
       " 'al</w>': 245,\n",
       " '9': 25,\n",
       " '2</w>': 154,\n",
       " ')</w>': 143,\n",
       " 'ON': 454,\n",
       " 'ma</w>': 672,\n",
       " 'nn': 599,\n",
       " 'p</w>': 120,\n",
       " 'ons</w>': 342,\n",
       " 'tu': 274,\n",
       " 'tion</w>': 293,\n",
       " 'cant</w>': 795,\n",
       " 'c</w>': 96,\n",
       " 'st</w>': 183,\n",
       " 'sw': 819,\n",
       " 'or</w>': 209,\n",
       " 'Shor</w>': 917,\n",
       " 'J': 40,\n",
       " 'da': 288,\n",
       " 'more</w>': 313,\n",
       " 'rep': 201,\n",
       " 'was</w>': 432,\n",
       " 'but</w>': 286,\n",
       " 'M</w>': 102,\n",
       " 'interesting</w>': 532,\n",
       " 'ich</w>': 718,\n",
       " 'gener': 663,\n",
       " 'Bi': 639,\n",
       " 'by</w>': 325,\n",
       " 'am': 585,\n",
       " 'ra': 265,\n",
       " 'r': 76,\n",
       " 'ear': 589,\n",
       " 'led</w>': 853,\n",
       " 'w': 81,\n",
       " 'are</w>': 275,\n",
       " 'scri': 888,\n",
       " 'cy</w>': 983,\n",
       " 'adv': 726,\n",
       " 'MA': 775,\n",
       " 'mes</w>': 512,\n",
       " 'ever</w>': 991,\n",
       " 'sig': 860,\n",
       " 'are': 501,\n",
       " 'reply</w>': 203,\n",
       " 'C': 33,\n",
       " 'see': 704,\n",
       " 'far</w>': 805,\n",
       " 'ati': 976,\n",
       " 'would</w>': 476,\n",
       " 'ima': 543,\n",
       " 'LL': 386,\n",
       " 'of</w>': 219,\n",
       " 'than</w>': 479,\n",
       " 'artic': 843,\n",
       " 'circ': 634,\n",
       " 'ones</w>': 698,\n",
       " 'compression</w>': 731,\n",
       " 'u': 79,\n",
       " 'cal</w>': 653,\n",
       " 'ity</w>': 381,\n",
       " 'all</w>': 364,\n",
       " ',</w>': 165,\n",
       " 'qc</w>': 469,\n",
       " 'out': 609,\n",
       " 'Q': 47,\n",
       " 'les</w>': 612,\n",
       " '20</w>': 763,\n",
       " 'ed</w>': 210,\n",
       " 'low</w>': 451,\n",
       " 'ar</w>': 387,\n",
       " 'have</w>': 418,\n",
       " 'made</w>': 483,\n",
       " 'gif</w>': 662,\n",
       " 'ine</w>': 447,\n",
       " 'in</w>': 226,\n",
       " 'your</w>': 296,\n",
       " 'my</w>': 280,\n",
       " 'these</w>': 713,\n",
       " '8</w>': 104,\n",
       " 'never</w>': 717,\n",
       " 'eb</w>': 803,\n",
       " 'ab': 277,\n",
       " 'uses</w>': 879,\n",
       " 'yet</w>': 896,\n",
       " 'can</w>': 334,\n",
       " 'circle</w>': 750,\n",
       " 'WE</w>': 974,\n",
       " 'gu': 391,\n",
       " '4</w>': 149,\n",
       " 'text</w>': 709,\n",
       " 'en': 195,\n",
       " 'fil': 659,\n",
       " 'building</w>': 740,\n",
       " 'tions</w>': 701,\n",
       " 'mðŸ›‘post</w>': 813,\n",
       " 'sk': 442,\n",
       " 'yacineMTB</w>': 760,\n",
       " 'SH': 966,\n",
       " 'Fo': 956,\n",
       " '8': 24,\n",
       " 'O</w>': 117,\n",
       " 'Q</w>': 175,\n",
       " 'ter</w>': 444,\n",
       " 'too</w>': 487,\n",
       " 'ou': 187,\n",
       " 'ard</w>': 302,\n",
       " 'tes</w>': 553,\n",
       " '3': 19,\n",
       " 'la': 268,\n",
       " 'rces</w>': 899,\n",
       " 'ori': 611,\n",
       " 'ying</w>': 445,\n",
       " 'ðŸ›‘': 90,\n",
       " 'ingbo': 727,\n",
       " 'discord</w>': 749,\n",
       " 'a': 59,\n",
       " 'JA': 959,\n",
       " 'comple': 530,\n",
       " 'resou': 840,\n",
       " 'ta': 329,\n",
       " 'jec': 421,\n",
       " 'ther</w>': 377,\n",
       " 'red</w>': 836,\n",
       " 'ING</w>': 906,\n",
       " 'ss': 551,\n",
       " 'index</w>': 919,\n",
       " 'WA': 973,\n",
       " '.': 14,\n",
       " '@</w>': 168,\n",
       " 'work</w>': 419,\n",
       " 'read': 838,\n",
       " 'â€¦': 87,\n",
       " 'disc': 626,\n",
       " 'real': 426,\n",
       " 'pixqc</w>': 495,\n",
       " 'ste': 522,\n",
       " 'go': 241,\n",
       " 'mm': 597,\n",
       " 'L': 42,\n",
       " 'be': 238,\n",
       " 'infor': 694,\n",
       " 'y</w>': 98,\n",
       " 'Bible</w>': 752,\n",
       " 'u</w>': 162,\n",
       " 'W</w>': 134,\n",
       " 'KI': 960,\n",
       " 'B</w>': 151,\n",
       " 'hy': 665,\n",
       " 'O': 45,\n",
       " '%</w>': 166,\n",
       " 'also</w>': 610,\n",
       " 'progra': 723,\n",
       " 'gh': 348,\n",
       " 'should</w>': 894,\n",
       " 'an</w>': 235,\n",
       " 'ph': 814,\n",
       " 'no': 249,\n",
       " 'ression</w>': 697,\n",
       " 'L</w>': 125,\n",
       " 'cre': 586,\n",
       " 'tually</w>': 490,\n",
       " 'about</w>': 628,\n",
       " 'i': 67,\n",
       " 'th': 177,\n",
       " 'startupmil': 746,\n",
       " 'ss</w>': 295,\n",
       " '5am</w>': 947,\n",
       " 'per</w>': 305,\n",
       " '(</w>': 160,\n",
       " 'done</w>': 989,\n",
       " 'ep</w>': 588,\n",
       " 'yrs</w>': 831,\n",
       " 'skill</w>': 900,\n",
       " 'ahead</w>': 788,\n",
       " 'bl': 650,\n",
       " 'great</w>': 742,\n",
       " 'like</w>': 276,\n",
       " 'ould</w>': 310,\n",
       " 'words</w>': 867,\n",
       " 'we</w>': 397,\n",
       " 'V': 52,\n",
       " 'mar': 812,\n",
       " 'vi': 301,\n",
       " \"'</w>\": 176,\n",
       " 'ce': 326,\n",
       " 'qu': 423,\n",
       " 'tard</w>': 518,\n",
       " '\"</w>': 161,\n",
       " 'lya': 671,\n",
       " 'lle': 811,\n",
       " 'ludw': 887,\n",
       " 'rc': 441,\n",
       " 'US': 971,\n",
       " 'a</w>': 118,\n",
       " 'v</w>': 156,\n",
       " 'ha': 246,\n",
       " 'ga': 298,\n",
       " 'high</w>': 728,\n",
       " ',': 12,\n",
       " 'ac': 217,\n",
       " 'ps</w>': 192,\n",
       " '0u': 761,\n",
       " 'Im</w>': 773,\n",
       " 'most</w>': 627,\n",
       " 'K': 41,\n",
       " 'Kant': 774,\n",
       " 'un': 308,\n",
       " 'AN': 498,\n",
       " 'start</w>': 895,\n",
       " 'DONE</w>': 915,\n",
       " 'tal': 552,\n",
       " 'gðŸ›‘post</w>': 998,\n",
       " 'coding</w>': 615,\n",
       " 'ver': 242,\n",
       " '25': 945,\n",
       " 'ck': 980,\n",
       " 'ick</w>': 719,\n",
       " 'coder</w>': 856,\n",
       " 'will</w>': 689,\n",
       " 'session</w>': 525,\n",
       " 'aw': 975,\n",
       " 'l': 70,\n",
       " 'ink</w>': 519,\n",
       " 'multi': 914,\n",
       " 'le</w>': 255,\n",
       " 'it': 212,\n",
       " 'ftw': 995,\n",
       " 'andrew</w>': 935,\n",
       " ':</w>': 148,\n",
       " ';</w>': 153,\n",
       " 'mp': 247,\n",
       " 'er': 188,\n",
       " 'which</w>': 873,\n",
       " 'ABA': 768,\n",
       " 'ills</w>': 569,\n",
       " 'N</w>': 140,\n",
       " 'II': 403,\n",
       " 'post</w>': 202,\n",
       " 'ðŸ’ª': 89,\n",
       " 'er</w>': 196,\n",
       " ';': 27,\n",
       " 'Then</w>': 969,\n",
       " 'here</w>': 461,\n",
       " 'ls</w>': 336,\n",
       " 'ðŸ›‘post</w>': 205,\n",
       " '.</w>': 172,\n",
       " 'Wo': 500,\n",
       " 'pre': 514,\n",
       " 'th</w>': 516,\n",
       " 'own</w>': 547,\n",
       " 'day</w>': 491,\n",
       " 'if</w>': 262,\n",
       " 'A</w>': 155,\n",
       " 'ALL': 767,\n",
       " 'ble</w>': 458,\n",
       " 'gori': 807,\n",
       " '#': 3,\n",
       " 'takes</w>': 884,\n",
       " 'ing': 285,\n",
       " 'achi': 857,\n",
       " 'he': 232,\n",
       " 'm': 71,\n",
       " 'es</w>': 216,\n",
       " 'IV': 958,\n",
       " 'pers</w>': 678,\n",
       " 'x</w>': 112,\n",
       " '1</w>': 129,\n",
       " 'and</w>': 220,\n",
       " 'cu': 404,\n",
       " 'thurs</w>': 748,\n",
       " 'hr</w>': 371,\n",
       " 'notes</w>': 869,\n",
       " '6</w>': 135,\n",
       " '?': 29,\n",
       " 'e</w>': 107,\n",
       " 'h': 66,\n",
       " 'on</w>': 206,\n",
       " 'tial</w>': 850,\n",
       " 'experi': 638,\n",
       " 'https</w>': 199,\n",
       " 'Ti': 968,\n",
       " 'techniques</w>': 939,\n",
       " 'people</w>': 739,\n",
       " 'gi': 370,\n",
       " 'E': 35,\n",
       " 'R</w>': 132,\n",
       " 'ðŸ’ª</w>': 150,\n",
       " 'hel': 621,\n",
       " 'LLM</w>': 737,\n",
       " 'H': 38,\n",
       " 'there</w>': 607,\n",
       " 'y': 83,\n",
       " 'U</w>': 174,\n",
       " 'pi': 350,\n",
       " 'am</w>': 435,\n",
       " 'optima</w>': 925,\n",
       " 'from</w>': 367,\n",
       " 'tech': 861,\n",
       " 'always</w>': 849,\n",
       " '$</w>': 144,\n",
       " 'sðŸ›‘reply</w>': 472,\n",
       " 'one': 564,\n",
       " 'sa': 550,\n",
       " 'man</w>': 613,\n",
       " 'many</w>': 673,\n",
       " 'K</w>': 124,\n",
       " 'ction</w>': 794,\n",
       " 'gorithms</w>': 930,\n",
       " 'IN': 535,\n",
       " 'od</w>': 362,\n",
       " 'lz': 809,\n",
       " 'ted</w>': 412,\n",
       " 'fe': 291,\n",
       " 'id</w>': 592,\n",
       " 'mate</w>': 855,\n",
       " 'ch</w>': 311,\n",
       " 'su': 237,\n",
       " 'it</w>': 214,\n",
       " 'ding</w>': 328,\n",
       " 'ev': 438,\n",
       " 'fol': 590,\n",
       " 'hu': 664,\n",
       " 'ting</w>': 603,\n",
       " 'wor': 248,\n",
       " 'could</w>': 503,\n",
       " 'ler</w>': 670,\n",
       " 'com': 448,\n",
       " 'ru': 470,\n",
       " '*': 10,\n",
       " 'fi': 335,\n",
       " 'sni': 821,\n",
       " 'war': 826,\n",
       " 'super</w>': 383,\n",
       " '30u': 946,\n",
       " 'o</w>': 119,\n",
       " 'â€™': 86,\n",
       " 'al': 191,\n",
       " 'ludwigABA': 938,\n",
       " 'du': 389,\n",
       " 'want</w>': 828,\n",
       " 'what</w>': 331,\n",
       " 'sh': 411,\n",
       " 'ma': 213,\n",
       " 'R': 48,\n",
       " 'than': 833,\n",
       " 'l</w>': 99,\n",
       " 'he</w>': 540,\n",
       " 'LET': 636,\n",
       " 'bu': 240,\n",
       " 'thu': 606,\n",
       " 'ning</w>': 410,\n",
       " 'when</w>': 414,\n",
       " 'ant': 480,\n",
       " 'bab': 790,\n",
       " 'ama': 784,\n",
       " 'ry</w>': 818,\n",
       " 'HA': 772,\n",
       " 'f</w>': 113,\n",
       " 'si': 230,\n",
       " 'btw</w>': 577,\n",
       " 'effe': 992,\n",
       " 'p': 74,\n",
       " 'oun': 340,\n",
       " 'ideas</w>': 743,\n",
       " 'idea</w>': 902,\n",
       " 'follow</w>': 747,\n",
       " 'dy': 657,\n",
       " 'z': 84,\n",
       " 'fin': 506,\n",
       " 'ssed</w>': 910,\n",
       " 'kes</w>': 544,\n",
       " 'back</w>': 538,\n",
       " 'd</w>': 106,\n",
       " 'learn</w>': 492,\n",
       " 'll</w>': 511,\n",
       " 'e': 63,\n",
       " 'over': 600,\n",
       " 'den': 658,\n",
       " 'son': 683,\n",
       " 'yac': 691,\n",
       " 'ces</w>': 793,\n",
       " 'thought</w>': 911,\n",
       " 'j</w>': 116,\n",
       " 'ing</w>': 182,\n",
       " 'tons</w>': 686,\n",
       " '7</w>': 103,\n",
       " 'sting</w>': 427,\n",
       " 'chan': 882,\n",
       " 'cti': 539,\n",
       " 'best</w>': 711,\n",
       " 'cra': 654,\n",
       " 'mil': 545,\n",
       " 'ks</w>': 394,\n",
       " 'G</w>': 110,\n",
       " 'being</w>': 622,\n",
       " 'bly</w>': 651,\n",
       " 'sunsett': 710,\n",
       " 'ound</w>': 494,\n",
       " 'DE': 769,\n",
       " 'Wooltard</w>': 534,\n",
       " 'maybe</w>': 614,\n",
       " 'use</w>': 631,\n",
       " 'as': 369,\n",
       " 'yðŸ›‘reply</w>': 556,\n",
       " 'lot</w>': 619,\n",
       " 'T</w>': 122,\n",
       " 'le': 208,\n",
       " 'the</w>': 190,\n",
       " 'pda': 677,\n",
       " 'stuff</w>': 433,\n",
       " 'hi': 299,\n",
       " 'xed</w>': 830,\n",
       " 'VE': 536,\n",
       " 'think</w>': 835,\n",
       " '78</w>': 766,\n",
       " 'gra': 460,\n",
       " 'two</w>': 904,\n",
       " '12</w>': 944,\n",
       " 'ta</w>': 474,\n",
       " 'Bo': 951,\n",
       " 'ctu': 984,\n",
       " 'ere</w>': 321,\n",
       " 'ar': 185,\n",
       " 'actually</w>': 523,\n",
       " 'dw': 656,\n",
       " '\\U0001fae1': 91,\n",
       " 'yo': 223,\n",
       " 'ward</w>': 827,\n",
       " '`': 58,\n",
       " 'ack</w>': 428,\n",
       " 'sle': 684,\n",
       " 'ke</w>': 244,\n",
       " 'not</w>': 431,\n",
       " 'ybe</w>': 557,\n",
       " 'yn': 690,\n",
       " 'ingboard</w>': 924,\n",
       " 'game</w>': 633,\n",
       " 'around</w>': 842,\n",
       " 'self</w>': 921,\n",
       " '+</w>': 157,\n",
       " 'mo': 270,\n",
       " 'lyair</w>': 754,\n",
       " '=': 28,\n",
       " 'S': 49,\n",
       " 'v': 80,\n",
       " 'ly</w>': 180,\n",
       " 'their</w>': 864,\n",
       " 'thou': 558,\n",
       " 'pp': 284,\n",
       " 'YYYY': 574,\n",
       " 'its</w>': 282,\n",
       " 'proble': 724,\n",
       " 'problems</w>': 923,\n",
       " 'ten': 517,\n",
       " 'cal': 792,\n",
       " '-': 13,\n",
       " 'teod': 708,\n",
       " 'bit</w>': 652,\n",
       " 'sunsettler</w>': 758,\n",
       " 'trust</w>': 531,\n",
       " 'don</w>': 799,\n",
       " 'recom': 839,\n",
       " 'papers</w>': 730,\n",
       " 'de': 236,\n",
       " 'ff': 994,\n",
       " 'gre': 459,\n",
       " ':': 26,\n",
       " 'n': 72,\n",
       " 'ne': 256,\n",
       " 'used</w>': 878,\n",
       " 'idiot</w>': 927,\n",
       " '2': 18,\n",
       " 'ee': 437,\n",
       " 'po': 186,\n",
       " 've</w>': 234,\n",
       " 'ilya': 858,\n",
       " 'informa': 920,\n",
       " 'stre': 852,\n",
       " 'Kantor</w>': 929,\n",
       " 'der</w>': 504,\n",
       " 'things</w>': 637,\n",
       " 'o': 73,\n",
       " 'xt</w>': 555,\n",
       " 'U': 51,\n",
       " '16</w>': 943,\n",
       " 'better</w>': 452,\n",
       " 'resting</w>': 520,\n",
       " 'ant</w>': 378,\n",
       " 'BE': 950,\n",
       " 'tðŸ›‘reply</w>': 685,\n",
       " 'g</w>': 94,\n",
       " 'amp': 977,\n",
       " 'ped</w>': 548,\n",
       " 'll': 595,\n",
       " 'ssion</w>': 402,\n",
       " 'ded</w>': 505,\n",
       " 'ran': 681,\n",
       " 'P</w>': 146,\n",
       " 'pynch</w>': 932,\n",
       " '0': 16,\n",
       " 'TT': 646,\n",
       " 'idio': 753,\n",
       " 'thing': 559,\n",
       " 'im': 408,\n",
       " 'only</w>': 847,\n",
       " 'conce': 629,\n",
       " '0</w>': 114,\n",
       " 'm</w>': 93,\n",
       " 'che': 420,\n",
       " 'ic': 258,\n",
       " 'us</w>': 823,\n",
       " 'king</w>': 318,\n",
       " 'ill</w>': 382,\n",
       " 'TH': 584,\n",
       " 'pla': 468,\n",
       " 'word</w>': 866,\n",
       " 'ine': 560,\n",
       " 'pu': 363,\n",
       " 'stu': 380,\n",
       " 'BA': 582,\n",
       " 'con': 279,\n",
       " 'Ms</w>': 776,\n",
       " 'ate</w>': 787,\n",
       " 'does</w>': 872,\n",
       " '-</w>': 167,\n",
       " 'W': 53,\n",
       " 'Sam</w>': 783,\n",
       " 'exper': 632,\n",
       " 'ry': 680,\n",
       " 'H</w>': 111,\n",
       " 'way</w>': 527,\n",
       " 'hrs</w>': 808,\n",
       " 'at</w>': 227,\n",
       " 'ment</w>': 572,\n",
       " 'fici': 905,\n",
       " '77': 765,\n",
       " 'ig': 542,\n",
       " '@': 30,\n",
       " 'ener': 566,\n",
       " 'ite</w>': 568,\n",
       " 'DON': 583,\n",
       " 'YY': 332,\n",
       " 'in': 178,\n",
       " '?</w>': 159,\n",
       " '~</w>': 158,\n",
       " 'co</w>': 200,\n",
       " '<unk>': 0,\n",
       " 'time</w>': 379,\n",
       " 'teodor</w>': 757,\n",
       " 'useful</w>': 385,\n",
       " 'dnt</w>': 801,\n",
       " 'ous</w>': 846,\n",
       " 'mul': 573,\n",
       " 'andre': 841,\n",
       " '16': 368,\n",
       " 'imp': 464,\n",
       " 'niqu': 886,\n",
       " 'inte': 415,\n",
       " 'Z': 56,\n",
       " 'ws</w>': 554,\n",
       " 'M': 43,\n",
       " 'idi': 667,\n",
       " 'know</w>': 669,\n",
       " 'gizmobly</w>': 926,\n",
       " 'Y</w>': 136,\n",
       " 'el': 406,\n",
       " 'they</w>': 400,\n",
       " 'thms</w>': 834,\n",
       " 'head</w>': 484,\n",
       " 'mu': 319,\n",
       " '6': 22,\n",
       " 'lear': 316,\n",
       " 'k</w>': 123,\n",
       " 'ce</w>': 355,\n",
       " 'dom</w>': 871,\n",
       " '4': 20,\n",
       " 'sy': 515,\n",
       " 'te': 233,\n",
       " 'get</w>': 354,\n",
       " 'something</w>': 745,\n",
       " 'q': 75,\n",
       " 'techniqu': 937,\n",
       " 'ffe': 508,\n",
       " 'much</w>': 453,\n",
       " 'comp': 322,\n",
       " 'se': 221,\n",
       " 'ye': 425,\n",
       " 'shit</w>': 893,\n",
       " 'xqc</w>': 477,\n",
       " '&': 6,\n",
       " 'c': 61,\n",
       " 'every</w>': 358,\n",
       " 'startupmillyair</w>': 759,\n",
       " 'is': 393,\n",
       " 'sti': 471,\n",
       " 'working</w>': 868,\n",
       " 'go</w>': 439,\n",
       " '~': 85,\n",
       " 'ti': 193,\n",
       " 'thing</w>': 446,\n",
       " 'br': 789,\n",
       " 'dm</w>': 987,\n",
       " '5</w>': 131,\n",
       " 'ðŸ›‘reply</w>': 204,\n",
       " 'fun</w>': 486,\n",
       " 'itor</w>': 854,\n",
       " 'z</w>': 115,\n",
       " 'ally</w>': 269,\n",
       " 'is</w>': 211,\n",
       " 'D</w>': 109,\n",
       " 'do</w>': 257,\n",
       " 'tha': 605,\n",
       " 'boo': 732,\n",
       " 'ex': 294,\n",
       " 'cur': 891,\n",
       " 'D': 34,\n",
       " 'E</w>': 100,\n",
       " '+': 11,\n",
       " 't</w>': 92,\n",
       " 'you</w>': 224,\n",
       " '_': 57,\n",
       " 'i</w>': 108,\n",
       " 'so</w>': 273,\n",
       " 'tting</w>': 481,\n",
       " 'wel': 829,\n",
       " 'ved</w>': 688,\n",
       " 'cool</w>': 450,\n",
       " 'ya': 398,\n",
       " 'chess</w>': 533,\n",
       " 'over</w>': 675,\n",
       " 'sleep</w>': 755,\n",
       " 'gar': 997,\n",
       " 'to</w>': 194,\n",
       " 'IT</w>': 640,\n",
       " 'one</w>': 352,\n",
       " 'hen</w>': 366,\n",
       " 'igABA': 909,\n",
       " 'ble': 502,\n",
       " 'asi': 785,\n",
       " 'IIII': 890,\n",
       " 'lz78</w>': 931,\n",
       " 'pyn': 817,\n",
       " 'ord</w>': 702,\n",
       " 'OM': 780,\n",
       " 'projec': 489,\n",
       " 'fro': 359,\n",
       " 'or': 198,\n",
       " 're</w>': 375,\n",
       " 'x': 82,\n",
       " 'bc</w>': 537,\n",
       " 'bad</w>': 979,\n",
       " 'new</w>': 488,\n",
       " 'mobly</w>': 874,\n",
       " '1': 17,\n",
       " 'P': 46,\n",
       " 'co': 215,\n",
       " 'cracked</w>': 497,\n",
       " 'ludwigABAP</w>': 940,\n",
       " 'ON</w>': 962,\n",
       " 'T': 50,\n",
       " 'me</w>': 222,\n",
       " 'iz': 510,\n",
       " 'ach</w>': 616,\n",
       " 'GE': 771,\n",
       " 'ct</w>': 982,\n",
       " '$': 4,\n",
       " 's': 77,\n",
       " '=</w>': 142,\n",
       " 'wa': 263,\n",
       " 'put</w>': 576,\n",
       " 'li': 218,\n",
       " 'per': 467,\n",
       " 'w</w>': 121,\n",
       " 'some': 571,\n",
       " 'il': 225,\n",
       " 'any</w>': 562,\n",
       " 'for': 390,\n",
       " 'lol</w>': 707,\n",
       " '7': 23,\n",
       " 'out</w>': 399,\n",
       " 'UL': 970,\n",
       " 'rs</w>': 396,\n",
       " 'ts</w>': 250,\n",
       " 'ghts</w>': 735,\n",
       " 'onna</w>': 913,\n",
       " 'ang': 561,\n",
       " 'ro': 228,\n",
       " 'programm': 922,\n",
       " 'ers</w>': 341,\n",
       " 'yacineMT': 756,\n",
       " 'val': 687,\n",
       " 'opti': 733,\n",
       " 'VERY</w>': 907,\n",
       " 'ways</w>': 721,\n",
       " 'dist': 722,\n",
       " 'ction': 908,\n",
       " 'be</w>': 287,\n",
       " 'LLMs</w>': 889,\n",
       " 'lon': 465,\n",
       " 'ments</w>': 725,\n",
       " 'really</w>': 696,\n",
       " 'h</w>': 105,\n",
       " 'au': 456,\n",
       " 'ld</w>': 254,\n",
       " 'had</w>': 715,\n",
       " 'F': 36,\n",
       " 'r</w>': 97,\n",
       " 'row</w>': 859,\n",
       " 'world</w>': 624,\n",
       " 'av': 457,\n",
       " 'beta</w>': 623,\n",
       " 'games</w>': 881,\n",
       " 't': 78,\n",
       " \"'\": 7,\n",
       " 'N': 44,\n",
       " 'paper</w>': 729,\n",
       " 'pro': 271,\n",
       " 'nsett': 674,\n",
       " 'ff</w>': 347,\n",
       " 'ys</w>': 478,\n",
       " '8T': 949,\n",
       " 'dG': 796,\n",
       " 'ging</w>': 806,\n",
       " 'C</w>': 139,\n",
       " 'the': 239,\n",
       " 'crac': 436,\n",
       " 'come</w>': 449,\n",
       " 'lif': 524,\n",
       " 'been</w>': 863,\n",
       " 'got</w>': 485,\n",
       " 'ppl</w>': 528,\n",
       " 'jo': 593,\n",
       " 'us': 289,\n",
       " 'inde': 693,\n",
       " 'some</w>': 401,\n",
       " 'n</w>': 101,\n",
       " '/</w>': 164,\n",
       " 'lu': 374,\n",
       " 'ro</w>': 549,\n",
       " 'side</w>': 570,\n",
       " 'ineMT': 744,\n",
       " 'bal</w>': 978,\n",
       " 'di': 264,\n",
       " 'tra': 822,\n",
       " 'ked</w>': 373,\n",
       " 'sðŸ›‘post</w>': 820,\n",
       " 'pe': 422,\n",
       " 'X': 54,\n",
       " 'Ma': 642,\n",
       " 'oney</w>': 912,\n",
       " 'easi': 993,\n",
       " 'see</w>': 706,\n",
       " 'post': 844,\n",
       " 'oppo': 885,\n",
       " 'ALLY</w>': 928,\n",
       " 'read</w>': 608,\n",
       " 'lðŸ›‘post</w>': 810,\n",
       " 'for</w>': 231,\n",
       " 'Ch': 954,\n",
       " 'aga': 786,\n",
       " 'â€™</w>': 128,\n",
       " 'fic': 507,\n",
       " '30</w>': 764,\n",
       " 'make</w>': 482,\n",
       " 'pri': 676,\n",
       " '07</w>': 941,\n",
       " 'fun': 714,\n",
       " 'wh': 266,\n",
       " 'ri': 272,\n",
       " 'down</w>': 720,\n",
       " 'CA': 952,\n",
       " '\"': 2,\n",
       " 'UP</w>': 972,\n",
       " 'how</w>': 344,\n",
       " 'ðŸ‘Œ': 88,\n",
       " 'tr': 330,\n",
       " 'pmil': 679,\n",
       " 'en</w>': 357,\n",
       " 'G': 37,\n",
       " 'ad</w>': 278,\n",
       " 'IK': 957,\n",
       " 'guy</w>': 738,\n",
       " '20': 581,\n",
       " 'algo</w>': 848,\n",
       " 'long</w>': 579,\n",
       " '!</w>': 170,\n",
       " 'ving</w>': 475,\n",
       " 'ough</w>': 563,\n",
       " 'dda': 988,\n",
       " 'as</w>': 346,\n",
       " 'sto': 567,\n",
       " 'projects</w>': 635,\n",
       " 'b</w>': 138,\n",
       " 'ron': 601,\n",
       " 'real</w>': 695,\n",
       " 'ves</w>': 825,\n",
       " 'LE': 499,\n",
       " 'sou': 602,\n",
       " 'ver</w>': 339,\n",
       " 'ent</w>': 365,\n",
       " 'ffici': 996,\n",
       " 'ad': 283,\n",
       " 'ST': 782,\n",
       " 're': 179,\n",
       " 'ca': 333}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ther'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([377])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
