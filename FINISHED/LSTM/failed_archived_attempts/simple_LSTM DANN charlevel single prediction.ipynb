{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import optax\n",
    "from tokenizers import CharBPETokenizer\n",
    "import functools\n",
    "import time\n",
    "\n",
    "\n",
    "gpu_device = jax.device_get('gpu')[0]\n",
    "cpu_device = jax.device_get('cpu')[0]\n",
    "# LSTM\n",
    "# xs = B, input_size = B, T, C\n",
    "# h = c = y = B, output_size = B, T, logits_size = B, T, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 155\n",
      "dog [66 77 69] dog\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "with open('data/dnbt_posts.txt', 'r') as file:\n",
    "  dataset = file.read()\n",
    "\n",
    "# tokenize\n",
    "vocab = sorted(list(set(dataset)))\n",
    "print(\"vocab length:\", len(vocab))\n",
    "\n",
    "token_to_char = dict(enumerate(vocab))\n",
    "char_to_token = dict([(v, k) for k, v in token_to_char.items()])\n",
    "decode = lambda tokens: \"\".join([token_to_char[int(token)] for token in tokens])\n",
    "encode = lambda chars: jnp.array([char_to_token[c] for c in chars])\n",
    "\n",
    "print(\"dog\", encode(\"dog\"), decode(encode(\"dog\")))\n",
    "\n",
    "dataset_tokens = encode(dataset)\n",
    "split_ratio = 0.8\n",
    "train_tokens = dataset_tokens[:int(len(dataset_tokens)*split_ratio)]\n",
    "test_tokens = dataset_tokens[int(len(dataset_tokens)*split_ratio):]\n",
    "del dataset\n",
    "del dataset_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layers = 1\n",
    "sequence_length = 25# 100\n",
    "model_size = 64# 512\n",
    "\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "\n",
    "\n",
    "# init LSTM params\n",
    "def init_LSTM_params(key, lstm_layers, input_size, model_size, output_size):\n",
    "  param_sets = 8 # manual, idc\n",
    "  keys = random.split(key, param_sets*lstm_layers + 2)\n",
    "  hxconcat_size = model_size + model_size\n",
    "  he = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / shape[0])\n",
    "  # supposedly xavier is better for networks using tanh\n",
    "  xavier = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / (shape[0] + shape[1]))\n",
    "  params = [\n",
    "    {\n",
    "      \"wU\" : xavier(keys[param_sets*i + 0], (hxconcat_size, model_size)),\n",
    "      \"bU\" : jnp.zeros((model_size,)),\n",
    "      \"wC\" : xavier(keys[param_sets*i + 6], (hxconcat_size, model_size)),\n",
    "      \"bC\" : jnp.zeros((model_size,)),\n",
    "      \"wF1\": xavier(keys[param_sets*i + 1], (hxconcat_size, model_size)),\n",
    "      \"bF1\": jnp.zeros((model_size,)),\n",
    "      \"wF2\": xavier(keys[param_sets*i + 2], (hxconcat_size, model_size)),\n",
    "      \"bF2\": jnp.zeros((model_size,)),\n",
    "      \"wO\" : xavier(keys[param_sets*i + 3], (hxconcat_size, model_size)),\n",
    "      \"bO\" : jnp.zeros((model_size,)),\n",
    "      \"h0\" : jnp.zeros((model_size,)),\n",
    "      \"c0\" : jnp.zeros((model_size,)),\n",
    "      #\"h0\" : random.normal(keys[param_sets*i + 4], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "      #\"c0\" : random.normal(keys[param_sets*i + 5], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "    }\n",
    "    for i in range(lstm_layers)\n",
    "  ]\n",
    "  params[0].update(\n",
    "    {\n",
    "    # then embedding table weight and bias\n",
    "    \"wEM\" : he(keys[param_sets*(param_sets - 1) + 2], (input_size, model_size)),\n",
    "    \"bEM\" : jnp.zeros((model_size,)),\n",
    "\n",
    "  })\n",
    "  params[-1].update(\n",
    "    {\n",
    "      # this is for the y layer, which i am probably imlementing wrong.\n",
    "      \"wY1\" : he(keys[param_sets*(lstm_layers-1) + 4], (model_size, output_size)),\n",
    "      \"bY1\" : jnp.zeros((output_size,)),\n",
    "    }\n",
    "  )\n",
    "  return params\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def dropout(dropout_key, original_tensor, dropout_rate):\n",
    "  # generate random of same shape\n",
    "  dropout_probs = random.uniform(dropout_key, shape=original_tensor.shape)\n",
    "  # mask = random < dropout_rate\n",
    "  mask = (dropout_probs > dropout_rate) / (1 - dropout_rate) # scale to keep avg the same\n",
    "  return original_tensor * mask\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def lstm_step(step_dropout_key, lstm_layer_params, layer_h, layer_c, current_xt, dropout_rate):\n",
    "  hxconcat = jax.lax.concatenate([layer_h, current_xt], dimension=1) #B, h ++ B, C => B, h+c\n",
    "  # update gate\n",
    "  update = jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wU\"] + lstm_layer_params[\"bU\"])\n",
    "  #update = dropout(step_dropout_keys[0], update, dropout_rate)\n",
    "  candidate = jax.nn.tanh(hxconcat @ lstm_layer_params[\"wC\"] + lstm_layer_params[\"bC\"])\n",
    "  #candidate = dropout(step_dropout_keys[1], candidate, dropout_rate)\n",
    "\n",
    "  # forget gate\n",
    "  forget = jax.nn.sigmoid(\n",
    "              hxconcat @ lstm_layer_params[\"wF1\"] + lstm_layer_params[\"bF1\"]\n",
    "            ) * jax.nn.tanh(\n",
    "              hxconcat @ lstm_layer_params[\"wF2\"] + lstm_layer_params[\"bF2\"]\n",
    "            )\n",
    "\n",
    "  # update c with update and forget\n",
    "  layer_c = layer_c + update * candidate + forget # (batch, c) => (batch, c)\n",
    "\n",
    "  # output\n",
    "  layer_h = jax.nn.tanh(layer_c) * jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wO\"] + lstm_layer_params[\"bO\"]) # (B, model_size)\n",
    "\n",
    "  next_layer_xt = dropout(step_dropout_key, layer_h, dropout_rate) # the next layer's input x is the current layer's hidden state\n",
    "  # karpathy: dropout after EACH LAYER not several times in the block. lol.\n",
    "\n",
    "  return (layer_h, layer_c), next_layer_xt\n",
    "\n",
    "\n",
    "# LSTM forward\n",
    "import functools\n",
    "@functools.partial(jax.jit, static_argnames=['dropout_rate', 'lstm_layers'])\n",
    "def lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate, lstm_layers=lstm_layers):\n",
    "  batches = xembeds_batch.shape[0]\n",
    "  lstm_layers = len(lstm_params)\n",
    "  # initialize h and c as random/learnable params\n",
    "  #h = jnp.tile(lstm_params[0][\"h0\"], (batches, lstm_layers, 1)) # B, lstm_layer, h_size\n",
    "  #c = jnp.tile(lstm_params[0][\"c0\"], (batches, lstm_layers, 1)) # B, lstm_layer, c_size\n",
    "  # wait.. these are the same for all of the layers.. maybe they shouldn't be\n",
    "  T = xembeds_batch.shape[1]\n",
    "  # take xembeds_batch and pass each xt through the same SINGULAR block. don't update the weight layer. there is only one layer.\n",
    "  dropout_keys = random.split(dropout_key, lstm_layers)\n",
    "\n",
    "  # for each layer:\n",
    "    # scan over xt\n",
    "    # carry : h, c\n",
    "    # a: xt\n",
    "    # b: h,c\n",
    "    # f = lambda ((h, c), xt) : lstm_step(h, c, xt, everything else) => h, c\n",
    "    # scans over xt\n",
    "    # for next layer: xt = h of previous layer. h = h0 and c = c0\n",
    "  \n",
    "  current_embeddings_batch = jnp.transpose(xembeds_batch, (1, 0, 2)) # B, T, C => T, B, C\n",
    "    # The reason for this is that jax.lax.scan only uses the leading dim. why? idk. its dumb, it needs an axis arg so i can scan over whatever\n",
    "\n",
    "  for lstm_layer in range(lstm_layers):\n",
    "    h = jnp.tile(lstm_params[lstm_layer][\"h0\"], (batches, 1))\n",
    "    c = jnp.tile(lstm_params[lstm_layer][\"c0\"], (batches, 1))\n",
    "    layer_dropout_key = dropout_keys[lstm_layer] # it doesnt matter if this is the same across all layers\n",
    "    # scan should be inexpensive since layer size is small while t size is usually LARGE\n",
    "    # scan :: (c -> a -> (c, b)) -> c -> [a] -> (c, [b])\n",
    "    # scan :: scanfunc -> h_and_c -> xs -> (h_and_c_final, hs_to_be_used_as_input_xt_in_next_layer)\n",
    "    # scanfunc :: (c -> a -> (c, b))\n",
    "    scanfunc = lambda hc, xt : lstm_step(layer_dropout_key, lstm_params[lstm_layer], hc[0], hc[1], xt, dropout_rate)\n",
    "      # for xs: scan along the t dimension! it scans along B by default\n",
    "      # to fix this, we transpose xs with jnp.transpose(current_embeddings_batch, (1, 0, 2))\n",
    "    current_embeddings_batch = jax.lax.scan(scanfunc, (h, c), current_embeddings_batch)[1] # (c, [b]) => [b] ==> B, T, C\n",
    "  \n",
    "\n",
    "  # finally turn current_embeddings_batch into ys (logits)\n",
    "  hs = jnp.transpose(current_embeddings_batch, (1, 0, 2)) # T, B, C => B, T, C\n",
    "  ys = hs @ lstm_params[-1]['wY1'] + lstm_params[-1][\"bY1\"] # B, T, model_size => B, T, vocab_size\n",
    "  return ys\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logits = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def final_token_loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch) # (B, T, C)\n",
    "  logit = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)[:, -1] # get last logit in each B. (B, vocab_size)\n",
    "  vocab_size = logit.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1) # get last y (the target). (B, vocab_size)\n",
    "  logprobs = jax.nn.log_softmax(logit, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1) # (B, vocab_size) => (B,)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses) # num\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "\n",
    "jitted_backwards_loss = jax.jit(jax.value_and_grad(final_token_loss, argnums=1), static_argnames=[\"dropout_rate\"])\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['vocab_size'])\n",
    "def embed(lstm_params, xtokens, vocab_size=len(vocab)):\n",
    "  xs_one_hot = jax.nn.one_hot(xtokens, vocab_size, axis=-1) #B, T, vocab_size\n",
    "  activations = xs_one_hot @ lstm_params[0][\"wEM\"] + lstm_params[0][\"bEM\"]\n",
    "  return activations\n",
    "\n",
    "\n",
    "lr = 1e-1\n",
    "lr_decay = 0.97\n",
    "decay_after = 10\n",
    "decay_every = 5\n",
    "optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "\n",
    "\n",
    "# make optimizer a static arg in jit or it breaks\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate):\n",
    "  step_loss, grads = jitted_backwards_loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate)\n",
    "  param_updates, updated_opt_state = optimizer.update(grads, opt_state, lstm_params)\n",
    "  updated_lstm_params = optax.apply_updates(lstm_params, param_updates) \n",
    "  return updated_lstm_params, updated_opt_state, step_loss, grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (1) | \"ke your own shakes w yogur\"\n",
      "PRED   (1) | \"ke your own shakes w yogut\"\n",
      "step (0, 49) || samples/sec: 1974 || loss: 3.4018 || val_loss: 3.4366 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (0, 50) || samples/sec: 1169437 || loss: 3.3992 || val_loss: 2.9033 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (0, 100) || samples/sec: 29459 || loss: 3.2284 || val_loss: 2.6206 val_acc: 0.2000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (0, 150) || samples/sec: 26271 || loss: 3.1477 || val_loss: 3.3975 val_acc: 0.0667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (1, 191) || samples/sec: 32535 || loss: 3.1201 || val_loss: 2.6407 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (1, 211) || samples/sec: 70701 || loss: 3.1065 || val_loss: 2.7552 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (1, 261) || samples/sec: 29920 || loss: 3.0743 || val_loss: 2.7255 val_acc: 0.2000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (1, 311) || samples/sec: 28975 || loss: 3.0528 || val_loss: 3.3022 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (2, 352) || samples/sec: 34174 || loss: 3.0479 || val_loss: 2.6658 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (2, 372) || samples/sec: 69027 || loss: 3.0452 || val_loss: 2.7223 val_acc: 0.4000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (2, 422) || samples/sec: 29109 || loss: 3.0372 || val_loss: 2.6133 val_acc: 0.2333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imple\"\n",
      "step (2, 472) || samples/sec: 29524 || loss: 3.0305 || val_loss: 3.2127 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other n\"\n",
      "step (3, 513) || samples/sec: 36938 || loss: 3.0304 || val_loss: 2.7242 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      ret\"\n",
      "step (3, 533) || samples/sec: 70574 || loss: 3.0295 || val_loss: 2.7004 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (3, 583) || samples/sec: 29809 || loss: 3.0258 || val_loss: 2.5706 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imple\"\n",
      "step (3, 633) || samples/sec: 29278 || loss: 3.0243 || val_loss: 3.3488 val_acc: 0.2000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other  \"\n",
      "step (4, 674) || samples/sec: 36218 || loss: 3.0276 || val_loss: 2.8095 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (4, 694) || samples/sec: 71558 || loss: 3.0296 || val_loss: 3.3990 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (4, 744) || samples/sec: 29820 || loss: 3.0351 || val_loss: 2.5697 val_acc: 0.2333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imple\"\n",
      "step (4, 794) || samples/sec: 29607 || loss: 3.0396 || val_loss: 3.3925 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (5, 835) || samples/sec: 36288 || loss: 3.0507 || val_loss: 2.7782 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      rep\"\n",
      "step (5, 855) || samples/sec: 76371 || loss: 3.0583 || val_loss: 3.7534 val_acc: 0.1333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it wase\"\n",
      "step (5, 905) || samples/sec: 29358 || loss: 3.0724 || val_loss: 2.6763 val_acc: 0.2333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (5, 955) || samples/sec: 29232 || loss: 3.0800 || val_loss: 3.4175 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (6, 996) || samples/sec: 36520 || loss: 3.0879 || val_loss: 2.8895 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (6, 1016) || samples/sec: 70645 || loss: 3.0936 || val_loss: 4.2371 val_acc: 0.2333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (6, 1066) || samples/sec: 29174 || loss: 3.0995 || val_loss: 2.6635 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (6, 1116) || samples/sec: 29481 || loss: 3.0997 || val_loss: 3.5982 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (7, 1157) || samples/sec: 34435 || loss: 3.0998 || val_loss: 2.8704 val_acc: 0.3667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (7, 1177) || samples/sec: 70467 || loss: 3.1016 || val_loss: 3.8072 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (7, 1227) || samples/sec: 28851 || loss: 3.0985 || val_loss: 2.5470 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could imply\"\n",
      "step (7, 1277) || samples/sec: 27820 || loss: 3.0962 || val_loss: 3.6734 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (8, 1318) || samples/sec: 33262 || loss: 3.0955 || val_loss: 2.8161 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (8, 1338) || samples/sec: 73229 || loss: 3.0967 || val_loss: 3.0505 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (8, 1388) || samples/sec: 29268 || loss: 3.0945 || val_loss: 2.4190 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (8, 1438) || samples/sec: 28960 || loss: 3.0930 || val_loss: 3.9096 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (9, 1479) || samples/sec: 35002 || loss: 3.0949 || val_loss: 2.7103 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (9, 1499) || samples/sec: 75674 || loss: 3.0967 || val_loss: 3.1206 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (9, 1549) || samples/sec: 29632 || loss: 3.0972 || val_loss: 2.6499 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (9, 1599) || samples/sec: 28910 || loss: 3.0959 || val_loss: 3.6507 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (10, 1640) || samples/sec: 35465 || loss: 3.0973 || val_loss: 2.7068 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (10, 1660) || samples/sec: 74643 || loss: 3.0983 || val_loss: 3.1394 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (10, 1710) || samples/sec: 29539 || loss: 3.0974 || val_loss: 2.6178 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (10, 1760) || samples/sec: 29803 || loss: 3.0966 || val_loss: 3.5342 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (11, 1801) || samples/sec: 36298 || loss: 3.0970 || val_loss: 2.7568 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (11, 1821) || samples/sec: 71420 || loss: 3.0978 || val_loss: 3.0978 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (11, 1871) || samples/sec: 30814 || loss: 3.0963 || val_loss: 2.6249 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (11, 1921) || samples/sec: 30505 || loss: 3.0948 || val_loss: 3.4750 val_acc: 0.1333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (12, 1962) || samples/sec: 38693 || loss: 3.0948 || val_loss: 2.7652 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (12, 1982) || samples/sec: 73197 || loss: 3.0954 || val_loss: 2.9938 val_acc: 0.3000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (12, 2032) || samples/sec: 30529 || loss: 3.0942 || val_loss: 2.6437 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (12, 2082) || samples/sec: 30083 || loss: 3.0916 || val_loss: 3.3476 val_acc: 0.1667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (13, 2123) || samples/sec: 37146 || loss: 3.0915 || val_loss: 2.8107 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (13, 2143) || samples/sec: 72209 || loss: 3.0914 || val_loss: 2.9380 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (13, 2193) || samples/sec: 30632 || loss: 3.0894 || val_loss: 2.6479 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (13, 2243) || samples/sec: 25184 || loss: 3.0878 || val_loss: 3.4418 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (14, 2284) || samples/sec: 35891 || loss: 3.0872 || val_loss: 2.8002 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (14, 2304) || samples/sec: 68129 || loss: 3.0869 || val_loss: 2.9328 val_acc: 0.3333 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (14, 2354) || samples/sec: 29208 || loss: 3.0848 || val_loss: 2.5872 val_acc: 0.2667 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (14, 2404) || samples/sec: 29203 || loss: 3.0826 || val_loss: 3.4236 val_acc: 0.1000 || LR = 0.100000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (15, 2445) || samples/sec: 2566 || loss: 3.0816 || val_loss: 2.7938 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (15, 2465) || samples/sec: 70566 || loss: 3.0812 || val_loss: 2.8681 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (15, 2515) || samples/sec: 28426 || loss: 3.0800 || val_loss: 2.5680 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (15, 2565) || samples/sec: 28779 || loss: 3.0783 || val_loss: 3.4831 val_acc: 0.0333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (16, 2606) || samples/sec: 33572 || loss: 3.0783 || val_loss: 2.8231 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (16, 2626) || samples/sec: 66550 || loss: 3.0784 || val_loss: 3.1317 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (16, 2676) || samples/sec: 28743 || loss: 3.0770 || val_loss: 2.5653 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (16, 2726) || samples/sec: 28844 || loss: 3.0757 || val_loss: 3.5327 val_acc: 0.0333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (17, 2767) || samples/sec: 35752 || loss: 3.0760 || val_loss: 2.8053 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (17, 2787) || samples/sec: 71890 || loss: 3.0764 || val_loss: 3.0149 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (17, 2837) || samples/sec: 29437 || loss: 3.0755 || val_loss: 2.5622 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (17, 2887) || samples/sec: 28634 || loss: 3.0741 || val_loss: 3.5414 val_acc: 0.0333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (18, 2928) || samples/sec: 35964 || loss: 3.0739 || val_loss: 2.8060 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (18, 2948) || samples/sec: 69854 || loss: 3.0740 || val_loss: 3.3283 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (18, 2998) || samples/sec: 29293 || loss: 3.0729 || val_loss: 2.5495 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (18, 3048) || samples/sec: 28856 || loss: 3.0717 || val_loss: 3.5239 val_acc: 0.1000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (19, 3089) || samples/sec: 33381 || loss: 3.0721 || val_loss: 2.7998 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (19, 3109) || samples/sec: 66816 || loss: 3.0723 || val_loss: 2.9394 val_acc: 0.3000 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (19, 3159) || samples/sec: 25411 || loss: 3.0713 || val_loss: 2.5412 val_acc: 0.3333 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (19, 3209) || samples/sec: 27652 || loss: 3.0704 || val_loss: 3.5477 val_acc: 0.0667 || LR = 0.097000\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (20, 3250) || samples/sec: 35599 || loss: 3.0703 || val_loss: 2.8137 val_acc: 0.3333 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (20, 3270) || samples/sec: 69281 || loss: 3.0703 || val_loss: 2.9438 val_acc: 0.3000 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \" far Used to think it was \"\n",
      "PRED   (1) | \" far Used to think it was \"\n",
      "step (20, 3320) || samples/sec: 29953 || loss: 3.0691 || val_loss: 2.5307 val_acc: 0.3333 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \" point where I could imple\"\n",
      "PRED   (1) | \" point where I could impl \"\n",
      "step (20, 3370) || samples/sec: 30284 || loss: 3.0680 || val_loss: 3.5338 val_acc: 0.0667 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \"ly arent Do things other p\"\n",
      "PRED   (1) | \"ly arent Do things other t\"\n",
      "step (21, 3411) || samples/sec: 34985 || loss: 3.0681 || val_loss: 2.8074 val_acc: 0.3333 || LR = 0.094090\n",
      "\n",
      "TARGET (1) | \" in the hamptons🛑      rep\"\n",
      "PRED   (1) | \" in the hamptons🛑      re \"\n",
      "step (21, 3431) || samples/sec: 73897 || loss: 3.0682 || val_loss: 3.2990 val_acc: 0.3000 || LR = 0.094090\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m train_data_idx \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m*\u001b[39msequence_length\u001b[38;5;241m*\u001b[39mtrain_batch_size\n\u001b[1;32m     31\u001b[0m next_train_data_idx \u001b[38;5;241m=\u001b[39m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39msequence_length\u001b[38;5;241m*\u001b[39mtrain_batch_size\n\u001b[0;32m---> 32\u001b[0m xtokens_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnext_train_data_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#(B, T)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m ytokens_batch \u001b[38;5;241m=\u001b[39m train_tokens[train_data_idx\u001b[38;5;241m+\u001b[39msequence_length:next_train_data_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:sequence_length] \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m dropout_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(epoch\u001b[38;5;241m*\u001b[39msteps \u001b[38;5;241m+\u001b[39m i) \u001b[38;5;66;03m# unique for every step\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:1249\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(operand, new_sizes, dimensions)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m   dyn_shape, static_new_sizes \u001b[38;5;241m=\u001b[39m _extract_tracers_dyn_shape(new_sizes)\n\u001b[0;32m-> 1249\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreshape_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdyn_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstatic_new_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msame_dims\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/core.py:438\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    436\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    437\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/core.py:442\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    441\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 442\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/core.py:955\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    953\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py:91\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     89\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "# set up lstm params\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "opt_state = optimizer.init(lstm_params)\n",
    "\n",
    "\n",
    "# train\n",
    "# for now just overfit on small sample idk lol\n",
    "train_batch_size = 100\n",
    "val_batch_size = 30\n",
    "\n",
    "dropout_rate = 0\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "print_every = 50\n",
    "j = 0\n",
    "losses = []\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "  if epoch > decay_after:\n",
    "    if epoch % decay_every == 0:\n",
    "      lr *= lr_decay\n",
    "      opt_state.hyperparams['learning_rate'] = lr\n",
    "  steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "  for i in range(steps): # probably wrong but w/e\n",
    "    # train\n",
    "    # B, T where T = sequence_length\n",
    "    train_data_idx = i*sequence_length*train_batch_size\n",
    "    next_train_data_idx = (i+1)*sequence_length*train_batch_size\n",
    "    xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "    ytokens_batch = train_tokens[train_data_idx+sequence_length:next_train_data_idx+1:sequence_length] # (B,)\n",
    "\n",
    "    dropout_key = random.PRNGKey(epoch*steps + i) # unique for every step\n",
    "\n",
    "    lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate)\n",
    "\n",
    "    j += 1\n",
    "    losses.append(step_loss)\n",
    "\n",
    "    if j % print_every == 0:\n",
    "      end = time.time()\n",
    "      duration = end - start\n",
    "      # train inference example (no dropout)\n",
    "      xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "      last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0)[:, -1] # B, C\n",
    "      prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "      # val batch\n",
    "      j = i % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "      val_idx = j*val_batch_size*sequence_length\n",
    "      next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "      xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "      ytokens_val_batch = test_tokens[val_idx + sequence_length:next_val_idx+1:sequence_length]\n",
    "      xembeds_val_batch = embed(lstm_params, xtokens_val_batch)\n",
    "      \n",
    "      last_logits_val_batch = lstm_forward(dropout_key, lstm_params, xembeds_val_batch, 0)[:, -1] # (B, C)\n",
    "      prediction_val_batch = jnp.argmax(last_logits_val_batch, axis=-1) # (B,)\n",
    "      ys_onehot = jax.nn.one_hot(ytokens_val_batch, len(vocab), axis=-1) # (B, vocab_size)\n",
    "      logprobs = jax.nn.log_softmax(last_logits_val_batch, axis=-1) # (B, vocab_size)\n",
    "      crossentropies = -jnp.sum(ys_onehot*logprobs,axis=-1) # (B,)\n",
    "      val_loss = jnp.mean(crossentropies) # num\n",
    "      val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "      x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "      y = decode([ytokens_batch[0]]).replace('\\n', ' ')\n",
    "      yhat = decode([prediction_batch[0]]).replace('\\n', ' ')\n",
    "      #print(epoch, epoch * samples + i, f\"{step_loss:1.4f}\", \"pred:\", x, \"=>\", y, \"?=\", yhat)\n",
    "      print(f'TARGET ({len(y)}) | \"{x}{y}\"')\n",
    "      print(f'PRED   ({len(yhat)}) | \"{x}{yhat}\"')\n",
    "      print(f\"step {(epoch, epoch * steps + i)} || samples/sec: {train_batch_size*print_every/(duration):0.0f} || loss: {sum(losses)/len(losses):1.4f} || val_loss: {val_loss:1.4f} val_acc: {val_accuracy:1.4f} || LR = {opt_state.hyperparams['learning_rate']:0.6f}\" )\n",
    "      print()\n",
    "      start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reply: @APIGuy forer eving being x youre at they theursing this they for mudy hint a staully\n",
      "\n",
      "Ave word wookin on stapface in'at what ban\n",
      "I fearny https://t.co/pLi&LHLRED oh an @cenderenters into taybey really of dato i to some googing code bagel id of this cerast you siwle\n",
      "\n",
      "LLe too fis hex mistyress tleed liter interestings Coive to, hrave to by what bothing stuch (… monsing, to coding, hofrul https://t.co/kuILDhtnB bnigh caring the wress you hock stulf more they they this and llike🛑"
     ]
    }
   ],
   "source": [
    "def inference(key, chars):\n",
    "  xtokens = encode(chars)[None, :]\n",
    "  xembed = embed(lstm_params, xtokens) # artificial single batch\n",
    "  logits = lstm_forward(key, lstm_params, xembed, 0)[0][-1] # logits of the first B and last T in the B T C. should be (C,)\n",
    "  yhattokens = random.choice(key, a=logits.shape[0], p=jax.nn.softmax(logits)) # no need for axis=-1 since logits are (C,)\n",
    "  sequence = yhattokens\n",
    "  return sequence\n",
    "\n",
    "steps = 1000\n",
    "import time\n",
    "seed = int(time.time())\n",
    "keys = random.split(random.PRNGKey(seed), steps)\n",
    "temperature = 0.5\n",
    "text =  \"\\n\"*50 + 'reply: @APIGuy'\n",
    "print(text.replace('\\n\\n', ''), end='')\n",
    "for i in range(steps):\n",
    "  next_token = inference(keys[i], text[-sequence_length:])\n",
    "  next_char = decode([next_token])[-1]\n",
    "  if next_char == '🛑':\n",
    "    print(next_char, end='')\n",
    "    break\n",
    "  text += next_char\n",
    "  print(next_char, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bC': Array(1.4255815e-07, dtype=float32),\n",
       "  'bEM': Array(0.0001805, dtype=float32),\n",
       "  'bF1': Array(2.783328e-06, dtype=float32),\n",
       "  'bF2': Array(6.059769e-08, dtype=float32),\n",
       "  'bO': Array(1.295508e-06, dtype=float32),\n",
       "  'bU': Array(5.294611e-08, dtype=float32),\n",
       "  'c0': Array(0.00393636, dtype=float32),\n",
       "  'h0': Array(1.3756433e-06, dtype=float32),\n",
       "  'wC': Array(8.250055e-06, dtype=float32),\n",
       "  'wEM': Array(0.00015424, dtype=float32),\n",
       "  'wF1': Array(0.00015612, dtype=float32),\n",
       "  'wF2': Array(3.3674626e-06, dtype=float32),\n",
       "  'wO': Array(8.082898e-05, dtype=float32),\n",
       "  'wU': Array(2.9702658e-06, dtype=float32)},\n",
       " {'bC': Array(5.3860526e-07, dtype=float32),\n",
       "  'bF1': Array(2.6429737e-08, dtype=float32),\n",
       "  'bF2': Array(4.2145675e-06, dtype=float32),\n",
       "  'bO': Array(0.01078398, dtype=float32),\n",
       "  'bU': Array(5.7055804e-05, dtype=float32),\n",
       "  'bY1': Array(0.10217474, dtype=float32),\n",
       "  'c0': Array(0.01917071, dtype=float32),\n",
       "  'h0': Array(3.621238e-07, dtype=float32),\n",
       "  'wC': Array(2.8099414e-06, dtype=float32),\n",
       "  'wF1': Array(1.7541853e-07, dtype=float32),\n",
       "  'wF2': Array(0.00011994, dtype=float32),\n",
       "  'wO': Array(0.05174534, dtype=float32),\n",
       "  'wU': Array(0.00034987, dtype=float32),\n",
       "  'wY1': Array(0.1735527, dtype=float32)}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(jnp.linalg.norm, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_PARAMS: 1183262\n",
      "DTYPE: float32\n",
      "TOTAL_MEGABYTES: 4.733048\n"
     ]
    }
   ],
   "source": [
    "getsize = lambda s: s.size\n",
    "sizes = jax.tree_util.tree_map(getsize, grads)\n",
    "total_params = 0\n",
    "for layer in sizes:\n",
    "  for _, v in layer.items():\n",
    "    total_params += v\n",
    "\n",
    "print(f\"TOTAL_PARAMS: {total_params}\")\n",
    "print(f\"DTYPE: {grads[0]['bC'].dtype}\")\n",
    "print(f\"TOTAL_MEGABYTES: {total_params*4/1_000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.profiler\n",
    "jax.profiler.save_device_memory_profile('test.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 25\n",
      " [[960 961 962 963 964 965 966 967 968 969]\n",
      " [970 971 972 973 974 975 976 977 978 979]\n",
      " [980 981 982 983 984 985 986 987 988 989]\n",
      " [990 991 992 993 994 995 996 997 998 999]] \n",
      "\n",
      " [[970]\n",
      " [980]\n",
      " [990]]\n"
     ]
    }
   ],
   "source": [
    "data = jnp.arange(1000)\n",
    "seqlen = 10\n",
    "bs = 4\n",
    "steps = len(data) // (bs*seqlen)\n",
    "idx = 24\n",
    "data_idx = idx*seqlen*bs\n",
    "next_data_idx = (idx+1)*seqlen*bs\n",
    "print(\n",
    "      f\"steps: {steps}\\n\",\n",
    "      data[data_idx:next_data_idx].reshape(-1, seqlen),\n",
    "      '\\n\\n',\n",
    "      data[data_idx+seqlen:next_data_idx+1:seqlen].reshape(-1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[67, 63],\n",
       "       [80,  1],\n",
       "       [71, 75],\n",
       "       [78, 77],\n",
       "       [81, 81]], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtokens_batch[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([63,  1, 75, 77, 81], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytokens_batch[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
