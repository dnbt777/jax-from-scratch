{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import optax\n",
    "from tokenizers import CharBPETokenizer\n",
    "import functools\n",
    "import time\n",
    "\n",
    "\n",
    "gpu_device = jax.device_get('gpu')[0]\n",
    "cpu_device = jax.device_get('cpu')[0]\n",
    "# LSTM\n",
    "# xs = B, input_size = B, T, C\n",
    "# h = c = y = B, output_size = B, T, logits_size = B, T, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 155\n",
      "dog [66 77 69] dog\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "with open('data/dnbt_posts.txt', 'r') as file:\n",
    "  dataset = file.read()\n",
    "\n",
    "# tokenize\n",
    "vocab = sorted(list(set(dataset)))\n",
    "print(\"vocab length:\", len(vocab))\n",
    "\n",
    "token_to_char = dict(enumerate(vocab))\n",
    "char_to_token = dict([(v, k) for k, v in token_to_char.items()])\n",
    "decode = lambda tokens: \"\".join([token_to_char[int(token)] for token in tokens])\n",
    "encode = lambda chars: jnp.array([char_to_token[c] for c in chars])\n",
    "\n",
    "print(\"dog\", encode(\"dog\"), decode(encode(\"dog\")))\n",
    "\n",
    "dataset_tokens = encode(dataset)\n",
    "split_ratio = 0.8\n",
    "train_tokens = dataset_tokens[:int(len(dataset_tokens)*split_ratio)]\n",
    "test_tokens = dataset_tokens[int(len(dataset_tokens)*split_ratio):]\n",
    "del dataset\n",
    "del dataset_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layers = 4\n",
    "sequence_length = 20\n",
    "model_size = 512\n",
    "\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "\n",
    "\n",
    "# init LSTM params\n",
    "def init_LSTM_params(key, lstm_layers, input_size, model_size, output_size):\n",
    "  param_sets = 8 # manual, idc\n",
    "  keys = random.split(key, param_sets*lstm_layers + 2)\n",
    "  hxconcat_size = model_size + model_size\n",
    "  he = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / shape[0])\n",
    "  params = [\n",
    "    {\n",
    "      \"wU\" : he(keys[param_sets*i + 0], (hxconcat_size, model_size)),\n",
    "      \"bU\" : jnp.zeros((model_size,)),\n",
    "      \"wC\" : he(keys[param_sets*i + 6], (hxconcat_size, model_size)),\n",
    "      \"bC\" : jnp.zeros((model_size,)),\n",
    "      \"wF1\": he(keys[param_sets*i + 1], (hxconcat_size, model_size)),\n",
    "      \"bF1\": jnp.zeros((model_size,)),\n",
    "      \"wF2\": he(keys[param_sets*i + 2], (hxconcat_size, model_size)),\n",
    "      \"bF2\": jnp.zeros((model_size,)),\n",
    "      \"wO\" : he(keys[param_sets*i + 3], (hxconcat_size, model_size)),\n",
    "      \"bO\" : jnp.zeros((model_size,)),\n",
    "      \"h0\" : jnp.zeros((model_size,)),\n",
    "      \"c0\" : jnp.zeros((model_size,)),\n",
    "      #\"h0\" : random.normal(keys[param_sets*i + 4], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "      #\"c0\" : random.normal(keys[param_sets*i + 5], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "    }\n",
    "    for i in range(lstm_layers)\n",
    "  ]\n",
    "  params[0].update(\n",
    "    {\n",
    "    # then embedding table weight and bias\n",
    "    \"wEM\" : he(keys[param_sets*(param_sets - 1) + 2], (input_size, model_size)),\n",
    "    \"bEM\" : jnp.zeros((model_size,)),\n",
    "\n",
    "  })\n",
    "  params[-1].update(\n",
    "    {\n",
    "      # this is for the y layer, which i am probably imlementing wrong.\n",
    "      \"wY1\" : he(keys[param_sets*(lstm_layers-1) + 4], (model_size, output_size)),\n",
    "      \"bY1\" : jnp.zeros((output_size,)),\n",
    "    }\n",
    "  )\n",
    "  return params\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def dropout(dropout_key, original_tensor, dropout_rate):\n",
    "  # generate random of same shape\n",
    "  dropout_probs = random.uniform(dropout_key, shape=original_tensor.shape)\n",
    "  # mask = random < dropout_rate\n",
    "  mask = (dropout_probs > dropout_rate) / (1 - dropout_rate) # scale to keep avg the same\n",
    "  return original_tensor * mask\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def lstm_step(step_dropout_key, lstm_layer_params, layer_h, layer_c, current_xt, dropout_rate):\n",
    "  hxconcat = jax.lax.concatenate([layer_h, current_xt], dimension=1) #B, h ++ B, C => B, h+c\n",
    "  # update gate\n",
    "  update = jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wU\"] + lstm_layer_params[\"bU\"])\n",
    "  #update = dropout(step_dropout_keys[0], update, dropout_rate)\n",
    "  candidate = jax.nn.tanh(hxconcat @ lstm_layer_params[\"wC\"] + lstm_layer_params[\"bC\"])\n",
    "  #candidate = dropout(step_dropout_keys[1], candidate, dropout_rate)\n",
    "\n",
    "  # forget gate\n",
    "  forget = jax.nn.sigmoid(\n",
    "              hxconcat @ lstm_layer_params[\"wF1\"] + lstm_layer_params[\"bF1\"]\n",
    "            ) * jax.nn.tanh(\n",
    "              hxconcat @ lstm_layer_params[\"wF2\"] + lstm_layer_params[\"bF2\"]\n",
    "            )\n",
    "\n",
    "  # update c with update and forget\n",
    "  layer_c = layer_c + update * candidate + forget # (batch, c) => (batch, c)\n",
    "\n",
    "  # output\n",
    "  layer_h = jax.nn.tanh(layer_c) * jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wO\"] + lstm_layer_params[\"bO\"]) # (B, model_size)\n",
    "\n",
    "  next_layer_xt = dropout(step_dropout_key, layer_h, dropout_rate) # the next layer's input x is the current layer's hidden state\n",
    "  # karpathy: dropout after EACH LAYER not several times in the block. lol.\n",
    "\n",
    "  return (layer_h, layer_c), next_layer_xt\n",
    "\n",
    "\n",
    "# LSTM forward\n",
    "import functools\n",
    "@functools.partial(jax.jit, static_argnames=['dropout_rate', 'lstm_layers'])\n",
    "def lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate, lstm_layers=lstm_layers):\n",
    "  batches = xembeds_batch.shape[0]\n",
    "  lstm_layers = len(lstm_params)\n",
    "  # initialize h and c as random/learnable params\n",
    "  #h = jnp.tile(lstm_params[0][\"h0\"], (batches, lstm_layers, 1)) # B, lstm_layer, h_size\n",
    "  #c = jnp.tile(lstm_params[0][\"c0\"], (batches, lstm_layers, 1)) # B, lstm_layer, c_size\n",
    "  # wait.. these are the same for all of the layers.. maybe they shouldn't be\n",
    "  T = xembeds_batch.shape[1]\n",
    "  # take xembeds_batch and pass each xt through the same SINGULAR block. don't update the weight layer. there is only one layer.\n",
    "  dropout_layers = 6*T*lstm_layers # manual\n",
    "  dropout_keys = random.split(dropout_key, dropout_layers)\n",
    "\n",
    "  # for each layer:\n",
    "    # scan over xt\n",
    "    # carry : h, c\n",
    "    # a: xt\n",
    "    # b: h,c\n",
    "    # f = lambda ((h, c), xt) : lstm_step(h, c, xt, everything else) => h, c\n",
    "    # scans over xt\n",
    "    # for next layer: xt = h of previous layer. h = h0 and c = c0\n",
    "  \n",
    "  current_embeddings_batch = jnp.transpose(xembeds_batch, (1, 0, 2)) # B, T, C => T, B, C\n",
    "    # The reason for this is that jax.lax.scan only uses the leading dim. why? idk. its dumb, it needs an axis arg so i can scan over whatever\n",
    "\n",
    "  for lstm_layer in range(lstm_layers):\n",
    "    h = jnp.tile(lstm_params[lstm_layer][\"h0\"], (batches, 1))\n",
    "    c = jnp.tile(lstm_params[lstm_layer][\"c0\"], (batches, 1))\n",
    "    layer_dropout_key = dropout_keys[lstm_layer] # it doesnt matter if this is the same across all layers\n",
    "    # scan should be inexpensive since layer size is small while t size is usually LARGE\n",
    "    # scan :: (c -> a -> (c, b)) -> c -> [a] -> (c, [b])\n",
    "    # scan :: scanfunc -> h_and_c -> xs -> (h_and_c_final, hs_to_be_used_as_input_xt_in_next_layer)\n",
    "    # scanfunc :: (c -> a -> (c, b))\n",
    "    scanfunc = lambda hc, xt : lstm_step(layer_dropout_key, lstm_params[lstm_layer], hc[0], hc[1], xt, dropout_rate)\n",
    "      # for xs: scan along the t dimension! it scans along B by default\n",
    "      # to fix this, we transpose xs with jnp.transpose(current_embeddings_batch, (1, 0, 2))\n",
    "    current_embeddings_batch = jax.lax.scan(scanfunc, (h, c), current_embeddings_batch)[1] # (c, [b]) => [b] ==> B, T, C\n",
    "  \n",
    "\n",
    "  # finally turn current_embeddings_batch into ys (logits)\n",
    "  hs = jnp.transpose(current_embeddings_batch, (1, 0, 2)) # T, B, C => B, T, C\n",
    "  ys = hs @ lstm_params[-1]['wY1'] + lstm_params[-1][\"bY1\"] # B, T, model_size => B, T, vocab_size\n",
    "  return ys\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logits = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def final_token_loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logit = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)[:, -1] # get last logit\n",
    "  vocab_size = logit.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)[:, -1] # get last y (the target)\n",
    "  logprobs = jax.nn.log_softmax(logit, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "\n",
    "jitted_backwards_loss = jax.jit(jax.value_and_grad(final_token_loss, argnums=1), static_argnames=[\"dropout_rate\"])\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['vocab_size'])\n",
    "def embed(lstm_params, xtokens, vocab_size=len(vocab)):\n",
    "  xs_one_hot = jax.nn.one_hot(xtokens, vocab_size, axis=-1) #B, T, vocab_size\n",
    "  activations = xs_one_hot @ lstm_params[0][\"wEM\"] + lstm_params[0][\"bEM\"]\n",
    "  xembeds = jax.nn.tanh(activations) # B, T, C\n",
    "  return xembeds # TODO ??? this doesnt seem to make a difference btw.\n",
    "\n",
    "\n",
    "lr = 2e-3\n",
    "lr_decay = 0.97\n",
    "decay_after = 10\n",
    "decay_every = 5\n",
    "optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "\n",
    "\n",
    "# make optimizer a static arg in jit or it breaks\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate):\n",
    "  step_loss, grads = jitted_backwards_loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate)\n",
    "  param_updates, updated_opt_state = optimizer.update(grads, opt_state, lstm_params)\n",
    "  updated_lstm_params = optax.apply_updates(lstm_params, param_updates) \n",
    "  return updated_lstm_params, updated_opt_state, step_loss, grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 441) || samples/sec: 327 || loss: 3.8576 || val_loss: 3.4963 val_acc: 0.1350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 353) || samples/sec: 1190 || loss: 3.6470 || val_loss: 3.2829 val_acc: 0.1850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 70) || samples/sec: 3450 || loss: 3.6179 || val_loss: 3.3287 val_acc: 0.1600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 497) || samples/sec: 2359 || loss: 3.5855 || val_loss: 3.3263 val_acc: 0.2000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 149) || samples/sec: 9196 || loss: 3.5780 || val_loss: 3.2557 val_acc: 0.1550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 460) || samples/sec: 2142 || loss: 3.5475 || val_loss: 3.7306 val_acc: 0.1500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 256) || samples/sec: 1148 || loss: 3.5132 || val_loss: 3.2855 val_acc: 0.1100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"ee                  \"\n",
      "step (0, 321) || samples/sec: 6535 || loss: 3.5064 || val_loss: 3.2265 val_acc: 0.1750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 233) || samples/sec: 1214 || loss: 3.4880 || val_loss: 3.3974 val_acc: 0.1250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 479) || samples/sec: 3465 || loss: 3.4829 || val_loss: 3.2665 val_acc: 0.1400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"e                   \"\n",
      "step (0, 363) || samples/sec: 49806 || loss: 3.4829 || val_loss: 3.3605 val_acc: 0.1450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 507) || samples/sec: 1357 || loss: 3.4730 || val_loss: 3.5291 val_acc: 0.1550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"eeeeeeeeeeeeeeeeeeee\"\n",
      "step (0, 57) || samples/sec: 1795 || loss: 3.4579 || val_loss: 3.3023 val_acc: 0.0700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \"e                   \"\n",
      "step (0, 238) || samples/sec: 9160 || loss: 3.4552 || val_loss: 3.1728 val_acc: 0.1650 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 6) || samples/sec: 15534 || loss: 3.4533 || val_loss: 3.2837 val_acc: 0.1750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"ee                  \"\n",
      "step (0, 498) || samples/sec: 1658 || loss: 3.4445 || val_loss: 3.3474 val_acc: 0.1450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \"ee                  \"\n",
      "step (0, 266) || samples/sec: 15476 || loss: 3.4446 || val_loss: 3.4087 val_acc: 0.1500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \"e                   \"\n",
      "step (0, 229) || samples/sec: 1732 || loss: 3.4383 || val_loss: 3.1913 val_acc: 0.1750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 11) || samples/sec: 2320 || loss: 3.4310 || val_loss: 3.2271 val_acc: 0.1600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (0, 25) || samples/sec: 2851 || loss: 3.4287 || val_loss: 3.2769 val_acc: 0.1550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21205) || samples/sec: 1672 || loss: 3.4247 || val_loss: 3.2003 val_acc: 0.1600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21567) || samples/sec: 4424 || loss: 3.4237 || val_loss: 3.1387 val_acc: 0.1700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21632) || samples/sec: 6987 || loss: 3.4248 || val_loss: 3.4406 val_acc: 0.1350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21544) || samples/sec: 1307 || loss: 3.4171 || val_loss: 3.2346 val_acc: 0.1900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21261) || samples/sec: 3772 || loss: 3.4155 || val_loss: 3.2597 val_acc: 0.1600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21688) || samples/sec: 2578 || loss: 3.4147 || val_loss: 3.3210 val_acc: 0.2000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21340) || samples/sec: 10044 || loss: 3.4143 || val_loss: 3.2457 val_acc: 0.1550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21651) || samples/sec: 2333 || loss: 3.4114 || val_loss: 3.6973 val_acc: 0.1500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21447) || samples/sec: 1251 || loss: 3.4076 || val_loss: 3.3303 val_acc: 0.1100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21512) || samples/sec: 7059 || loss: 3.4067 || val_loss: 3.1748 val_acc: 0.1750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21424) || samples/sec: 1310 || loss: 3.4036 || val_loss: 3.3911 val_acc: 0.1250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21670) || samples/sec: 3728 || loss: 3.4024 || val_loss: 3.2259 val_acc: 0.1400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21554) || samples/sec: 49995 || loss: 3.4024 || val_loss: 3.3629 val_acc: 0.1400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21698) || samples/sec: 1356 || loss: 3.4009 || val_loss: 3.5316 val_acc: 0.1550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21248) || samples/sec: 1803 || loss: 3.3966 || val_loss: 3.2927 val_acc: 0.1750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21429) || samples/sec: 9142 || loss: 3.3958 || val_loss: 3.1689 val_acc: 0.1700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21197) || samples/sec: 15566 || loss: 3.3953 || val_loss: 3.2658 val_acc: 0.1800 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21689) || samples/sec: 1666 || loss: 3.3924 || val_loss: 3.3235 val_acc: 0.1550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21457) || samples/sec: 15516 || loss: 3.3927 || val_loss: 3.4068 val_acc: 0.1600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21420) || samples/sec: 1677 || loss: 3.3905 || val_loss: 3.1680 val_acc: 0.1750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21202) || samples/sec: 2153 || loss: 3.3876 || val_loss: 3.2238 val_acc: 0.1600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (1, 21216) || samples/sec: 2678 || loss: 3.3862 || val_loss: 3.2831 val_acc: 0.1550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (2, 42396) || samples/sec: 1566 || loss: 3.3832 || val_loss: 3.2074 val_acc: 0.1450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (2, 42758) || samples/sec: 4115 || loss: 3.3825 || val_loss: 3.0998 val_acc: 0.1950 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (2, 42823) || samples/sec: 6588 || loss: 3.3824 || val_loss: 3.4733 val_acc: 0.1350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (2, 42735) || samples/sec: 1227 || loss: 3.3736 || val_loss: 3.2412 val_acc: 0.1850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (2, 42452) || samples/sec: 3516 || loss: 3.3702 || val_loss: 3.2465 val_acc: 0.1550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"                    \"\n",
      "step (2, 42879) || samples/sec: 2398 || loss: 3.3660 || val_loss: 3.2226 val_acc: 0.2000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (2, 42531) || samples/sec: 9351 || loss: 3.3646 || val_loss: 3.2144 val_acc: 0.1750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (2, 42842) || samples/sec: 2307 || loss: 3.3589 || val_loss: 3.7755 val_acc: 0.1500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"                    \"\n",
      "step (2, 42638) || samples/sec: 1244 || loss: 3.3487 || val_loss: 3.2973 val_acc: 0.1300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \" eeeaaeeaea  a  a   \"\n",
      "step (2, 42703) || samples/sec: 6968 || loss: 3.3464 || val_loss: 2.9571 val_acc: 0.2150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"  iiiiiiiiiiiiiiiiii\"\n",
      "step (2, 42615) || samples/sec: 1313 || loss: 3.3375 || val_loss: 3.2641 val_acc: 0.1800 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \" ttt t       t  tt  \"\n",
      "step (2, 42861) || samples/sec: 3734 || loss: 3.3335 || val_loss: 2.9152 val_acc: 0.2350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"    t      t    t   \"\n",
      "step (2, 42745) || samples/sec: 54063 || loss: 3.3333 || val_loss: 3.1910 val_acc: 0.1900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"     t   t t t  t  t\"\n",
      "step (2, 42889) || samples/sec: 1456 || loss: 3.3237 || val_loss: 3.4230 val_acc: 0.1650 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"     tete  ttte  tte\"\n",
      "step (2, 42439) || samples/sec: 1934 || loss: 3.3141 || val_loss: 3.0652 val_acc: 0.2200 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \"eee    tetet te     \"\n",
      "step (2, 42620) || samples/sec: 9857 || loss: 3.3122 || val_loss: 2.9373 val_acc: 0.2350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \"  t  tt     tete   t\"\n",
      "step (2, 42388) || samples/sec: 16424 || loss: 3.3107 || val_loss: 2.9523 val_acc: 0.2450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"e nn nn ten tn te te\"\n",
      "step (2, 42880) || samples/sec: 1790 || loss: 3.3028 || val_loss: 2.9274 val_acc: 0.2400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \"e   tet  te tet  tn \"\n",
      "step (2, 42648) || samples/sec: 16390 || loss: 3.3017 || val_loss: 3.3857 val_acc: 0.1750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \" t     to   tot     \"\n",
      "step (2, 42611) || samples/sec: 1788 || loss: 3.2918 || val_loss: 2.9254 val_acc: 0.2850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"      tte   ta      \"\n",
      "step (2, 42393) || samples/sec: 2332 || loss: 3.2841 || val_loss: 2.8216 val_acc: 0.2300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \"     t  t  tot    to\"\n",
      "step (2, 42407) || samples/sec: 2885 || loss: 3.2792 || val_loss: 3.0441 val_acc: 0.2100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \"      to t to  to  t\"\n",
      "step (3, 63587) || samples/sec: 1675 || loss: 3.2698 || val_loss: 2.9175 val_acc: 0.2300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \" tt te    tt       t\"\n",
      "step (3, 63949) || samples/sec: 4328 || loss: 3.2663 || val_loss: 2.8298 val_acc: 0.2550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"teeeereee tnnee rete\"\n",
      "step (3, 64014) || samples/sec: 7028 || loss: 3.2645 || val_loss: 3.2035 val_acc: 0.2000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \"en         n tn  t  \"\n",
      "step (3, 63926) || samples/sec: 1311 || loss: 3.2524 || val_loss: 2.9679 val_acc: 0.2650 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \"     tn tnth  tnte  \"\n",
      "step (3, 63643) || samples/sec: 3724 || loss: 3.2475 || val_loss: 3.0235 val_acc: 0.2400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"  n tn t  tonn ton  \"\n",
      "step (3, 64070) || samples/sec: 2546 || loss: 3.2409 || val_loss: 2.9169 val_acc: 0.2550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"       t  to   to  t\"\n",
      "step (3, 63722) || samples/sec: 9695 || loss: 3.2391 || val_loss: 2.9717 val_acc: 0.2150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"ttt t  to tt   to  t\"\n",
      "step (3, 64033) || samples/sec: 2306 || loss: 3.2318 || val_loss: 3.6358 val_acc: 0.1350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"  n         n ta n  \"\n",
      "step (3, 63829) || samples/sec: 1236 || loss: 3.2189 || val_loss: 2.8242 val_acc: 0.2400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"taaaaaa tt e t  tae \"\n",
      "step (3, 63894) || samples/sec: 7012 || loss: 3.2161 || val_loss: 2.6133 val_acc: 0.2750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"taeeeeeaaee tle e   \"\n",
      "step (3, 63806) || samples/sec: 1314 || loss: 3.2050 || val_loss: 3.0025 val_acc: 0.2100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \"t    t       t  to  \"\n",
      "step (3, 64052) || samples/sec: 3760 || loss: 3.2004 || val_loss: 2.6362 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"    to     to   toe \"\n",
      "step (3, 63936) || samples/sec: 54787 || loss: 3.2001 || val_loss: 2.9505 val_acc: 0.2500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"  n  n   anthn  ahrt\"\n",
      "step (3, 64080) || samples/sec: 1473 || loss: 3.1903 || val_loss: 2.9708 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"     ton   tnneeetne\"\n",
      "step (3, 63630) || samples/sec: 1949 || loss: 3.1805 || val_loss: 2.8686 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \"t ne   tnte etoee   \"\n",
      "step (3, 63811) || samples/sec: 9895 || loss: 3.1785 || val_loss: 2.7940 val_acc: 0.2300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \"     tn     ton    t\"\n",
      "step (3, 63579) || samples/sec: 16791 || loss: 3.1772 || val_loss: 2.7465 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"  nn n  ian in iaeia\"\n",
      "step (3, 64071) || samples/sec: 1807 || loss: 3.1699 || val_loss: 2.6713 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \" n  ian  iantan  in \"\n",
      "step (3, 63839) || samples/sec: 17021 || loss: 3.1687 || val_loss: 3.2196 val_acc: 0.2050 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \" n     toe  tn   n  \"\n",
      "step (3, 63802) || samples/sec: 1805 || loss: 3.1595 || val_loss: 2.7770 val_acc: 0.2950 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"       th   toe     \"\n",
      "step (3, 63584) || samples/sec: 2321 || loss: 3.1522 || val_loss: 2.6538 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \" n n t  n  ton    to\"\n",
      "step (3, 63598) || samples/sec: 2872 || loss: 3.1475 || val_loss: 2.8031 val_acc: 0.2600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \" n  r then thertn  n\"\n",
      "step (4, 84778) || samples/sec: 1688 || loss: 3.1385 || val_loss: 2.7579 val_acc: 0.3050 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \" n  tor   tn       t\"\n",
      "step (4, 85140) || samples/sec: 4432 || loss: 3.1353 || val_loss: 2.6430 val_acc: 0.3100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"teeeereee ttneeere e\"\n",
      "step (4, 85205) || samples/sec: 6989 || loss: 3.1336 || val_loss: 3.1206 val_acc: 0.2300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \"tnn n      n tteen  \"\n",
      "step (4, 85117) || samples/sec: 1317 || loss: 3.1227 || val_loss: 2.7741 val_acc: 0.2750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \" n   tn tnten tnten \"\n",
      "step (4, 84834) || samples/sec: 3771 || loss: 3.1185 || val_loss: 2.8298 val_acc: 0.2350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"  n tn n  tonn thne \"\n",
      "step (4, 85261) || samples/sec: 2566 || loss: 3.1133 || val_loss: 2.7048 val_acc: 0.2800 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"  n n  n  toa  to nt\"\n",
      "step (4, 84913) || samples/sec: 9833 || loss: 3.1116 || val_loss: 2.8447 val_acc: 0.2250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"tn  ne thenn   toe t\"\n",
      "step (4, 85224) || samples/sec: 2316 || loss: 3.1054 || val_loss: 3.5506 val_acc: 0.1700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"  n         n taer  \"\n",
      "step (4, 85020) || samples/sec: 1241 || loss: 3.0943 || val_loss: 2.8004 val_acc: 0.2450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"trac  eacs aan  c  e\"\n",
      "step (4, 85085) || samples/sec: 6923 || loss: 3.0920 || val_loss: 2.5879 val_acc: 0.2850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"tr     ao o tsore  e\"\n",
      "step (4, 84997) || samples/sec: 1308 || loss: 3.0828 || val_loss: 2.8864 val_acc: 0.2150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \"t ng tn    e r  tor \"\n",
      "step (4, 85243) || samples/sec: 3753 || loss: 3.0791 || val_loss: 2.5946 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"    ti  r  tor  toe \"\n",
      "step (4, 85127) || samples/sec: 54324 || loss: 3.0788 || val_loss: 2.9531 val_acc: 0.2800 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"  n  ng  anahng ah t\"\n",
      "step (4, 85271) || samples/sec: 1467 || loss: 3.0704 || val_loss: 2.7650 val_acc: 0.2650 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"     teng  tnne  tn \"\n",
      "step (4, 84821) || samples/sec: 1935 || loss: 3.0620 || val_loss: 2.8197 val_acc: 0.2650 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \" ene   tntonetoe    \"\n",
      "step (4, 85002) || samples/sec: 9784 || loss: 3.0603 || val_loss: 2.6484 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \" nn  tn     ton    t\"\n",
      "step (4, 84770) || samples/sec: 16526 || loss: 3.0592 || val_loss: 2.6555 val_acc: 0.3250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"  tn    ian in iaeih\"\n",
      "step (4, 85262) || samples/sec: 1783 || loss: 3.0525 || val_loss: 2.6601 val_acc: 0.2650 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \" n  ion  ihrihne in \"\n",
      "step (4, 85030) || samples/sec: 16523 || loss: 3.0517 || val_loss: 3.0756 val_acc: 0.2500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \" ne    toe  tnn  r  \"\n",
      "step (4, 84993) || samples/sec: 1785 || loss: 3.0444 || val_loss: 2.6858 val_acc: 0.2750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"      tto   toet    \"\n",
      "step (4, 84775) || samples/sec: 2312 || loss: 3.0385 || val_loss: 2.5564 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \" n n ng t  Ao     to\"\n",
      "step (4, 84789) || samples/sec: 2869 || loss: 3.0344 || val_loss: 2.7424 val_acc: 0.2650 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \" n  r toer the tn  n\"\n",
      "step (5, 105969) || samples/sec: 1682 || loss: 3.0271 || val_loss: 2.7292 val_acc: 0.3200 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \" nn tore  tt       t\"\n",
      "step (5, 106331) || samples/sec: 4473 || loss: 3.0243 || val_loss: 2.5728 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"te   ceee ap  e r   \"\n",
      "step (5, 106396) || samples/sec: 7029 || loss: 3.0227 || val_loss: 2.9975 val_acc: 0.2350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \" nn        n tteene \"\n",
      "step (5, 106308) || samples/sec: 1303 || loss: 3.0139 || val_loss: 2.6268 val_acc: 0.2900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \" n n tn tntenetntene\"\n",
      "step (5, 106025) || samples/sec: 3736 || loss: 3.0102 || val_loss: 2.6676 val_acc: 0.2750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"    tn ng to n thne \"\n",
      "step (5, 106452) || samples/sec: 2550 || loss: 3.0057 || val_loss: 2.6023 val_acc: 0.3100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"       ne toe  to  t\"\n",
      "step (5, 106104) || samples/sec: 9807 || loss: 3.0041 || val_loss: 2.7906 val_acc: 0.2150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \" n  n  the     toe t\"\n",
      "step (5, 106415) || samples/sec: 2297 || loss: 2.9990 || val_loss: 3.5204 val_acc: 0.2000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"  n       ee  toer  \"\n",
      "step (5, 106211) || samples/sec: 1242 || loss: 2.9900 || val_loss: 2.7095 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"c  c    cs   n  c   \"\n",
      "step (5, 106276) || samples/sec: 7045 || loss: 2.9882 || val_loss: 2.5226 val_acc: 0.2850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"rT     coTo @sorer  \"\n",
      "step (5, 106188) || samples/sec: 1321 || loss: 2.9808 || val_loss: 2.8812 val_acc: 0.2300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \"  tg tn r  e ne tore\"\n",
      "step (5, 106434) || samples/sec: 3790 || loss: 2.9778 || val_loss: 2.5591 val_acc: 0.3000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"    te  re tite ti  \"\n",
      "step (5, 106318) || samples/sec: 54868 || loss: 2.9776 || val_loss: 2.9318 val_acc: 0.2750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"  teeng  antong ah t\"\n",
      "step (5, 106462) || samples/sec: 1476 || loss: 2.9707 || val_loss: 2.6930 val_acc: 0.2900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"     teng  annee tne\"\n",
      "step (5, 106012) || samples/sec: 1939 || loss: 2.9639 || val_loss: 2.7775 val_acc: 0.2450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \" ene   tnten toe i  \"\n",
      "step (5, 106193) || samples/sec: 9736 || loss: 2.9625 || val_loss: 2.5552 val_acc: 0.2950 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \"  n  an     ton e  t\"\n",
      "step (5, 105961) || samples/sec: 16499 || loss: 2.9616 || val_loss: 2.6247 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"  in    ien in ioeih\"\n",
      "step (5, 106453) || samples/sec: 1786 || loss: 2.9561 || val_loss: 2.5588 val_acc: 0.2950 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \" n  ion  ih th   in \"\n",
      "step (5, 106221) || samples/sec: 16786 || loss: 2.9552 || val_loss: 3.0841 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \" ne   etee  tnn  r  \"\n",
      "step (5, 106184) || samples/sec: 1783 || loss: 2.9492 || val_loss: 2.6292 val_acc: 0.2950 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"       to   toate   \"\n",
      "step (5, 105966) || samples/sec: 2303 || loss: 2.9443 || val_loss: 2.5446 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \" n r ng    aon    to\"\n",
      "step (5, 105980) || samples/sec: 2831 || loss: 2.9409 || val_loss: 2.7540 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \" nd retoer the tn  n\"\n",
      "step (6, 127160) || samples/sec: 1673 || loss: 2.9350 || val_loss: 2.6677 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \" nn tore  tt       t\"\n",
      "step (6, 127522) || samples/sec: 4429 || loss: 2.9327 || val_loss: 2.5318 val_acc: 0.3300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"rhhhhpaae hptdeere  \"\n",
      "step (6, 127587) || samples/sec: 7018 || loss: 2.9314 || val_loss: 3.0209 val_acc: 0.2350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \" nne       n tpeeng \"\n",
      "step (6, 127499) || samples/sec: 1321 || loss: 2.9243 || val_loss: 2.4997 val_acc: 0.3250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \" n   tn tntenetntene\"\n",
      "step (6, 127216) || samples/sec: 3771 || loss: 2.9213 || val_loss: 2.5747 val_acc: 0.2900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"    tn ng to   thne \"\n",
      "step (6, 127643) || samples/sec: 2564 || loss: 2.9177 || val_loss: 2.5475 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"       ne toe  to  t\"\n",
      "step (6, 127295) || samples/sec: 9983 || loss: 2.9163 || val_loss: 2.7432 val_acc: 0.2300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"nnn n  the n   toe t\"\n",
      "step (6, 127606) || samples/sec: 2324 || loss: 2.9119 || val_loss: 3.3222 val_acc: 0.2300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"  n       ee  toanee\"\n",
      "step (6, 127402) || samples/sec: 1241 || loss: 2.9044 || val_loss: 2.7308 val_acc: 0.2750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"rT r a hcs   nn @   \"\n",
      "step (6, 127467) || samples/sec: 7024 || loss: 2.9028 || val_loss: 2.5173 val_acc: 0.3000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"rT    ccalo @serer  \"\n",
      "step (6, 127379) || samples/sec: 1302 || loss: 2.8966 || val_loss: 2.8663 val_acc: 0.2250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \"n tg tn r  eene tore\"\n",
      "step (6, 127625) || samples/sec: 3724 || loss: 2.8940 || val_loss: 2.5508 val_acc: 0.3100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"    tieene tete te  \"\n",
      "step (6, 127509) || samples/sec: 54533 || loss: 2.8938 || val_loss: 2.9710 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"  theng  tntong th t\"\n",
      "step (6, 127653) || samples/sec: 1475 || loss: 2.8880 || val_loss: 2.6478 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"     teng  tndee tne\"\n",
      "step (6, 127203) || samples/sec: 1953 || loss: 2.8823 || val_loss: 2.7622 val_acc: 0.2400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \" ene   tnten tee    \"\n",
      "step (6, 127384) || samples/sec: 9897 || loss: 2.8811 || val_loss: 2.4567 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \"     tn e   ton e  t\"\n",
      "step (6, 127152) || samples/sec: 16786 || loss: 2.8803 || val_loss: 2.5838 val_acc: 0.3050 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"  an    aet an aotth\"\n",
      "step (6, 127644) || samples/sec: 1803 || loss: 2.8757 || val_loss: 2.5372 val_acc: 0.2850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \" n  ton  ih th   tn \"\n",
      "step (6, 127412) || samples/sec: 16710 || loss: 2.8750 || val_loss: 3.0393 val_acc: 0.2750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \" ne    toe  tnt  re \"\n",
      "step (6, 127375) || samples/sec: 1803 || loss: 2.8699 || val_loss: 2.5940 val_acc: 0.2800 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"       to   toate   \"\n",
      "step (6, 127157) || samples/sec: 2317 || loss: 2.8658 || val_loss: 2.5119 val_acc: 0.2950 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \"nn r ng    aorh   to\"\n",
      "step (6, 127171) || samples/sec: 2841 || loss: 2.8630 || val_loss: 2.7181 val_acc: 0.2900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \" n  retoet the tn en\"\n",
      "step (7, 148351) || samples/sec: 1658 || loss: 2.8581 || val_loss: 2.6497 val_acc: 0.2950 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \" nn tor   tte r    t\"\n",
      "step (7, 148713) || samples/sec: 4431 || loss: 2.8562 || val_loss: 2.5400 val_acc: 0.3300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"hhhhhhahe @ptdeer  h\"\n",
      "step (7, 148778) || samples/sec: 6965 || loss: 2.8552 || val_loss: 3.0687 val_acc: 0.2450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \" nne       n tpetng \"\n",
      "step (7, 148690) || samples/sec: 1314 || loss: 2.8491 || val_loss: 2.4158 val_acc: 0.3500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \"nn   tn tntenetntene\"\n",
      "step (7, 148407) || samples/sec: 3762 || loss: 2.8465 || val_loss: 2.5171 val_acc: 0.3050 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"    tn ng tou  thne \"\n",
      "step (7, 148834) || samples/sec: 2548 || loss: 2.8434 || val_loss: 2.4783 val_acc: 0.3300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"       ne toa  tou t\"\n",
      "step (7, 148486) || samples/sec: 9905 || loss: 2.8422 || val_loss: 2.6827 val_acc: 0.2500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"nnn ne the     toe t\"\n",
      "step (7, 148797) || samples/sec: 2294 || loss: 2.8386 || val_loss: 3.3595 val_acc: 0.2250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"  n r r   he  tooche\"\n",
      "step (7, 148593) || samples/sec: 1233 || loss: 2.8319 || val_loss: 2.6258 val_acc: 0.3000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"hTTbBa hcs  an  @   \"\n",
      "step (7, 148658) || samples/sec: 6919 || loss: 2.8305 || val_loss: 2.4900 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"hTAAAAAAAABB@aerh   \"\n",
      "step (7, 148570) || samples/sec: 1308 || loss: 2.8254 || val_loss: 2.8277 val_acc: 0.2300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \"nnt  tn r  etne tome\"\n",
      "step (7, 148816) || samples/sec: 3744 || loss: 2.8232 || val_loss: 2.5671 val_acc: 0.3050 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"n   teeene tete te  \"\n",
      "step (7, 148700) || samples/sec: 56361 || loss: 2.8230 || val_loss: 2.9840 val_acc: 0.2650 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"  theng  a shn  ah t\"\n",
      "step (7, 148844) || samples/sec: 1477 || loss: 2.8181 || val_loss: 2.5762 val_acc: 0.3500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"n    tesg  tntee tne\"\n",
      "step (7, 148394) || samples/sec: 1945 || loss: 2.8133 || val_loss: 2.7448 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \"n ne   tnseshtee    \"\n",
      "step (7, 148575) || samples/sec: 9787 || loss: 2.8123 || val_loss: 2.4074 val_acc: 0.3450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \"  n  tn en eteshe  t\"\n",
      "step (7, 148343) || samples/sec: 16403 || loss: 2.8116 || val_loss: 2.5750 val_acc: 0.3300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"  tne n iet in aotih\"\n",
      "step (7, 148835) || samples/sec: 1787 || loss: 2.8074 || val_loss: 2.5403 val_acc: 0.3000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \" ne ton  ih thn  tne\"\n",
      "step (7, 148603) || samples/sec: 16619 || loss: 2.8069 || val_loss: 2.9877 val_acc: 0.2750 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \"nne    tee  tnte re \"\n",
      "step (7, 148566) || samples/sec: 1786 || loss: 2.8025 || val_loss: 2.5779 val_acc: 0.2800 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"       to   toote   \"\n",
      "step (7, 148348) || samples/sec: 2293 || loss: 2.7990 || val_loss: 2.4824 val_acc: 0.2900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \"nn r ng    tont   te\"\n",
      "step (7, 148362) || samples/sec: 2834 || loss: 2.7965 || val_loss: 2.6854 val_acc: 0.2800 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \" n  r toet the tn en\"\n",
      "step (8, 169542) || samples/sec: 1671 || loss: 2.7923 || val_loss: 2.6243 val_acc: 0.3400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \" nn tore  tt er    t\"\n",
      "step (8, 169904) || samples/sec: 4438 || loss: 2.7907 || val_loss: 2.5123 val_acc: 0.3250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"hhA__tA eh@ptdeere h\"\n",
      "step (8, 169969) || samples/sec: 6981 || loss: 2.7898 || val_loss: 3.0135 val_acc: 0.2500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \" nne       n tpetne \"\n",
      "step (8, 169881) || samples/sec: 1318 || loss: 2.7844 || val_loss: 2.3662 val_acc: 0.3600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \"nn   tn tnceketncene\"\n",
      "step (8, 169598) || samples/sec: 3780 || loss: 2.7821 || val_loss: 2.4697 val_acc: 0.3400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"  r tneng tou  thne \"\n",
      "step (8, 170025) || samples/sec: 2561 || loss: 2.7794 || val_loss: 2.4198 val_acc: 0.3500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"    r  ne toa  tou t\"\n",
      "step (8, 169677) || samples/sec: 9943 || loss: 2.7784 || val_loss: 2.5257 val_acc: 0.2850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"nnn ne the     toe t\"\n",
      "step (8, 169988) || samples/sec: 2318 || loss: 2.7752 || val_loss: 3.1745 val_acc: 0.2350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"  ter     heu tooche\"\n",
      "step (8, 169784) || samples/sec: 1237 || loss: 2.7694 || val_loss: 2.6099 val_acc: 0.2950 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"hhhcra pcs arn  @   \"\n",
      "step (8, 169849) || samples/sec: 6868 || loss: 2.7683 || val_loss: 2.4604 val_acc: 0.3200 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"hhrrrrrcoy  @aacer _\"\n",
      "step (8, 169761) || samples/sec: 1300 || loss: 2.7638 || val_loss: 2.7810 val_acc: 0.2400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \"n te tn te ehne tome\"\n",
      "step (8, 170007) || samples/sec: 3732 || loss: 2.7619 || val_loss: 2.5572 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"    teeene tete te  \"\n",
      "step (8, 169891) || samples/sec: 54047 || loss: 2.7617 || val_loss: 2.8009 val_acc: 0.3000 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"  theng  ancon  ah t\"\n",
      "step (8, 170035) || samples/sec: 1444 || loss: 2.7574 || val_loss: 2.5433 val_acc: 0.3450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"n    tesg  tntee tne\"\n",
      "step (8, 169585) || samples/sec: 1941 || loss: 2.7532 || val_loss: 2.7402 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \"nene   tnsen tee    \"\n",
      "step (8, 169766) || samples/sec: 9800 || loss: 2.7523 || val_loss: 2.3437 val_acc: 0.3400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \"  n  tn er eteshe  t\"\n",
      "step (8, 169534) || samples/sec: 16033 || loss: 2.7517 || val_loss: 2.4337 val_acc: 0.3850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"  tn  n iat in iouth\"\n",
      "step (8, 170026) || samples/sec: 1801 || loss: 2.7481 || val_loss: 2.4800 val_acc: 0.3250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \" ne ton  ah thne tne\"\n",
      "step (8, 169794) || samples/sec: 16541 || loss: 2.7476 || val_loss: 2.6974 val_acc: 0.3400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \" ne  r toe etnt ere \"\n",
      "step (8, 169757) || samples/sec: 1803 || loss: 2.7437 || val_loss: 2.5811 val_acc: 0.2650 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"       tor  toote t \"\n",
      "step (8, 169539) || samples/sec: 2333 || loss: 2.7405 || val_loss: 2.3924 val_acc: 0.3100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \"nt rtng TB tont   te\"\n",
      "step (8, 169553) || samples/sec: 2867 || loss: 2.7383 || val_loss: 2.4353 val_acc: 0.2950 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \" n  r toet the tteon\"\n",
      "step (9, 190733) || samples/sec: 1663 || loss: 2.7345 || val_loss: 2.5893 val_acc: 0.3300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \" tn tore  tterr  r t\"\n",
      "step (9, 191095) || samples/sec: 4378 || loss: 2.7331 || val_loss: 2.4601 val_acc: 0.3400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"hh  roly  @sndeere@h\"\n",
      "step (9, 191160) || samples/sec: 7011 || loss: 2.7323 || val_loss: 2.7393 val_acc: 0.2900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \" nte e     n tputne \"\n",
      "step (9, 191072) || samples/sec: 1303 || loss: 2.7276 || val_loss: 2.3541 val_acc: 0.3550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \"nn   tn tnceketncet \"\n",
      "step (9, 190789) || samples/sec: 3757 || loss: 2.7256 || val_loss: 2.4196 val_acc: 0.3600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"  r tneng tou  thme \"\n",
      "step (9, 191216) || samples/sec: 2494 || loss: 2.7231 || val_loss: 2.3832 val_acc: 0.3400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"    rt ne toa  tou t\"\n",
      "step (9, 190868) || samples/sec: 9017 || loss: 2.7222 || val_loss: 2.4840 val_acc: 0.3250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"nnn ne thet    too t\"\n",
      "step (9, 191179) || samples/sec: 2118 || loss: 2.7192 || val_loss: 3.1715 val_acc: 0.2400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"  te      heu toocte\"\n",
      "step (9, 190975) || samples/sec: 1136 || loss: 2.7140 || val_loss: 2.5712 val_acc: 0.3100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"hrroly  csxahrr @   \"\n",
      "step (9, 191040) || samples/sec: 6452 || loss: 2.7130 || val_loss: 2.4990 val_acc: 0.3050 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"     reply: @mefengl\"\n",
      "PRED   (20) | \"hBB  rroly: @aurir  \"\n",
      "step (9, 190952) || samples/sec: 1200 || loss: 2.7092 || val_loss: 2.7542 val_acc: 0.2550 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"9 Be elon start comp\"\n",
      "PRED   (20) | \"ntte tter  etneetome\"\n",
      "step (9, 191198) || samples/sec: 3432 || loss: 2.7074 || val_loss: 2.5359 val_acc: 0.3400 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"re, become more dysf\"\n",
      "PRED   (20) | \"n   teeere tote te  \"\n",
      "step (9, 191082) || samples/sec: 48723 || loss: 2.7072 || val_loss: 2.7917 val_acc: 0.3100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e things I want to m\"\n",
      "PRED   (20) | \"  theng  a con  ah t\"\n",
      "step (9, 191226) || samples/sec: 1324 || loss: 2.7034 || val_loss: 2.5380 val_acc: 0.3500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"long line? aight imm\"\n",
      "PRED   (20) | \"nd  etekg  antee tne\"\n",
      "step (9, 190776) || samples/sec: 1752 || loss: 2.6995 || val_loss: 2.7272 val_acc: 0.2900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"t work a bit less🛑  \"\n",
      "PRED   (20) | \"netou etnsenhtoet   \"\n",
      "step (9, 190957) || samples/sec: 8898 || loss: 2.6986 || val_loss: 2.3096 val_acc: 0.3300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ould almost rather j\"\n",
      "PRED   (20) | \"  n  tnden etoshe  t\"\n",
      "step (9, 190725) || samples/sec: 14887 || loss: 2.6981 || val_loss: 2.4904 val_acc: 0.3600 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"g about how at yc th\"\n",
      "PRED   (20) | \"  in  n iat in iorth\"\n",
      "step (9, 191217) || samples/sec: 1627 || loss: 2.6947 || val_loss: 2.4611 val_acc: 0.3100 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"ive ways to take adv\"\n",
      "PRED   (20) | \" ne aon  ah thne ane\"\n",
      "step (9, 190985) || samples/sec: 15031 || loss: 2.6942 || val_loss: 2.7317 val_acc: 0.3300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" model best outcome \"\n",
      "PRED   (20) | \" ne  r to   tnteere \"\n",
      "step (9, 190948) || samples/sec: 1625 || loss: 2.6906 || val_loss: 2.5556 val_acc: 0.2850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"icle77 cool project \"\n",
      "PRED   (20) | \"       torl toote t \"\n",
      "step (9, 190730) || samples/sec: 2095 || loss: 2.6877 || val_loss: 2.3948 val_acc: 0.3150 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"@yacineMTB Facts, ke\"\n",
      "PRED   (20) | \"nn ltng TB tont   ie\"\n",
      "step (9, 190744) || samples/sec: 2586 || loss: 2.6857 || val_loss: 2.5314 val_acc: 0.2900 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"onder what the optim\"\n",
      "PRED   (20) | \" td r toet the tteun\"\n",
      "step (10, 211924) || samples/sec: 1515 || loss: 2.6823 || val_loss: 2.5593 val_acc: 0.3250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" is fully extended w\"\n",
      "PRED   (20) | \"tnn tore  tferr    t\"\n",
      "step (10, 212286) || samples/sec: 4018 || loss: 2.6810 || val_loss: 2.4678 val_acc: 0.3300 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"   reply: @andrew_py\"\n",
      "PRED   (20) | \"hT_hhtA_e hpndeer_@h\"\n",
      "step (10, 212351) || samples/sec: 6320 || loss: 2.6803 || val_loss: 2.7899 val_acc: 0.2700 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" @kuberdenis @startu\"\n",
      "PRED   (20) | \" ntete     n tpurlee\"\n",
      "step (10, 212263) || samples/sec: 1186 || loss: 2.6759 || val_loss: 2.3728 val_acc: 0.3450 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"imo. as a kid i had \"\n",
      "PRED   (20) | \"nn   tn tnceketncet \"\n",
      "step (10, 211980) || samples/sec: 3567 || loss: 2.6740 || val_loss: 2.4019 val_acc: 0.3350 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"joy using your time \"\n",
      "PRED   (20) | \"  r tn ng tou  thme \"\n",
      "step (10, 212407) || samples/sec: 2563 || loss: 2.6719 || val_loss: 2.3541 val_acc: 0.3500 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"roductive when you w\"\n",
      "PRED   (20) | \"    rtine toa  tou t\"\n",
      "step (10, 212059) || samples/sec: 9869 || loss: 2.6711 || val_loss: 2.5864 val_acc: 0.2850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" aside though, why w\"\n",
      "PRED   (20) | \"nnn ne thett   toe t\"\n",
      "step (10, 212370) || samples/sec: 2314 || loss: 2.6685 || val_loss: 3.3230 val_acc: 0.2250 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \"e dude!!  for procra\"\n",
      "PRED   (20) | \"  te      hhu tooche\"\n",
      "step (10, 212166) || samples/sec: 1239 || loss: 2.6638 || val_loss: 2.6412 val_acc: 0.2850 || LR = 0.002000\n",
      "\n",
      "TARGET (20) | \" reply: @AyNio2 ayyy\"\n",
      "PRED   (20) | \"hBBcBaahcsxean  @   \"\n",
      "step (10, 212231) || samples/sec: 6897 || loss: 2.6629 || val_loss: 2.4691 val_acc: 0.3050 || LR = 0.002000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m ytokens_batch \u001b[38;5;241m=\u001b[39m train_tokens[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39msequence_length\u001b[38;5;241m*\u001b[39mtrain_batch_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length)\n\u001b[1;32m     33\u001b[0m dropout_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(epoch\u001b[38;5;241m*\u001b[39msamples \u001b[38;5;241m+\u001b[39m i) \u001b[38;5;66;03m# unique for every step\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m lstm_params, opt_state, step_loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtokens_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytokens_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     38\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(step_loss)\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "# set up lstm params\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "opt_state = optimizer.init(lstm_params)\n",
    "\n",
    "\n",
    "# train\n",
    "# for now just overfit on small sample idk lol\n",
    "train_batch_size = 100\n",
    "val_batch_size = 10\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "print_every = 20\n",
    "j = 0\n",
    "losses = []\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "  if epoch > decay_after:\n",
    "    if epoch % decay_every == 0:\n",
    "      lr *= lr_decay\n",
    "      opt_state.hyperparams['learning_rate'] = lr\n",
    "  samples = (len(train_tokens) - 1) // sequence_length\n",
    "  for i in range(0, len(train_tokens) - 1 - sequence_length*train_batch_size, sequence_length*train_batch_size): # probably wrong but w/e\n",
    "    # train\n",
    "    # B, T where T = sequence_length\n",
    "    xtokens_batch = train_tokens[i:i+sequence_length*train_batch_size].reshape(-1, sequence_length)\n",
    "    ytokens_batch = train_tokens[i+1:i+sequence_length*train_batch_size+1].reshape(-1, sequence_length)\n",
    "\n",
    "    dropout_key = random.PRNGKey(epoch*samples + i) # unique for every step\n",
    "\n",
    "    lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate)\n",
    "\n",
    "    j += 1\n",
    "    losses.append(step_loss)\n",
    "\n",
    "    if j % print_every == 0:\n",
    "      end = time.time()\n",
    "      duration = end - start\n",
    "      # train inference example (no dropout)\n",
    "      xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch\n",
    "      logits_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0)\n",
    "      prediction_batch = jnp.argmax(logits_batch, axis=-1)\n",
    "\n",
    "      # val batch\n",
    "      j = i % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "      idx = j*val_batch_size*sequence_length\n",
    "      xtokens_val_batch = test_tokens[idx:idx+sequence_length*val_batch_size].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "      ytokens_val_batch = test_tokens[idx+1:idx+sequence_length*val_batch_size+1].reshape(-1, sequence_length)\n",
    "      xembeds_val_batch = embed(lstm_params, xtokens_val_batch)\n",
    "      \n",
    "      logits_val_batch = lstm_forward(dropout_key, lstm_params, xembeds_val_batch, 0)\n",
    "      prediction_val_batch = jnp.argmax(logits_val_batch, axis=-1)\n",
    "      ys_onehot = jax.nn.one_hot(ytokens_val_batch, len(vocab), axis=-1)\n",
    "      logprobs = jax.nn.log_softmax(logits_val_batch, axis=-1)\n",
    "      crossentropies = -jnp.sum(ys_onehot*logprobs,axis=-1)\n",
    "      val_loss = jnp.mean(crossentropies) #lmao\n",
    "      val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "      x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "      y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "      yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "      #print(epoch, epoch * samples + i, f\"{step_loss:1.4f}\", \"pred:\", x, \"=>\", y, \"?=\", yhat)\n",
    "      print(f'TARGET ({len(y)}) | \"{y}\"')\n",
    "      print(f'PRED   ({len(yhat)}) | \"{yhat}\"')\n",
    "      print(f\"step {(epoch, epoch * samples + j)} || samples/sec: {train_batch_size*print_every/(duration):0.0f} || loss: {sum(losses)/len(losses):1.4f} || val_loss: {val_loss:1.4f} val_acc: {val_accuracy:1.4f} || LR = {opt_state.hyperparams['learning_rate']:0.6f}\" )\n",
    "      print()\n",
    "      start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post: a piosg @Ays it atf sabcson threts forcret drasper plr intapale fmafer sraqded reat mikeyod mow te repre froth thit thea and menn a min ceos to 8Ishl 4lafan a porttps://t.co/tovqG6L4xE yourascorf,G this yo biach the. yau pemf yao exang at carlt ec artls als seery susfd ray ors you mes Iv pA)o boster 4 c pywe stirings ctogerl reeld time sry lohes inlad, baph seme ser nes ot modcer the s,hes at os as fan ar,orfiky meol af slarlise wor tou doar rancminus of like &rad biseoding (E5P5Q siverd the eotf is o hos sot i got at and ib ono sot the an to ceearsty would if the e cind for etf tre muncss i canctus f nela. soil as youl, his newur to ceog f the praa casp, hoc thy:WTre meot enars ar pore?\n",
      "\n",
      "e in the tor lew it youd ther jogatioln nejertacts giws\n",
      "\n",
      "-f Y*merungs frocks. maging bad is bring ML the remre meef ti ne ined a saff wirisibe thiuld"
     ]
    }
   ],
   "source": [
    "def inference(key, chars):\n",
    "  xtokens = encode(chars)[None, :]\n",
    "  xembed = embed(lstm_params, xtokens) # artificial single batch\n",
    "  logits = lstm_forward(key, lstm_params, xembed, 0)[0][-1] # logits of the first B and last T in the B T C. should be (C,)\n",
    "  yhattokens = random.choice(key, a=logits.shape[0], p=jax.nn.softmax(logits)) # no need for axis=-1 since logits are (C,)\n",
    "  sequence = yhattokens\n",
    "  return sequence\n",
    "\n",
    "steps = 1000\n",
    "import time\n",
    "seed = int(time.time())\n",
    "keys = random.split(random.PRNGKey(seed), steps)\n",
    "temperature = 0.5\n",
    "text =  \"\\n\"*50 + 'post: '\n",
    "print(text.replace('\\n\\n', ''), end='')\n",
    "for i in range(steps):\n",
    "  yseq = inference(keys[i], text[-sequence_length:])\n",
    "  next_char = decode([yseq])[-1]\n",
    "  if next_char == '🛑':\n",
    "    break\n",
    "  text += next_char\n",
    "  print(next_char, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bC': Array(0.00024058, dtype=float32),\n",
       "  'bEM': Array(0.0003749, dtype=float32),\n",
       "  'bF1': Array(1.8734916e-05, dtype=float32),\n",
       "  'bF2': Array(0.0002467, dtype=float32),\n",
       "  'bO': Array(5.4436307e-05, dtype=float32),\n",
       "  'bU': Array(2.0121175e-05, dtype=float32),\n",
       "  'c0': Array(0.00227709, dtype=float32),\n",
       "  'h0': Array(0.00339398, dtype=float32),\n",
       "  'wC': Array(0.00184112, dtype=float32),\n",
       "  'wEM': Array(0.00018773, dtype=float32),\n",
       "  'wF1': Array(0.00021604, dtype=float32),\n",
       "  'wF2': Array(0.00192728, dtype=float32),\n",
       "  'wO': Array(0.00067066, dtype=float32),\n",
       "  'wU': Array(0.00023371, dtype=float32)},\n",
       " {'bC': Array(0.00018975, dtype=float32),\n",
       "  'bF1': Array(2.3798742e-05, dtype=float32),\n",
       "  'bF2': Array(0.0001887, dtype=float32),\n",
       "  'bO': Array(5.7448495e-05, dtype=float32),\n",
       "  'bU': Array(2.3484547e-05, dtype=float32),\n",
       "  'wC': Array(0.00251008, dtype=float32),\n",
       "  'wF1': Array(0.00043969, dtype=float32),\n",
       "  'wF2': Array(0.00262315, dtype=float32),\n",
       "  'wO': Array(0.00120152, dtype=float32),\n",
       "  'wU': Array(0.0004416, dtype=float32)},\n",
       " {'bC': Array(0.00025577, dtype=float32),\n",
       "  'bF1': Array(3.4147743e-05, dtype=float32),\n",
       "  'bF2': Array(0.00026724, dtype=float32),\n",
       "  'bO': Array(8.655426e-05, dtype=float32),\n",
       "  'bU': Array(3.8467442e-05, dtype=float32),\n",
       "  'wC': Array(0.00366674, dtype=float32),\n",
       "  'wF1': Array(0.00070666, dtype=float32),\n",
       "  'wF2': Array(0.00400147, dtype=float32),\n",
       "  'wO': Array(0.00189952, dtype=float32),\n",
       "  'wU': Array(0.0007839, dtype=float32)},\n",
       " {'bC': Array(0.00037901, dtype=float32),\n",
       "  'bF1': Array(5.0532577e-05, dtype=float32),\n",
       "  'bF2': Array(0.00036836, dtype=float32),\n",
       "  'bO': Array(0.00013936, dtype=float32),\n",
       "  'bU': Array(4.395673e-05, dtype=float32),\n",
       "  'wC': Array(0.00599721, dtype=float32),\n",
       "  'wF1': Array(0.00103374, dtype=float32),\n",
       "  'wF2': Array(0.00557374, dtype=float32),\n",
       "  'wO': Array(0.00314904, dtype=float32),\n",
       "  'wU': Array(0.0009353, dtype=float32)},\n",
       " {'bC': Array(0.00051676, dtype=float32),\n",
       "  'bF1': Array(6.6895984e-05, dtype=float32),\n",
       "  'bF2': Array(0.00047469, dtype=float32),\n",
       "  'bO': Array(0.0001973, dtype=float32),\n",
       "  'bU': Array(6.516224e-05, dtype=float32),\n",
       "  'wC': Array(0.0095788, dtype=float32),\n",
       "  'wF1': Array(0.00141883, dtype=float32),\n",
       "  'wF2': Array(0.0082759, dtype=float32),\n",
       "  'wO': Array(0.00477626, dtype=float32),\n",
       "  'wU': Array(0.00132365, dtype=float32)},\n",
       " {'bC': Array(0.00062228, dtype=float32),\n",
       "  'bF1': Array(8.397065e-05, dtype=float32),\n",
       "  'bF2': Array(0.00063485, dtype=float32),\n",
       "  'bO': Array(0.00040754, dtype=float32),\n",
       "  'bU': Array(8.509394e-05, dtype=float32),\n",
       "  'wC': Array(0.01258676, dtype=float32),\n",
       "  'wF1': Array(0.0018015, dtype=float32),\n",
       "  'wF2': Array(0.01300235, dtype=float32),\n",
       "  'wO': Array(0.01061753, dtype=float32),\n",
       "  'wU': Array(0.00188636, dtype=float32)},\n",
       " {'bC': Array(0.00145247, dtype=float32),\n",
       "  'bF1': Array(0.00011476, dtype=float32),\n",
       "  'bF2': Array(0.00056345, dtype=float32),\n",
       "  'bO': Array(0.00109061, dtype=float32),\n",
       "  'bU': Array(8.637089e-05, dtype=float32),\n",
       "  'wC': Array(0.03620593, dtype=float32),\n",
       "  'wF1': Array(0.0025577, dtype=float32),\n",
       "  'wF2': Array(0.00722106, dtype=float32),\n",
       "  'wO': Array(0.0273155, dtype=float32),\n",
       "  'wU': Array(0.00133963, dtype=float32)},\n",
       " {'bC': Array(0.00081929, dtype=float32),\n",
       "  'bF1': Array(0.00015749, dtype=float32),\n",
       "  'bF2': Array(0.00108281, dtype=float32),\n",
       "  'bO': Array(0.00537025, dtype=float32),\n",
       "  'bU': Array(0.00019396, dtype=float32),\n",
       "  'bY1': Array(0.03360792, dtype=float32),\n",
       "  'wC': Array(0.00909183, dtype=float32),\n",
       "  'wF1': Array(0.00195572, dtype=float32),\n",
       "  'wF2': Array(0.01635675, dtype=float32),\n",
       "  'wO': Array(0.10457181, dtype=float32),\n",
       "  'wU': Array(0.00273188, dtype=float32),\n",
       "  'wY1': Array(0.3076808, dtype=float32)}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(jnp.linalg.norm, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_PARAMS: 1183262\n",
      "DTYPE: float32\n",
      "TOTAL_MEGABYTES: 4.733048\n"
     ]
    }
   ],
   "source": [
    "getsize = lambda s: s.size\n",
    "sizes = jax.tree_util.tree_map(getsize, grads)\n",
    "total_params = 0\n",
    "for layer in sizes:\n",
    "  for _, v in layer.items():\n",
    "    total_params += v\n",
    "\n",
    "print(f\"TOTAL_PARAMS: {total_params}\")\n",
    "print(f\"DTYPE: {grads[0]['bC'].dtype}\")\n",
    "print(f\"TOTAL_MEGABYTES: {total_params*4/1_000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.arange(1, 10)[, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.profiler\n",
    "jax.profiler.save_device_memory_profile('test.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
