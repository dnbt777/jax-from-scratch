{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import optax\n",
    "from tokenizers import CharBPETokenizer\n",
    "import functools\n",
    "import time\n",
    "\n",
    "\n",
    "gpu_device = jax.device_get('gpu')[0]\n",
    "cpu_device = jax.device_get('cpu')[0]\n",
    "# LSTM\n",
    "# xs = B, input_size = B, T, C\n",
    "# h = c = y = B, output_size = B, T, logits_size = B, T, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 84\n",
      "removed: 🫡ɴ𝘂我𝗪ᴇ😎👀𝗼😤📈ʀᴛ#𝗲{ʟ’🤣👌𝘁ɪ🚀🤦🤷~𝗶ʜ𝘀|𝗿`ᴏ️🎉💪‍😁😭😉$👍們🍰𝗱[吧🌑}*]𝗯😆”𝗰𝗵“🧠🤔😢♂^ᴡ𝗻ᴄᴘᴀ走📉🤯☠\n",
      "dog [59 70 62] dog\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "with open('data/dnbt_posts.txt', 'r') as file:\n",
    "  dataset = file.read()\n",
    "\n",
    "removed_chars = []\n",
    "frequencies = []\n",
    "for c in set(dataset):\n",
    "  frequencies.append((dataset.count(c), c, c.isalnum()))\n",
    "  if dataset.count(c) < 50:\n",
    "    removed_chars.append(c)\n",
    "    dataset = dataset.replace(c, '')\n",
    "\n",
    "\n",
    "# tokenize\n",
    "vocab = sorted(list(set(dataset)))\n",
    "print(\"vocab length:\", len(vocab))\n",
    "\n",
    "token_to_char = dict(enumerate(vocab))\n",
    "char_to_token = dict([(v, k) for k, v in token_to_char.items()])\n",
    "decode = lambda tokens: \"\".join([token_to_char[int(token)] for token in tokens])\n",
    "encode = lambda chars: jnp.array([char_to_token[c] for c in chars])\n",
    "\n",
    "dataset_tokens = encode(dataset)\n",
    "split_ratio = 0.9\n",
    "train_tokens = dataset_tokens[:int(len(dataset_tokens)*split_ratio)]\n",
    "test_tokens = dataset_tokens[int(len(dataset_tokens)*split_ratio):]\n",
    "del dataset\n",
    "del dataset_tokens\n",
    "\n",
    "\n",
    "print(\"removed:\", \"\".join(removed_chars))\n",
    "print(\"dog\", encode(\"dog\"), decode(encode(\"dog\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'ɪ', True),\n",
       " (1, 'ɴ', True),\n",
       " (1, 'ʜ', True),\n",
       " (1, 'ᴘ', True),\n",
       " (1, 'ᴛ', True),\n",
       " (1, '☠', False),\n",
       " (1, '們', True),\n",
       " (1, '吧', True),\n",
       " (1, '我', True),\n",
       " (1, '走', True),\n",
       " (1, '𝗪', True),\n",
       " (1, '𝗯', True),\n",
       " (1, '𝗰', True),\n",
       " (1, '𝗱', True),\n",
       " (1, '𝗶', True),\n",
       " (1, '𝗿', True),\n",
       " (1, '𝘂', True),\n",
       " (1, '🌑', False),\n",
       " (1, '🍰', False),\n",
       " (1, '🎉', False),\n",
       " (1, '👀', False),\n",
       " (1, '📈', False),\n",
       " (1, '📉', False),\n",
       " (1, '😁', False),\n",
       " (1, '😉', False),\n",
       " (1, '😢', False),\n",
       " (1, '😤', False),\n",
       " (1, '🚀', False),\n",
       " (1, '🤦', False),\n",
       " (1, '🤯', False),\n",
       " (1, '🤷', False),\n",
       " (1, '🧠', False),\n",
       " (2, '|', False),\n",
       " (2, 'ʀ', True),\n",
       " (2, 'ʟ', True),\n",
       " (2, 'ᴏ', True),\n",
       " (2, '\\u200d', False),\n",
       " (2, '♂', False),\n",
       " (2, '𝗵', True),\n",
       " (2, '𝗻', True),\n",
       " (2, '𝗼', True),\n",
       " (2, '𝘀', True),\n",
       " (2, '𝘁', True),\n",
       " (2, '😆', False),\n",
       " (3, 'ᴀ', True),\n",
       " (3, 'ᴄ', True),\n",
       " (3, 'ᴡ', True),\n",
       " (3, '’', False),\n",
       " (3, '“', False),\n",
       " (3, '”', False),\n",
       " (3, '️', False),\n",
       " (3, '𝗲', True),\n",
       " (3, '👌', False),\n",
       " (3, '👍', False),\n",
       " (3, '😭', False),\n",
       " (3, '🤣', False),\n",
       " (4, '}', False),\n",
       " (4, '😎', False),\n",
       " (4, '🤔', False),\n",
       " (6, '{', False),\n",
       " (6, 'ᴇ', True),\n",
       " (6, '💪', False),\n",
       " (12, '[', False),\n",
       " (12, ']', False),\n",
       " (20, '~', False),\n",
       " (24, '\\U0001fae1', False),\n",
       " (31, '*', False),\n",
       " (34, '^', False),\n",
       " (37, '$', False),\n",
       " (40, '`', False),\n",
       " (49, '#', False),\n",
       " (96, '%', False),\n",
       " (105, '=', False),\n",
       " (135, 'X', True),\n",
       " (138, '+', False),\n",
       " (145, 'Q', True),\n",
       " (171, 'Z', True),\n",
       " (175, '…', False),\n",
       " (187, '&', False),\n",
       " (194, ';', False),\n",
       " (209, 'J', True),\n",
       " (229, '8', True),\n",
       " (250, 'K', True),\n",
       " (286, 'V', True),\n",
       " (297, 'U', True),\n",
       " (315, '6', True),\n",
       " (325, 'F', True),\n",
       " (334, '9', True),\n",
       " (378, '-', False),\n",
       " (384, \"'\", False),\n",
       " (410, '5', True),\n",
       " (410, 'Y', True),\n",
       " (417, 'D', True),\n",
       " (434, '3', True),\n",
       " (436, '!', False),\n",
       " (444, '4', True),\n",
       " (459, 'W', True),\n",
       " (465, '\"', False),\n",
       " (474, 'G', True),\n",
       " (477, 'H', True),\n",
       " (480, 'q', True),\n",
       " (501, 'C', True),\n",
       " (503, 'R', True),\n",
       " (516, '?', False),\n",
       " (531, 'N', True),\n",
       " (546, '7', True),\n",
       " (568, '(', False),\n",
       " (589, ')', False),\n",
       " (627, 'P', True),\n",
       " (643, '2', True),\n",
       " (670, 'E', True),\n",
       " (682, 'L', True),\n",
       " (810, 'O', True),\n",
       " (822, 'S', True),\n",
       " (898, 'M', True),\n",
       " (960, '1', True),\n",
       " (1040, '_', False),\n",
       " (1120, 'z', True),\n",
       " (1124, 'B', True),\n",
       " (1173, 'j', True),\n",
       " (1219, 'A', True),\n",
       " (1304, 'T', True),\n",
       " (1320, '0', True),\n",
       " (1452, 'I', True),\n",
       " (1660, 'x', True),\n",
       " (2170, ',', False),\n",
       " (2459, '.', False),\n",
       " (2854, '/', False),\n",
       " (3812, 'v', True),\n",
       " (3916, '🛑', False),\n",
       " (4470, 'k', True),\n",
       " (4567, '@', False),\n",
       " (5029, ':', False),\n",
       " (6053, 'b', True),\n",
       " (6701, 'w', True),\n",
       " (7380, 'f', True),\n",
       " (9323, 'g', True),\n",
       " (10381, 'm', True),\n",
       " (10829, 'c', True),\n",
       " (12176, 'u', True),\n",
       " (12812, 'd', True),\n",
       " (12825, 'y', True),\n",
       " (13082, 'p', True),\n",
       " (14943, 'h', True),\n",
       " (19799, 'l', True),\n",
       " (22950, 'r', True),\n",
       " (23399, 'n', True),\n",
       " (23707, 's', True),\n",
       " (25692, 'i', True),\n",
       " (26141, 'a', True),\n",
       " (28892, '\\n', False),\n",
       " (30301, 'o', True),\n",
       " (34031, 't', True),\n",
       " (42329, 'e', True),\n",
       " (75840, ' ', False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm network & other functions\n",
    "def init_LSTM_params(key, lstm_layers, input_size, model_size, output_size):\n",
    "  param_sets = 8 # manual, idc\n",
    "  keys = random.split(key, param_sets*lstm_layers + 2)\n",
    "  hxconcat_size = model_size + model_size\n",
    "  he = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / shape[0])\n",
    "  # supposedly xavier is better for networks using tanh\n",
    "  xavier = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / (shape[0] + shape[1]))\n",
    "  params = [\n",
    "    {\n",
    "      \"wU\" : xavier(keys[param_sets*i + 0], (hxconcat_size, model_size)),\n",
    "      \"bU\" : jnp.zeros((model_size,)),\n",
    "      \"wC\" : xavier(keys[param_sets*i + 6], (hxconcat_size, model_size)),\n",
    "      \"bC\" : jnp.zeros((model_size,)),\n",
    "      \"wF\": xavier(keys[param_sets*i + 1], (hxconcat_size, model_size)),\n",
    "      \"bF\": jnp.zeros((model_size,)),\n",
    "      \"wO\" : xavier(keys[param_sets*i + 3], (hxconcat_size, model_size)),\n",
    "      \"bO\" : jnp.zeros((model_size,)),\n",
    "      \"h0\" : jnp.zeros((model_size,)),\n",
    "      \"c0\" : jnp.zeros((model_size,)),\n",
    "      #\"h0\" : random.normal(keys[param_sets*i + 4], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "      #\"c0\" : random.normal(keys[param_sets*i + 5], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "    }\n",
    "    for i in range(lstm_layers)\n",
    "  ]\n",
    "  params[0].update(\n",
    "    {\n",
    "    # then embedding table weight and bias\n",
    "    \"wEM\" : xavier(keys[param_sets*(param_sets - 1) + 2], (input_size, model_size)),\n",
    "    \"bEM\" : jnp.zeros((model_size,)),\n",
    "\n",
    "  })\n",
    "  params[-1].update(\n",
    "    {\n",
    "      # this is for the y layer, which i am probably imlementing wrong.\n",
    "      \"wY1\" : xavier(keys[param_sets*(lstm_layers-1) + 4], (model_size, model_size)),\n",
    "      \"bY1\" : jnp.zeros((model_size,)),\n",
    "      \"wY2\" : xavier(keys[param_sets*(lstm_layers-1) + 5], (model_size, output_size)),\n",
    "      \"bY2\" : jnp.zeros((output_size,)),\n",
    "    }\n",
    "  )\n",
    "  return params\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def dropout(dropout_key, original_tensor, dropout_rate):\n",
    "  # generate random of same shape\n",
    "  dropout_probs = random.uniform(dropout_key, shape=original_tensor.shape)\n",
    "  # mask = random < dropout_rate\n",
    "  mask = (dropout_probs > dropout_rate) / (1 - dropout_rate) # scale to keep avg the same\n",
    "  return original_tensor * mask\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[]) # static dropout rate?\n",
    "def lstm_step(step_dropout_key, lstm_layer_params, layer_h, layer_c, current_xt, dropout_rate):\n",
    "  hxconcat = jax.lax.concatenate([layer_h, current_xt], dimension=1) #B, h ++ B, C => B, h+c\n",
    "  # update gate\n",
    "  forget_gate = jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wF\"] + lstm_layer_params[\"bF\"])\n",
    "  #update = dropout(step_dropout_keys[0], update, dropout_rate)\n",
    "\n",
    "  # forget\n",
    "  layer_c = layer_c * forget_gate\n",
    "\n",
    "  input_node = jax.nn.tanh(hxconcat @ lstm_layer_params[\"wC\"] + lstm_layer_params[\"bC\"])\n",
    "  #candidate = dropout(step_dropout_keys[1], candidate, dropout_rate)\n",
    "  update = jax.nn.sigmoid(\n",
    "              hxconcat @ lstm_layer_params[\"wU\"] + lstm_layer_params[\"bU\"]\n",
    "            )\n",
    "  input_gate =  update * input_node\n",
    "\n",
    "  # update\n",
    "  layer_c = layer_c + input_gate\n",
    "\n",
    "  # output\n",
    "  layer_h = jax.nn.tanh(layer_c) * jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wO\"] + lstm_layer_params[\"bO\"]) # (B, model_size)\n",
    "\n",
    "  next_layer_xt = dropout(step_dropout_key, layer_h, dropout_rate) # the next layer's input x is the current layer's hidden state\n",
    "  # karpathy: dropout after EACH LAYER not several times in the block. lol.\n",
    "\n",
    "  # i may also need to do dropout horizontally (i.e. dropout the hidden state memory each block)\n",
    "\n",
    "  return (layer_h, layer_c), next_layer_xt\n",
    "\n",
    "\n",
    "# LSTM forward\n",
    "import functools\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate):\n",
    "  batches = xembeds_batch.shape[0]\n",
    "  lstm_layers = len(lstm_params)\n",
    "  model_size = lstm_params[0][\"h0\"].size\n",
    "  # initialize h and c as random/learnable params\n",
    "  #h = jnp.tile(lstm_params[0][\"h0\"], (batches, lstm_layers, 1)) # B, lstm_layer, h_size\n",
    "  #c = jnp.tile(lstm_params[0][\"c0\"], (batches, lstm_layers, 1)) # B, lstm_layer, c_size\n",
    "  # wait.. these are the same for all of the layers.. maybe they shouldn't be\n",
    "  T = xembeds_batch.shape[1]\n",
    "  # take xembeds_batch and pass each xt through the same SINGULAR block. don't update the weight layer. there is only one layer.\n",
    "  dropout_keys = random.split(dropout_key, lstm_layers)\n",
    "\n",
    "  # for each layer:\n",
    "    # scan over xt\n",
    "    # carry : h, c\n",
    "    # a: xt\n",
    "    # b: h,c\n",
    "    # f = lambda ((h, c), xt) : lstm_step(h, c, xt, everything else) => h, c\n",
    "    # scans over xt\n",
    "    # for next layer: xt = h of previous layer. h = h0 and c = c0\n",
    "  \n",
    "  current_embeddings_batch = jnp.transpose(xembeds_batch, (1, 0, 2)) # B, T, C => T, B, C\n",
    "    # The reason for this is that jax.lax.scan only uses the leading dim. why? idk. its dumb, it needs an axis arg so i can scan over whatever\n",
    "\n",
    "  for lstm_layer in range(lstm_layers):\n",
    "    h = jnp.tile(lstm_params[lstm_layer][\"h0\"], (batches, 1))\n",
    "    c = jnp.tile(lstm_params[lstm_layer][\"c0\"], (batches, 1))\n",
    "    # zeroes makes the backprop faster\n",
    "    #h = jnp.zeros((batches, model_size))\n",
    "    #c = jnp.zeros((batches, model_size))\n",
    "    layer_dropout_key = dropout_keys[lstm_layer] # it doesnt matter if this is the same across all layers\n",
    "    # scan should be inexpensive since layer size is small while t size is usually LARGE\n",
    "    # scan :: (c -> a -> (c, b)) -> c -> [a] -> (c, [b])\n",
    "    # scan :: scanfunc -> h_and_c -> xs -> (h_and_c_final, hs_to_be_used_as_input_xt_in_next_layer)\n",
    "    # scanfunc :: (c -> a -> (c, b))\n",
    "    scanfunc = lambda hc, xt : lstm_step(layer_dropout_key, lstm_params[lstm_layer], hc[0], hc[1], xt, dropout_rate)\n",
    "      # for xs: scan along the t dimension! it scans along B by default\n",
    "      # to fix this, we transpose xs with jnp.transpose(current_embeddings_batch, (1, 0, 2))\n",
    "    current_embeddings_batch = jax.lax.scan(scanfunc, (h, c), current_embeddings_batch)[1] # (c, [b]) => [b] ==> B, T, C\n",
    "  \n",
    "\n",
    "  # finally turn current_embeddings_batch into ys (logits)\n",
    "  hs = jnp.transpose(current_embeddings_batch, (1, 0, 2)) # T, B, C => B, T, C\n",
    "  ys = jax.nn.relu(hs @ lstm_params[-1]['wY1'] + lstm_params[-1][\"bY1\"]) # B, T, model_size => B, T, vocab_size\n",
    "  ys = ys @ lstm_params[-1]['wY2'] + lstm_params[-1][\"bY2\"]\n",
    "  return ys\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def loss_func(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logits = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def loss_and_value(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logits = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  predictions = jnp.argmax(logprobs, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss, predictions\n",
    "\n",
    "\n",
    "jitted_backwards_loss = jax.jit(jax.value_and_grad(loss_func, argnums=1), static_argnames=[])\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['vocab_size'])\n",
    "def embed(lstm_params, xtokens, vocab_size=len(vocab)):\n",
    "  xs_one_hot = jax.nn.one_hot(xtokens, vocab_size, axis=-1) #B, T, vocab_size\n",
    "  activations = xs_one_hot @ lstm_params[0][\"wEM\"] + lstm_params[0][\"bEM\"]\n",
    "  return activations\n",
    "\n",
    "# make optimizer a static arg in jit or it breaks\n",
    "@functools.partial(jax.jit, static_argnames=[\"optimizer\"])\n",
    "def train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer):\n",
    "  step_loss, grads = jitted_backwards_loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate)\n",
    "  param_updates, updated_opt_state = optimizer.update(grads, opt_state, lstm_params)\n",
    "  updated_lstm_params = optax.apply_updates(lstm_params, param_updates) \n",
    "  return updated_lstm_params, updated_opt_state, step_loss, grads\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laddering params\n",
    "lstm_layers = 3\n",
    "model_size = 512# 512\n",
    "\n",
    "# laddering\n",
    "# memorize, generalize, move up in complexity. memorize, generalize, move up in complexity.\n",
    "# until desired complexity is reached\n",
    "\n",
    "# strategy: aim for extremely low error*epochs\n",
    "SEQ_LEN      = [2,    4,    10,   25,   50,   50,   50,   50,   ]\n",
    "LR           = [2e-2, 2e-2, 2e-2, 2e-2, 2e-2, 1e-2, 5e-3, 2e-3,]\n",
    "DROPOUT_RATE = [0.00, 0.00, 0.00, 0.1,  0.20, 0.20, 0.20, 0.20]\n",
    "EPOCHS       = [1,    1,    1,    1,    1,    1,    1,    1,  ]\n",
    "rungs = list(zip(SEQ_LEN, LR, DROPOUT_RATE, EPOCHS)) # causes problems if generator\n",
    "\n",
    "# test epoch params\n",
    "# use this to find the optimal LR for laddering steps. could be automated but whatever\n",
    "target_rung = 2\n",
    "sequence_length, lr, dropout_rate, test_epochs = rungs[target_rung]\n",
    "resume_checkpoint = False # saves the checkpoint for rung-1, and reruns rung $rung. this was you dont have to re-climb every rung\n",
    "\n",
    "print_every = 1000\n",
    "train_batch_size = 100\n",
    "val_batch_size = 200\n",
    "\n",
    "# loss: 2.2663 || val_loss: 1.6317 val_acc: 0.5529 || 5 epochs total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (2) | \"ep\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 0/1234 || samples/sec: 17840 || loss: 5.0435 || val_loss: 4.9277 val_acc: 0.1700 || LR = 0.020000\n",
      "TARGET (2) | \"um\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 1000/1234 || samples/sec: 14780 || loss: 2.7572 || val_loss: 2.9164 val_acc: 0.2325 || LR = 0.020000\n",
      "TARGET (4) | \"eply\"\n",
      "PRED   (4) | \"epue\"\n",
      "r,e,s | 1/8, 0/1, 0/739 || samples/sec: 19693 || loss: 2.7359 || val_loss: 2.6774 val_acc: 0.2637 || LR = 0.020000\n",
      "TARGET (10) | \"eply: @tri\"\n",
      "PRED   (10) | \"eply: @aue\"\n",
      "r,e,s | 2/8, 0/1, 0/335 || samples/sec: 7177 || loss: 2.6325 || val_loss: 2.3668 val_acc: 0.3500 || LR = 0.020000\n",
      "ended after rung 2\n"
     ]
    }
   ],
   "source": [
    "# run test epoch\n",
    "\n",
    "# setup vars\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "losses = []\n",
    "start = time.time()\n",
    "\n",
    "# train\n",
    "# train rungs\n",
    "for r, rung in enumerate(rungs):\n",
    "  if resume_checkpoint and r < target_rung:\n",
    "    # skip rungs until the target rung\n",
    "    continue\n",
    "\n",
    "  if r > target_rung:\n",
    "    print(f\"ended after rung {r-1}\")\n",
    "    break\n",
    "  \n",
    "  if resume_checkpoint and r == target_rung:\n",
    "    lstm_params = lstm_params_checkpoint\n",
    "\n",
    "  sequence_length, lr, dropout_rate, epochs = rung\n",
    "  # initialize if first rung\n",
    "  if r == 0:\n",
    "    lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "    optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "    opt_state = optimizer.init(lstm_params)\n",
    "  else:\n",
    "    opt_state.hyperparams['learning_rate'] = lr\n",
    "\n",
    "  # train\n",
    "  for epoch in range(epochs):\n",
    "      steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "      for step in range(steps): # probably wrong but w/e\n",
    "        # train\n",
    "        # B, T where T = sequence_length\n",
    "        train_data_idx = step*sequence_length*train_batch_size\n",
    "        next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "        xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "        ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "\n",
    "        dropout_key = random.PRNGKey(epoch*steps + step) # unique for every step\n",
    "        lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer)\n",
    "\n",
    "        losses.append(step_loss)\n",
    "\n",
    "        if ((epoch*step + step) % print_every == 0) or (epoch + steps == 0):\n",
    "          end = time.time()\n",
    "          duration = end - start\n",
    "          # train inference example (no dropout)\n",
    "          xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "          last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0) # B, C\n",
    "          prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "          # val batch\n",
    "          j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "          val_idx = j*val_batch_size*sequence_length\n",
    "          next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "          xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "          ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "          \n",
    "          val_loss, prediction_val_batch = loss_and_value(dropout_key, lstm_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "          val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "          # print train status\n",
    "          x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "          y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "          yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "          #print(f'INPUT  ({len(x)}) | \"{x}\"')\n",
    "          lines = [\n",
    "            f'TARGET ({len(y)}) | \"{y}\"',\n",
    "            f'PRED   ({len(yhat)}) | \"{yhat}\"',\n",
    "            f\"r,e,s | {r}/{len(rungs)}, {epoch}/{epochs}, {step}/{steps} || samples/sec: {train_batch_size*print_every/(duration):0.0f} || \"\n",
    "            f\"loss: {sum(losses)/len(losses):1.4f} || val_loss: {val_loss:1.4f} val_acc: {val_accuracy:1.4f} || \" \n",
    "            f\"LR = {opt_state.hyperparams['learning_rate']:0.6f}\",\n",
    "          ]\n",
    "          print(\"\\n\".join(lines))\n",
    "          start = time.time()\n",
    "  if r < target_rung:\n",
    "    # stop saving checkpoint after training the target rung\n",
    "    lstm_params_checkpoint = lstm_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new rung 0\n",
      "TARGET (2) | \"ep\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 0/1234 || samples/sec: 3394 || loss: 5.0435 || val_loss: 4.9277 val_acc: 0.1700 || LR = 0.020000\n",
      "TARGET (2) | \"ep\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 100/1234 || samples/sec: 16252 || loss: 3.2576 || val_loss: 3.4519 val_acc: 0.2000 || LR = 0.020000\n",
      "TARGET (2) | \"sc\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 200/1234 || samples/sec: 16144 || loss: 3.0758 || val_loss: 2.6926 val_acc: 0.2325 || LR = 0.020000\n",
      "TARGET (2) | \"nt\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 300/1234 || samples/sec: 15518 || loss: 2.9792 || val_loss: 2.6449 val_acc: 0.2400 || LR = 0.020000\n",
      "TARGET (2) | \"ik\"\n",
      "PRED   (2) | \"yn\"\n",
      "r,e,s | 0/8, 0/1, 400/1234 || samples/sec: 18901 || loss: 2.9016 || val_loss: 2.7475 val_acc: 0.2875 || LR = 0.020000\n",
      "TARGET (2) | \"  \"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 500/1234 || samples/sec: 16580 || loss: 2.8631 || val_loss: 2.6574 val_acc: 0.2925 || LR = 0.020000\n",
      "TARGET (2) | \"ce\"\n",
      "PRED   (2) | \"io\"\n",
      "r,e,s | 0/8, 0/1, 600/1234 || samples/sec: 19732 || loss: 2.8318 || val_loss: 2.6309 val_acc: 0.2625 || LR = 0.020000\n",
      "TARGET (2) | \"r \"\n",
      "PRED   (2) | \"t \"\n",
      "r,e,s | 0/8, 0/1, 700/1234 || samples/sec: 17938 || loss: 2.8106 || val_loss: 2.5344 val_acc: 0.3125 || LR = 0.020000\n",
      "TARGET (2) | \"ad\"\n",
      "PRED   (2) | \"et\"\n",
      "r,e,s | 0/8, 0/1, 800/1234 || samples/sec: 17296 || loss: 2.7898 || val_loss: 2.3705 val_acc: 0.3725 || LR = 0.020000\n",
      "TARGET (2) | \"a \"\n",
      "PRED   (2) | \"tn\"\n",
      "r,e,s | 0/8, 0/1, 900/1234 || samples/sec: 19206 || loss: 2.7729 || val_loss: 2.8531 val_acc: 0.2775 || LR = 0.020000\n",
      "TARGET (2) | \"um\"\n",
      "PRED   (2) | \"  \"\n",
      "r,e,s | 0/8, 0/1, 1000/1234 || samples/sec: 18457 || loss: 2.7572 || val_loss: 2.9164 val_acc: 0.2325 || LR = 0.020000\n",
      "TARGET (2) | \"ly\"\n",
      "PRED   (2) | \"ly\"\n",
      "r,e,s | 0/8, 0/1, 1100/1234 || samples/sec: 19145 || loss: 2.7474 || val_loss: 2.5498 val_acc: 0.2925 || LR = 0.020000\n",
      "TARGET (2) | \"re\"\n",
      "PRED   (2) | \"n \"\n",
      "r,e,s | 0/8, 0/1, 1200/1234 || samples/sec: 14029 || loss: 2.7390 || val_loss: 2.5747 val_acc: 0.3375 || LR = 0.020000\n",
      "new rung 1\n",
      "TARGET (4) | \"eply\"\n",
      "PRED   (4) | \"epue\"\n",
      "r,e,s | 1/8, 0/1, 0/739 || samples/sec: 3414 || loss: 2.7359 || val_loss: 2.6774 val_acc: 0.2637 || LR = 0.020000\n",
      "TARGET (4) | \"scri\"\n",
      "PRED   (4) | \"   o\"\n",
      "r,e,s | 1/8, 0/1, 100/739 || samples/sec: 2869 || loss: 2.7210 || val_loss: 2.4939 val_acc: 0.3237 || LR = 0.020000\n",
      "TARGET (4) | \"ike \"\n",
      "PRED   (4) | \"yne \"\n",
      "r,e,s | 1/8, 0/1, 200/739 || samples/sec: 8850 || loss: 2.7046 || val_loss: 2.4320 val_acc: 0.3287 || LR = 0.020000\n",
      "TARGET (4) | \"cert\"\n",
      "PRED   (4) | \"tor \"\n",
      "r,e,s | 1/8, 0/1, 300/739 || samples/sec: 10106 || loss: 2.6903 || val_loss: 2.5155 val_acc: 0.3250 || LR = 0.020000\n",
      "TARGET (4) | \"ade \"\n",
      "PRED   (4) | \"ene \"\n",
      "r,e,s | 1/8, 0/1, 400/739 || samples/sec: 9793 || loss: 2.6772 || val_loss: 2.3128 val_acc: 0.3637 || LR = 0.020000\n",
      "TARGET (4) | \"umbe\"\n",
      "PRED   (4) | \" tee\"\n",
      "r,e,s | 1/8, 0/1, 500/739 || samples/sec: 10242 || loss: 2.6622 || val_loss: 2.4757 val_acc: 0.3212 || LR = 0.020000\n",
      "TARGET (4) | \"re, \"\n",
      "PRED   (4) | \"u   \"\n",
      "r,e,s | 1/8, 0/1, 600/739 || samples/sec: 10773 || loss: 2.6494 || val_loss: 2.3292 val_acc: 0.3462 || LR = 0.020000\n",
      "TARGET (4) | \"hess\"\n",
      "PRED   (4) | \"o   \"\n",
      "r,e,s | 1/8, 0/1, 700/739 || samples/sec: 10826 || loss: 2.6380 || val_loss: 2.2231 val_acc: 0.3775 || LR = 0.020000\n",
      "new rung 2\n",
      "TARGET (10) | \"eply: @tri\"\n",
      "PRED   (10) | \"eply: @aue\"\n",
      "r,e,s | 2/8, 0/1, 0/335 || samples/sec: 3427 || loss: 2.6325 || val_loss: 2.3668 val_acc: 0.3500 || LR = 0.020000\n",
      "TARGET (10) | \"    reply:\"\n",
      "PRED   (10) | \"     eply:\"\n",
      "r,e,s | 2/8, 0/1, 100/335 || samples/sec: 1935 || loss: 2.6150 || val_loss: 2.4232 val_acc: 0.3595 || LR = 0.020000\n",
      "TARGET (10) | \"umbers 1-1\"\n",
      "PRED   (10) | \" seer  t0 \"\n",
      "r,e,s | 2/8, 0/1, 200/335 || samples/sec: 3742 || loss: 2.5970 || val_loss: 2.1697 val_acc: 0.3895 || LR = 0.020000\n",
      "TARGET (10) | \"long line?\"\n",
      "PRED   (10) | \"toog tong \"\n",
      "r,e,s | 2/8, 0/1, 300/335 || samples/sec: 4092 || loss: 2.5793 || val_loss: 2.1895 val_acc: 0.4065 || LR = 0.020000\n",
      "new rung 3\n",
      "TARGET (25) | \"eply: @trickylabyrinth A \"\n",
      "PRED   (25) | \"eply: @leanhe yrea ngeewh\"\n",
      "r,e,s | 3/8, 0/1, 0/140 || samples/sec: 2250 || loss: 2.5733 || val_loss: 2.1076 val_acc: 0.4178 || LR = 0.020000\n",
      "TARGET (25) | \"far Used to think it was \"\n",
      "PRED   (25) | \"tone o   th theng tn tin \"\n",
      "r,e,s | 3/8, 0/1, 100/140 || samples/sec: 1141 || loss: 2.5571 || val_loss: 2.0725 val_acc: 0.4260 || LR = 0.020000\n",
      "new rung 4\n",
      "TARGET (50) | \"eply: @trickylabyrinth A better model architecture\"\n",
      "PRED   (50) | \"e ly: @leotk  lcl  ng et @etter tare stneeenh tire\"\n",
      "r,e,s | 4/8, 0/1, 0/70 || samples/sec: 1825 || loss: 2.5503 || val_loss: 1.9748 val_acc: 0.4532 || LR = 0.020000\n",
      "new rung 5\n",
      "TARGET (50) | \"eply: @trickylabyrinth A better model architecture\"\n",
      "PRED   (50) | \"e ly: @yeetke enl  ng eth@e ter tare st ee n  tire\"\n",
      "r,e,s | 5/8, 0/1, 0/70 || samples/sec: 950 || loss: 2.5398 || val_loss: 1.9481 val_acc: 0.4586 || LR = 0.010000\n",
      "new rung 6\n",
      "TARGET (50) | \"eply: @trickylabyrinth A better model architecture\"\n",
      "PRED   (50) | \"e ly: @yeatk  ynl_ ng et ie tir tare  t ee n  tire\"\n",
      "r,e,s | 6/8, 0/1, 0/70 || samples/sec: 1142 || loss: 2.5280 || val_loss: 1.9247 val_acc: 0.4641 || LR = 0.005000\n",
      "new rung 7\n",
      "TARGET (50) | \"eply: @trickylabyrinth A better model architecture\"\n",
      "PRED   (50) | \"e ly: @aeatk  ynl  ngeet @e ter tare  t ee n  tise\"\n",
      "r,e,s | 7/8, 0/1, 0/70 || samples/sec: 1045 || loss: 2.5158 || val_loss: 1.9084 val_acc: 0.4711 || LR = 0.002000\n"
     ]
    }
   ],
   "source": [
    "# train (laddering)\n",
    "\n",
    "\n",
    "# init some parameters\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "train_batch_size = 100\n",
    "val_batch_size = 200\n",
    "print_every = 100\n",
    "j = 0\n",
    "losses = []\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# train rungs\n",
    "for r, rung in enumerate(rungs):\n",
    "  print(f\"new rung {r}\")\n",
    "  sequence_length, lr, dropout_rate, epochs = rung\n",
    "  # initialize if first rung\n",
    "  if r == 0:\n",
    "    lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "    optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "    opt_state = optimizer.init(lstm_params)\n",
    "  else:\n",
    "    opt_state.hyperparams['learning_rate'] = lr\n",
    "\n",
    "  # train\n",
    "  for epoch in range(epochs):\n",
    "      steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "      for step in range(steps): # probably wrong but w/e\n",
    "        # train\n",
    "        # B, T where T = sequence_length\n",
    "        train_data_idx = step*sequence_length*train_batch_size\n",
    "        next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "        xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "        ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "\n",
    "        dropout_key = random.PRNGKey(epoch*steps + step) # unique for every step\n",
    "        lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer)\n",
    "\n",
    "        losses.append(step_loss)\n",
    "\n",
    "        if ((epoch*step + step) % print_every == 0) or (epoch + steps == 0):\n",
    "          end = time.time()\n",
    "          duration = end - start\n",
    "          # train inference example (no dropout)\n",
    "          xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "          last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0) # B, C\n",
    "          prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "          # val batch\n",
    "          j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "          val_idx = j*val_batch_size*sequence_length\n",
    "          next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "          xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "          ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "          \n",
    "          val_loss, prediction_val_batch = loss_and_value(dropout_key, lstm_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "          val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "          # print train status\n",
    "          x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "          y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "          yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "          #print(f'INPUT  ({len(x)}) | \"{x}\"')\n",
    "          lines = [\n",
    "            f'TARGET ({len(y)}) | \"{y}\"',\n",
    "            f'PRED   ({len(yhat)}) | \"{yhat}\"',\n",
    "            f\"r,e,s | {r}/{len(rungs)}, {epoch}/{epochs}, {step}/{steps} || samples/sec: {train_batch_size*print_every/(duration):0.0f} || \"\n",
    "            f\"loss: {sum(losses)/len(losses):1.4f} || val_loss: {val_loss:1.4f} val_acc: {val_accuracy:1.4f} || \" \n",
    "            f\"LR = {opt_state.hyperparams['learning_rate']:0.6f}\",\n",
    "          ]\n",
    "          print(\"\\n\".join(lines))\n",
    "          start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "# train engine parameters\n",
    "\n",
    "# this is the function that takes the current hyperparameters\n",
    "# and makes candidate ones to test.\n",
    "#@jax.jit\n",
    "from itertools import product\n",
    "from jax import random as jrand\n",
    "def make_candidates(key, current_sequence_length, current_learning_rate, current_dropout_rate):\n",
    "  # get lr candidates\n",
    "  upscale = jnp.array([1, 2, 10, 100], dtype=jnp.float32)\n",
    "  downscale = 1.0 / upscale # 8x, 4x, 2x, 1x, 0.5x, 0.25x, etc\n",
    "  scale = jnp.concatenate([upscale, downscale])\n",
    "  lr_candidates = current_learning_rate * scale\n",
    "\n",
    "  # get seq length candidates (this does have an effect)\n",
    "  #sequence_length_candidates = jnp.array(list(set([current_sequence_length, 2, 4, 8, 15, 25, 50, 100])))\n",
    "  sequence_length_candidates = jnp.array([current_sequence_length]) # dont change this\n",
    "\n",
    "  # future: dropout\n",
    "  dropout_candidates = jnp.array(list(set([0, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])))\n",
    "\n",
    "  # get all possible combinations as a generator\n",
    "  # randomly shuffle by preshuffling the inputs\n",
    "  keys = jrand.split(key, 10)\n",
    "  candidates = product(\n",
    "    jrand.permutation(keys[0], sequence_length_candidates, independent=True),\n",
    "    jrand.permutation(keys[1], lr_candidates, independent=True),\n",
    "    jrand.permutation(keys[2], dropout_candidates, independent=True),\n",
    "  )\n",
    "\n",
    "  return candidates\n",
    "\n",
    "\n",
    "## PARAMETERS ##\n",
    "\n",
    "# every n epochs, do another hyperparameter search\n",
    "# lookahead k steps or epochs.\n",
    "# pick the best set of hyperparameters and do the next n epochs with them. repeat\n",
    "# in the future, update whenever final_loss < 0.95*start_loss\n",
    "retune_min_epochs = 20 # how many epochs minimum to train for before rechecking hyperparameters. set super high for normal h tuning\n",
    "lookahead_steps = 30 # steps to train to test candidate hyperparameters\n",
    "candidate_eval_func = lambda final_loss: -final_loss # in this case, candidates are evaluated higher if their final loss is low\n",
    "\n",
    "initial_lr = 3e-4\n",
    "\n",
    "\n",
    "# model params\n",
    "lstm_layers = 2\n",
    "model_size = 512\n",
    "initial_sequence_length = 100\n",
    "initial_dropout_rate = 0.4 # for now\n",
    "\n",
    "candidate_limit = 100\n",
    "\n",
    "\n",
    "# general testing params\n",
    "epochs = 10000\n",
    "print_every = 100\n",
    "train_batch_size = 100\n",
    "val_batch_size = 200\n",
    "\n",
    "test_key = jrand.PRNGKey(1203)\n",
    "print(len(list(make_candidates(test_key, initial_lr, initial_sequence_length, 0.1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retuning: initial hyperparameter tuning\n",
      "this will be slow as functions are jitted\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "old: todo\n",
      "new: (Array(100, dtype=int32), Array(0.00294, dtype=float32), Array(0.8, dtype=float32)) => -3.318880081176758\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"e toetot  teoeeee te teee toe  tot toete   ee teaeeetee tooeee      reply: @ooo  n     @oee    to t \"\n",
      "e:0/10000 s:33/34 || samples/sec: 14312 || loss: 2.8495 || val_loss: 3.2569 val_acc: 0.2062 || LR = 0.002940\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  tt ton  tntet eean tene ao   ton aersl   et tnton te  antor       reply: @aoteen   B @n   e  tt ne\"\n",
      "e:1/10000 s:33/34 || samples/sec: 1316590 || loss: 2.5645 || val_loss: 2.5969 val_acc: 0.2580 || LR = 0.002940\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"e tnttonh tntet eetn tete to t ton aetrr   et tntol teu tntol       reply: @aaleinhBAB @0   e  tt le\"\n",
      "e:2/10000 s:33/34 || samples/sec: 1502124 || loss: 2.4211 || val_loss: 2.3734 val_acc: 0.2912 || LR = 0.002881\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"e tn tonh t tett  tn tet  to t tonhaearrt het tnton teu tnton       reply: @aaceenABAP @n   er tntne\"\n",
      "e:3/10000 s:33/34 || samples/sec: 1502851 || loss: 2.3408 || val_loss: 2.2511 val_acc: 0.3138 || LR = 0.002881\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"e tnethnh t tette tf tet  to t ton tealnt het tnton tot t ton       reply: @aaciegABAP @    er tntle\"\n",
      "e:4/10000 s:33/34 || samples/sec: 1379675 || loss: 2.2625 || val_loss: 2.1551 val_acc: 0.3319 || LR = 0.002824\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t tn thnh tntetde tf tatd to t ton teaert tet t ton tot t ton       reply: @seswigABAP @ d  er tntne\"\n",
      "e:5/10000 s:33/34 || samples/sec: 1299801 || loss: 2.1907 || val_loss: 2.0692 val_acc: 0.3475 || LR = 0.002824\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t tn thth t tetdt tf tanl to t ton teaert tot t ton tot t ton       reply: @aadwigABAP @ d  er tnele\"\n",
      "e:6/10000 s:33/34 || samples/sec: 1247597 || loss: 2.1242 || val_loss: 1.9969 val_acc: 0.3613 || LR = 0.002767\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t tn thth t tetdt tf tavleto t ton teaert tet t tor tot t tor       reply: @sedwigABAP @ dther tnere\"\n",
      "e:7/10000 s:33/34 || samples/sec: 1283248 || loss: 2.0883 || val_loss: 1.9339 val_acc: 0.3736 || LR = 0.002767\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t ts thth t tetdt tf tavleto t ton teaert tot t ton tot t ton      rreply: @sudwigABAP @ dther txpre\"\n",
      "e:8/10000 s:33/34 || samples/sec: 1310093 || loss: 2.0527 || val_loss: 1.8830 val_acc: 0.3848 || LR = 0.002712\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  tn thth t tetdt tf tavleto t ton teaert tot t ton tot tnton       reply: @yudwigABAP @ dther tnpne\"\n",
      "e:9/10000 s:33/34 || samples/sec: 1322915 || loss: 1.9920 || val_loss: 1.8391 val_acc: 0.3948 || LR = 0.002712\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t ts thth t tetdt tf tavl to t ton teaert iot t tonttot tntont      reply: @sudwigABAP @ dther tnpnp\"\n",
      "e:10/10000 s:33/34 || samples/sec: 1318841 || loss: 1.9560 || val_loss: 1.7992 val_acc: 0.4041 || LR = 0.002658\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t tetde tf tavl to t ton teaert iot t tonttot tntont      reply: @yudwigABAP @ duher tapnp\"\n",
      "e:11/10000 s:33/34 || samples/sec: 1319024 || loss: 1.9322 || val_loss: 1.7648 val_acc: 0.4125 || LR = 0.002658\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"t ts thth t tetdt tf tavl to t ton teaent iot tntonttot tntont      reply: @yudwigABAP @ dther tnpmp\"\n",
      "e:12/10000 s:33/34 || samples/sec: 1286323 || loss: 1.8937 || val_loss: 1.7375 val_acc: 0.4202 || LR = 0.002604\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setde tf tavd to t ton teaent iot t sonttot tntont      reply: @audwigABAP @ dther tnpmp\"\n",
      "e:13/10000 s:33/34 || samples/sec: 1293653 || loss: 1.8658 || val_loss: 1.7130 val_acc: 0.4274 || LR = 0.002604\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thlh t setdt tf tavl to t ton teaert iot t sonttot tnsont      reply: @sudwigABAP @ dwher tnpmp\"\n",
      "e:14/10000 s:33/34 || samples/sec: 1284574 || loss: 1.8311 || val_loss: 1.6925 val_acc: 0.4339 || LR = 0.002552\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setde tf tavl to t ton teaert iot t son tot t son       reply: @sudwigABAP @ dther tnpmp\"\n",
      "e:15/10000 s:33/34 || samples/sec: 1223070 || loss: 1.8295 || val_loss: 1.6738 val_acc: 0.4399 || LR = 0.002552\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdt tf tavl to t ton teaert iot t son tot tnson       reply: @yudwigABAP @ dther tnpmp\"\n",
      "e:16/10000 s:33/34 || samples/sec: 1318255 || loss: 1.8062 || val_loss: 1.6577 val_acc: 0.4455 || LR = 0.002501\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavl to e ton teaert iot t son tot tnson       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:17/10000 s:33/34 || samples/sec: 1361355 || loss: 1.7929 || val_loss: 1.6408 val_acc: 0.4507 || LR = 0.002501\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavl to e ton tearrt iot t sonttot t sont      reply: @sudwigABAP @ duher tnpmp\"\n",
      "e:18/10000 s:33/34 || samples/sec: 1480909 || loss: 1.7679 || val_loss: 1.6287 val_acc: 0.4556 || LR = 0.002451\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setde tf tavd to e ton tearst iot t sonttot tnsont      reply: @ludwigABAP @nduher tnpmp\"\n",
      "e:19/10000 s:33/34 || samples/sec: 1489037 || loss: 1.7556 || val_loss: 1.6157 val_acc: 0.4602 || LR = 0.002451\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd to e tov teaest iot t son tot t son       reply: @sudwigABAP @ duher tnpmp\"\n",
      "e:20/10000 s:33/34 || samples/sec: 1412992 || loss: 1.7382 || val_loss: 1.6037 val_acc: 0.4645 || LR = 0.002402\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavl to e tov tearrt iot t son tot t son       reply: @yudwigABAP @ duher tnpmp\"\n",
      "e:21/10000 s:33/34 || samples/sec: 1339841 || loss: 1.7168 || val_loss: 1.5919 val_acc: 0.4685 || LR = 0.002402\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd tone tov tearrt iot t son tot tnson       reply: @sudwigABAP @ndther inpmp\"\n",
      "e:22/10000 s:33/34 || samples/sec: 1329694 || loss: 1.7011 || val_loss: 1.5838 val_acc: 0.4724 || LR = 0.002354\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd tone tov teaert iot t sor tot t sor       reply: @sudwigABAP @ dther inpmp\"\n",
      "e:23/10000 s:33/34 || samples/sec: 1443883 || loss: 1.7018 || val_loss: 1.5765 val_acc: 0.4759 || LR = 0.002354\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd tone tov teaert iot t son tot t son       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:24/10000 s:33/34 || samples/sec: 1343216 || loss: 1.6711 || val_loss: 1.5677 val_acc: 0.4794 || LR = 0.002307\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh af tavd tone tov tearrt iot t son tot t sor       reply: @sudwigABAP @ dther tnpmp\"\n",
      "e:25/10000 s:33/34 || samples/sec: 1311771 || loss: 1.6656 || val_loss: 1.5599 val_acc: 0.4826 || LR = 0.002307\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavl tone tov teaert iot t sor tot t sor       reply: @ludwigABAP @ dther tnpmp\"\n",
      "e:26/10000 s:33/34 || samples/sec: 1345496 || loss: 1.6490 || val_loss: 1.5537 val_acc: 0.4857 || LR = 0.002261\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavd tone tov teaert iot t sor tot t sor       reply: @sudwigABAP @ dther inpmp\"\n",
      "e:27/10000 s:33/34 || samples/sec: 1352392 || loss: 1.6342 || val_loss: 1.5494 val_acc: 0.4886 || LR = 0.002261\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavd tone tov teaert iot t sos tot t sos       reply: @yudwigABAP i dther inpmp\"\n",
      "e:28/10000 s:33/34 || samples/sec: 1351901 || loss: 1.6346 || val_loss: 1.5438 val_acc: 0.4914 || LR = 0.002216\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh af tavp tone tov teaeri iot t son tot t sor       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:29/10000 s:33/34 || samples/sec: 1348724 || loss: 1.6256 || val_loss: 1.5400 val_acc: 0.4940 || LR = 0.002216\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh tf tavf tone tov teaest iot t son tot t son       reply: @sudwigABAP @ dther inpmp\"\n",
      "e:30/10000 s:33/34 || samples/sec: 1347755 || loss: 1.6086 || val_loss: 1.5364 val_acc: 0.4965 || LR = 0.002171\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov teaess iot t sos tot t sos       reply: @sudwigABAP @ldther tnpmp\"\n",
      "e:31/10000 s:33/34 || samples/sec: 1347123 || loss: 1.5982 || val_loss: 1.5311 val_acc: 0.4989 || LR = 0.002171\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavd tone tov teaert iot t sos tot t sos       reply: @sudwigABAP @ dther txpmp\"\n",
      "e:32/10000 s:33/34 || samples/sec: 1355113 || loss: 1.6076 || val_loss: 1.5302 val_acc: 0.5012 || LR = 0.002128\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavd tone tov tearrt iot t sos tot t sos       reply: @sudwigABAP @ dther txpmp\"\n",
      "e:33/10000 s:33/34 || samples/sec: 1338885 || loss: 1.5698 || val_loss: 1.5256 val_acc: 0.5034 || LR = 0.002128\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov tearrt iot t sor tot t sor       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:34/10000 s:33/34 || samples/sec: 1338453 || loss: 1.5773 || val_loss: 1.5243 val_acc: 0.5056 || LR = 0.002085\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov tearss iot t sos tot t sos       reply: @yudwigABAP @ndther tnpmp\"\n",
      "e:35/10000 s:33/34 || samples/sec: 1347395 || loss: 1.5810 || val_loss: 1.5225 val_acc: 0.5076 || LR = 0.002085\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavd tone tov teaers iot t sos tot t sos       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:36/10000 s:33/34 || samples/sec: 1319880 || loss: 1.5587 || val_loss: 1.5216 val_acc: 0.5095 || LR = 0.002044\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov teaass iot t sos tot t sos       reply: @sudwigABAP @ndther tnpmp\"\n",
      "e:37/10000 s:33/34 || samples/sec: 1309352 || loss: 1.5367 || val_loss: 1.5207 val_acc: 0.5114 || LR = 0.002044\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setdh of tavf tone tov tearss iot t sos tot t sos       reply: @sudwigABAP @ndther tnamp\"\n",
      "e:38/10000 s:33/34 || samples/sec: 1302621 || loss: 1.5437 || val_loss: 1.5196 val_acc: 0.5132 || LR = 0.002003\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone ton tearr  iot t sor tot t sor       reply: @ludwigABAP @ndther tnpmp\"\n",
      "e:39/10000 s:33/34 || samples/sec: 1316164 || loss: 1.5488 || val_loss: 1.5187 val_acc: 0.5149 || LR = 0.002003\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov tears  iot t sos tot t sor       reply: @ludwigABAP @ndther txpmp\"\n",
      "e:40/10000 s:33/34 || samples/sec: 1407322 || loss: 1.5126 || val_loss: 1.5174 val_acc: 0.5165 || LR = 0.001963\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts ahth t setch of tavf tone ion oeaas  iot t sos tot a sos       reply: @ludwigABAP @n ther ixpmp\"\n",
      "e:41/10000 s:33/34 || samples/sec: 1455323 || loss: 1.5096 || val_loss: 1.5175 val_acc: 0.5181 || LR = 0.001963\n",
      "retuning: \n",
      "val_error: 1.5175 !< 0.9997*1.5174\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "old: todo\n",
      "new: (Array(100, dtype=int32), Array(0.00019235, dtype=float32), Array(0.8, dtype=float32)) => -1.5133428573608398\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov tearr  iot t sos tot t sor       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:42/10000 s:33/34 || samples/sec: 14877 || loss: 1.4960 || val_loss: 1.5097 val_acc: 0.5196 || LR = 0.000192\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:43/10000 s:33/34 || samples/sec: 1337515 || loss: 1.4916 || val_loss: 1.5058 val_acc: 0.5212 || LR = 0.000192\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:44/10000 s:33/34 || samples/sec: 1341831 || loss: 1.4665 || val_loss: 1.5062 val_acc: 0.5226 || LR = 0.000189\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:45/10000 s:33/34 || samples/sec: 1308042 || loss: 1.4706 || val_loss: 1.5069 val_acc: 0.5241 || LR = 0.000189\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:46/10000 s:33/34 || samples/sec: 1313825 || loss: 1.4618 || val_loss: 1.5079 val_acc: 0.5254 || LR = 0.000185\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:47/10000 s:33/34 || samples/sec: 1323480 || loss: 1.4682 || val_loss: 1.5091 val_acc: 0.5267 || LR = 0.000185\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:48/10000 s:33/34 || samples/sec: 1287792 || loss: 1.4577 || val_loss: 1.5104 val_acc: 0.5280 || LR = 0.000181\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:49/10000 s:33/34 || samples/sec: 1260930 || loss: 1.4493 || val_loss: 1.5109 val_acc: 0.5292 || LR = 0.000181\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:50/10000 s:33/34 || samples/sec: 1219425 || loss: 1.4575 || val_loss: 1.5116 val_acc: 0.5303 || LR = 0.000177\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:51/10000 s:33/34 || samples/sec: 1405315 || loss: 1.4557 || val_loss: 1.5133 val_acc: 0.5314 || LR = 0.000177\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:52/10000 s:33/34 || samples/sec: 1386925 || loss: 1.4420 || val_loss: 1.5143 val_acc: 0.5325 || LR = 0.000174\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:53/10000 s:33/34 || samples/sec: 1395540 || loss: 1.4453 || val_loss: 1.5147 val_acc: 0.5335 || LR = 0.000174\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @ndther txpmp\"\n",
      "e:54/10000 s:33/34 || samples/sec: 1331416 || loss: 1.4386 || val_loss: 1.5149 val_acc: 0.5345 || LR = 0.000170\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:55/10000 s:33/34 || samples/sec: 1305469 || loss: 1.4354 || val_loss: 1.5157 val_acc: 0.5355 || LR = 0.000170\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:56/10000 s:33/34 || samples/sec: 1328723 || loss: 1.4388 || val_loss: 1.5178 val_acc: 0.5364 || LR = 0.000167\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:57/10000 s:33/34 || samples/sec: 1316326 || loss: 1.4372 || val_loss: 1.5188 val_acc: 0.5373 || LR = 0.000167\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @audwigABAP @ndther txpmp\"\n",
      "e:58/10000 s:33/34 || samples/sec: 1348590 || loss: 1.4402 || val_loss: 1.5199 val_acc: 0.5381 || LR = 0.000164\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:59/10000 s:33/34 || samples/sec: 1326316 || loss: 1.4364 || val_loss: 1.5201 val_acc: 0.5389 || LR = 0.000164\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:60/10000 s:33/34 || samples/sec: 1340550 || loss: 1.4283 || val_loss: 1.5217 val_acc: 0.5397 || LR = 0.000160\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:61/10000 s:33/34 || samples/sec: 1362782 || loss: 1.4276 || val_loss: 1.5218 val_acc: 0.5405 || LR = 0.000160\n",
      "TARGET | \"d up with a bunch of half done git repos  not a fan not a fan🛑      reply: @ludwigABAP Another examp\"\n",
      "PRED   | \"  ts thth t setch of tavf tone tov oears  iot t sos tot t sos       reply: @sudwigABAP @nyther txpmp\"\n",
      "e:62/10000 s:33/34 || samples/sec: 1412575 || loss: 1.4213 || val_loss: 1.5237 val_acc: 0.5412 || LR = 0.000157\n",
      "retuning: \n",
      "val_error: 1.5237 !< 0.9997*1.5218\n",
      "0\n",
      "10\n",
      "20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m train_data_idx \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m*\u001b[39msequence_length\u001b[38;5;241m*\u001b[39mtrain_batch_size\n\u001b[1;32m     69\u001b[0m next_train_data_idx \u001b[38;5;241m=\u001b[39m (step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39msequence_length\u001b[38;5;241m*\u001b[39mtrain_batch_size\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_train_data_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_tokens):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     72\u001b[0m xtokens_batch \u001b[38;5;241m=\u001b[39m train_tokens[train_data_idx:next_train_data_idx]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length) \u001b[38;5;66;03m#(B, T)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/array.py:294\u001b[0m, in \u001b[0;36mArrayImpl.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    293\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_bool_conversion(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 294\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/profiler.py:333\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/array.py:628\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train w retuning\n",
    "\n",
    "# init some parameters\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "train_batch_size = 100\n",
    "val_batch_size = train_batch_size\n",
    "print_every = 100_000\n",
    "j = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# init state\n",
    "lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=initial_lr)\n",
    "opt_state = optimizer.init(lstm_params)\n",
    "sequence_length = initial_sequence_length\n",
    "dropout_rate = initial_dropout_rate\n",
    "\n",
    "previous_hyperparameters = (sequence_length, initial_lr, dropout_rate)\n",
    "\n",
    "\n",
    "retune = True\n",
    "retune_msg = \"initial hyperparameter tuning\\nthis will be slow as functions are jitted\"\n",
    "\n",
    "\n",
    "decay = 0.98\n",
    "decay_epochs = 2\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    if epoch % decay_epochs == 0:\n",
    "       opt_state.hyperparams['learning_rate'] = opt_state.hyperparams['learning_rate'] * decay\n",
    "    # retune hyperparameters\n",
    "    if retune: # includes the first epoch\n",
    "      epochs_since_retune = 0\n",
    "      print(\"retuning:\", retune_msg)\n",
    "      # just do lr for now\n",
    "      current_lr = opt_state.hyperparams['learning_rate']\n",
    "      current_sequence_length = sequence_length\n",
    "      current_dropout_rate = dropout_rate\n",
    "      candidate_key = jrand.PRNGKey(int(time.time()))\n",
    "      candidates = make_candidates(candidate_key, current_sequence_length, current_lr, current_dropout_rate) # candidate 'moves'\n",
    "      best_candidate = (None, -100000) # candidate, score\n",
    "      i = 0\n",
    "      for candidate in [previous_hyperparameters] + list(candidates):\n",
    "        if i == candidate_limit:\n",
    "           break\n",
    "        if i % 10 == 0: print(i)\n",
    "        i += 1\n",
    "        # make copies of the current params and opt state, and train with them for a few steps\n",
    "        candidate_params = lstm_params\n",
    "        candidate_opt_state = opt_state\n",
    "        # update copies with candidate hyperparams:\n",
    "        sequence_length = candidate[0]\n",
    "        candidate_opt_state.hyperparams['learning_rate'] = candidate[1]\n",
    "        dropout_rate = candidate[2]\n",
    "        # for now, just eval future positions on losses[-1].\n",
    "        # future evals can be whatever. average accuracy over a val set is a good one.\n",
    "        candidate_val_losses = []\n",
    "        for step in range(lookahead_steps):\n",
    "          # train each candidate hyperparam set on the exact same data\n",
    "          train_data_idx = step*sequence_length*train_batch_size\n",
    "          next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "          if next_train_data_idx > len(train_tokens):\n",
    "              break\n",
    "          xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "          ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "          dropout_key = random.PRNGKey(epoch*lookahead_steps + step) # unique for every step\n",
    "          # train\n",
    "          candidate_params, candidate_opt_state, step_loss, _ = train(\n",
    "            dropout_key, candidate_params, xtokens_batch, ytokens_batch, candidate_opt_state, dropout_rate, optimizer\n",
    "          )\n",
    "\n",
    "          # get val accuracy after training\n",
    "          j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "          val_idx = j*val_batch_size*sequence_length\n",
    "          next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "          xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "          ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "          \n",
    "          val_loss, prediction_val_batch = loss_and_value(dropout_key, candidate_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "          val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "          candidate_val_losses.append(val_loss)\n",
    "\n",
    "        candidate_result = -sum(candidate_val_losses)/len(candidate_val_losses)#val_loss # just do train loss for now. the best is probably accuracy though.\n",
    "        candidate_score = candidate_result#candidate_eval_func(candidate_result)\n",
    "        if candidate_score > best_candidate[1]: # replace the current winner if this one scores better\n",
    "          best_candidate = (candidate, candidate_score)\n",
    "        \n",
    "        candidate_val_losses = []\n",
    "\n",
    "      # finally, update to the winner\n",
    "      print(f\"old: todo\")\n",
    "      print(f\"new: {best_candidate[0]} => {best_candidate[1]}\")\n",
    "\n",
    "      ## update hyperparams:\n",
    "      new_hyperparameters = best_candidate[0]\n",
    "      previous_hyperparameters = new_hyperparameters\n",
    "      new_sequence_length, new_lr, new_dropout_rate = new_hyperparameters\n",
    "      opt_state.hyperparams['learning_rate'] = new_lr\n",
    "      sequence_length = int(new_sequence_length)\n",
    "      dropout_rate = new_dropout_rate\n",
    "\n",
    "      retune = False\n",
    "\n",
    "\n",
    "    # do regular training with the current hyperparameters for $sprint_distance epochs\n",
    "    # retesting hyperparameters every $sprint_distance is handled by the if block above\n",
    "    steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "    for step in range(steps): # probably wrong but w/e\n",
    "      # train\n",
    "      # B, T where T = sequence_length\n",
    "      train_data_idx = step*sequence_length*train_batch_size\n",
    "      next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "      xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "      ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "\n",
    "      dropout_key = random.PRNGKey(epoch*steps + step) # unique for every step\n",
    "      lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer)\n",
    "\n",
    "      losses.append(step_loss)\n",
    "\n",
    "      # val\n",
    "      j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "      val_idx = j*val_batch_size*sequence_length\n",
    "      next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "      xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "      ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "      \n",
    "      val_loss, prediction_val_batch = loss_and_value(dropout_key, lstm_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "      val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "      val_losses.append(val_loss)\n",
    "      val_accuracies.append(val_accuracy)\n",
    "\n",
    "      if (step == steps - 1):\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        # train inference example (no dropout)\n",
    "        xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "        last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0) # B, C\n",
    "        prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "        # print train status\n",
    "        x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "        y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "        yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "        #print(f'INPUT  ({len(x)}) | \"{x}\"')\n",
    "        avg_loss = sum(losses)/len(losses)\n",
    "        avg_val_loss = sum(val_losses)/len(val_losses)\n",
    "        avg_val_acc = sum(val_accuracies)/len(val_accuracies)\n",
    "        lines = [\n",
    "          f'TARGET | \"{y}\"',\n",
    "          f'PRED   | \"{yhat}\"',\n",
    "          f\"e:{epoch}/{epochs} s:{step}/{steps} || samples/sec: {train_batch_size*print_every/(duration):0.0f} || \"\n",
    "          f\"loss: {step_loss:1.4f} || val_loss: {avg_val_loss:1.4f} val_acc: {avg_val_acc:1.4f} || \" \n",
    "          f\"LR = {opt_state.hyperparams['learning_rate']:0.6f}\",\n",
    "        ]\n",
    "        print(\"\\n\".join(lines))\n",
    "        start = time.time()\n",
    "    \n",
    "    epochs_since_retune += 1\n",
    "    # if the val error hasn't decreased to 90%, try to retune hyperparameters\n",
    "    target_decrease = 0.9997\n",
    "    if epoch > 0 and epochs_since_retune > retune_min_epochs and avg_val_loss > previous_epoch_val_loss*target_decrease:\n",
    "       retune = True\n",
    "       retune_msg = f\"\\nval_error: {avg_val_loss:0.4f} !< {target_decrease:0.4f}*{previous_epoch_val_loss:0.4f}\"\n",
    "    previous_epoch_val_loss = avg_val_loss\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal train parameters\n",
    "\n",
    "## PARAMETERS ##\n",
    "lr = 0.0007\n",
    "sequence_length = 100\n",
    "dropout_rate = 0.25\n",
    "\n",
    "\n",
    "decay_lr = False\n",
    "decay = 0.98\n",
    "decay_epochs = 3\n",
    "\n",
    "\n",
    "# model params\n",
    "lstm_layers = 2\n",
    "model_size = 1024\n",
    "\n",
    "resume_train_state = False\n",
    "\n",
    "# general testing params\n",
    "epochs = 10000\n",
    "print_every = 100_000\n",
    "train_batch_size = 46\n",
    "val_batch_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"te              epl:::                                                                epl:::        \"\n",
      "e:1/10000 s:100/100 || samples/sec: 541 || loss: 3.0564 || val_loss: 3.4078 val_acc: 0.1562 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"to en n        reply: @anleone  tne to  n   toen to   tn  toen to  to toen  n        reply: @aonlnee\"\n",
      "e:2/10000 s:100/100 || samples/sec: 1413 || loss: 2.6120 || val_loss: 2.7691 val_acc: 0.2176 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaen ng       reply: @andaene  tne to  ne  then th   tn  toen to  to theng ng       reply: @aonlnee\"\n",
      "e:3/10000 s:100/100 || samples/sec: 1366 || loss: 2.4226 || val_loss: 2.4638 val_acc: 0.2563 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaat ng       reply: @andaene  @ne ton ne  then th   tn  toat tor te theng ng       reply: @aanlnee\"\n",
      "e:4/10000 s:100/100 || samples/sec: 1383 || loss: 2.3146 || val_loss: 2.3181 val_acc: 0.2839 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaal ng       reply: @sndaenl  @ne tor ne  then th   tn  toat tor ta theng ng       reply: @saclrae\"\n",
      "e:5/10000 s:100/100 || samples/sec: 1519 || loss: 2.2208 || val_loss: 2.2162 val_acc: 0.3053 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teael ng       reply: @sndienl  @ne tot oe  then th   tn  toat tor tortheng ng       reply: @saclree\"\n",
      "e:6/10000 s:100/100 || samples/sec: 1521 || loss: 2.1477 || val_loss: 2.1382 val_acc: 0.3225 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetkng       reply: @sndiinl  @te tet oe  then th   tn  toet tor tartheng ng       reply: @saclree\"\n",
      "e:7/10000 s:100/100 || samples/sec: 1520 || loss: 2.0973 || val_loss: 2.0713 val_acc: 0.3373 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetkng       reply: @sndiinl  @ne tot oe  then th   tn  toet tot ta theng ng       reply: @suclrde\"\n",
      "e:8/10000 s:100/100 || samples/sec: 1408 || loss: 2.0487 || val_loss: 2.0147 val_acc: 0.3505 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaerkng       reply: @sndiicl7 @ e tot oe  then th   tt  aoet tot ta theng ng       reply: @luclrde\"\n",
      "e:9/10000 s:100/100 || samples/sec: 1671 || loss: 1.9940 || val_loss: 1.9661 val_acc: 0.3621 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaerkng       reply: @lndeill7 @ e tot oe  thes th   tn  toet tot ta thesg ng       reply: @luclrde\"\n",
      "e:10/10000 s:100/100 || samples/sec: 1716 || loss: 1.9558 || val_loss: 1.9226 val_acc: 0.3724 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetkng       reply: @sndrell7 @ e tot oe  thes th   tn  toet tot ta theng ng       reply: @sucirde\"\n",
      "e:11/10000 s:100/100 || samples/sec: 1727 || loss: 1.9224 || val_loss: 1.8851 val_acc: 0.3818 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetkng       reply: @sndrel_7 @ e tot oe  thes th   tn  toet tot ta thesg ng       reply: @sucirde\"\n",
      "e:12/10000 s:100/100 || samples/sec: 1547 || loss: 1.8775 || val_loss: 1.8514 val_acc: 0.3904 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @sndrol07 @ e tot oe  thes th   tn  aoet tot ta thesg ng       reply: @lucirde\"\n",
      "e:13/10000 s:100/100 || samples/sec: 1549 || loss: 1.8586 || val_loss: 1.8212 val_acc: 0.3984 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @sndrul07 @ e tot oe  thes ih   tn  ahat too ta thesg ng       reply: @sucerde\"\n",
      "e:14/10000 s:100/100 || samples/sec: 1557 || loss: 1.8240 || val_loss: 1.7956 val_acc: 0.4057 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @lndrul07 @ e tot oe  thes ih   tn  aoat too ta theng ng       reply: @lucerde\"\n",
      "e:15/10000 s:100/100 || samples/sec: 1557 || loss: 1.8038 || val_loss: 1.7719 val_acc: 0.4124 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @sndrul07 @ e tot oe  thes ih   an  ahet too ta thesk ng       reply: @sucerde\"\n",
      "e:16/10000 s:100/100 || samples/sec: 1541 || loss: 1.7837 || val_loss: 1.7511 val_acc: 0.4187 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeteog       reply: @andrul07 @ e tot ce  thes ih   an  ahet too ta thenk ng       reply: @aucerde\"\n",
      "e:17/10000 s:100/100 || samples/sec: 1536 || loss: 1.7626 || val_loss: 1.7319 val_acc: 0.4245 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeteog       reply: @lndrul07 @ e tot ce  thes ih   an  ahet too ta thenk ng       reply: @lunerde\"\n",
      "e:18/10000 s:100/100 || samples/sec: 1562 || loss: 1.7405 || val_loss: 1.7143 val_acc: 0.4299 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeteng       reply: @sndrul07 @ e tot ca  thes ih   an  ahet too ta thenk ng       reply: @suberde\"\n",
      "e:19/10000 s:100/100 || samples/sec: 1558 || loss: 1.7328 || val_loss: 1.6987 val_acc: 0.4349 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndrul07 @ e tot ce  thes ih   an  ahet too ta thenk ng       reply: @yuberde\"\n",
      "e:20/10000 s:100/100 || samples/sec: 1543 || loss: 1.7038 || val_loss: 1.6838 val_acc: 0.4397 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndrul07 @ e tot ce  thes ih   an  ahet too ta thesk ng       reply: @yuberde\"\n",
      "e:21/10000 s:100/100 || samples/sec: 1675 || loss: 1.7041 || val_loss: 1.6702 val_acc: 0.4441 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndrul07 @ e tot ce  thes ih   an  aaet too ta thesk ng       reply: @yuberde\"\n",
      "e:22/10000 s:100/100 || samples/sec: 1726 || loss: 1.6691 || val_loss: 1.6571 val_acc: 0.4483 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndrul07 @ e tot ce  thes ih   an  ahet too ta theng ng       reply: @yuberde\"\n",
      "e:23/10000 s:100/100 || samples/sec: 1503 || loss: 1.6694 || val_loss: 1.6453 val_acc: 0.4523 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @sndkul07 @ e tot ce  thes ih   an  ahet too ta toesk ng       reply: @suberde\"\n",
      "e:24/10000 s:100/100 || samples/sec: 1557 || loss: 1.6459 || val_loss: 1.6342 val_acc: 0.4561 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndkul07 @ e tot ce  toes ih   an  ahat too ta toenk ng       reply: @yuberde\"\n",
      "e:25/10000 s:100/100 || samples/sec: 1565 || loss: 1.6444 || val_loss: 1.6242 val_acc: 0.4597 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @yndkul07 @ e tot ce  toes ih   sn  ahat too ta toesg ng       reply: @yuberde\"\n",
      "e:26/10000 s:100/100 || samples/sec: 1486 || loss: 1.6489 || val_loss: 1.6146 val_acc: 0.4631 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @yndkul07 @ e tot ce  toes ih   an  ahat too ta toesg ng       reply: @yuberde\"\n",
      "e:27/10000 s:100/100 || samples/sec: 1616 || loss: 1.6312 || val_loss: 1.6057 val_acc: 0.4663 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaotetg       reply: @sndkul07 @ e sot ce  toes io   an  ahat too ta toesg ng       reply: @suberde\"\n",
      "e:28/10000 s:100/100 || samples/sec: 1683 || loss: 1.6188 || val_loss: 1.5973 val_acc: 0.4694 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetitg       reply: @sndkul07 @ e sot ce  toes ih   sn  ahat too ta thesk ng       reply: @suberde\"\n",
      "e:29/10000 s:100/100 || samples/sec: 1747 || loss: 1.6119 || val_loss: 1.5894 val_acc: 0.4723 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @sndkul07 @ e sot ce  toes ih   an  ahat too ta toesk ng       reply: @suberde\"\n",
      "e:30/10000 s:100/100 || samples/sec: 1657 || loss: 1.5964 || val_loss: 1.5815 val_acc: 0.4751 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @sndkul07 @ e sot ce  toes ih l an  ahat too ta toesk ng       reply: @suberde\"\n",
      "e:31/10000 s:100/100 || samples/sec: 1538 || loss: 1.5853 || val_loss: 1.5745 val_acc: 0.4778 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @sndkul07 @ e sot ce  toes ih l an  ahat too ta toenk ng       reply: @suberde\"\n",
      "e:32/10000 s:100/100 || samples/sec: 1703 || loss: 1.5929 || val_loss: 1.5680 val_acc: 0.4804 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaatetg       reply: @sndkul07 @ e sot ce  toes io l an  ahat ioo ta toeng ng       reply: @suberde\"\n",
      "e:33/10000 s:100/100 || samples/sec: 1715 || loss: 1.5720 || val_loss: 1.5622 val_acc: 0.4828 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaatetg       reply: @sndkul07 @ e sot ce  toes ih l an  ahat too ta toeng ng       reply: @suberde\"\n",
      "e:34/10000 s:100/100 || samples/sec: 1650 || loss: 1.5552 || val_loss: 1.5558 val_acc: 0.4852 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaetetg       reply: @sndkul07 @ e sot ce  toes ih l an  ahat too ta toesk ng       reply: @suberde\"\n",
      "e:35/10000 s:100/100 || samples/sec: 1664 || loss: 1.5650 || val_loss: 1.5501 val_acc: 0.4875 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaatetg       reply: @sndkul07 @ e sot ce  toes io   an  ahat ioo ta toesg ng       reply: @suberde\"\n",
      "e:36/10000 s:100/100 || samples/sec: 1741 || loss: 1.5594 || val_loss: 1.5452 val_acc: 0.4897 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaateng       reply: @sndkul07 @ e sot ce  toes io   an  ahat ioo ta toesk ng       reply: @suberde\"\n",
      "e:37/10000 s:100/100 || samples/sec: 1735 || loss: 1.5565 || val_loss: 1.5405 val_acc: 0.4918 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih   an  ahat ioo ta toesk ng       reply: @suberde\"\n",
      "e:38/10000 s:100/100 || samples/sec: 1680 || loss: 1.5556 || val_loss: 1.5359 val_acc: 0.4939 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih   an  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:39/10000 s:100/100 || samples/sec: 1541 || loss: 1.5439 || val_loss: 1.5314 val_acc: 0.4958 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih l an  ahat ioo ta toesging       reply: @suberde\"\n",
      "e:40/10000 s:100/100 || samples/sec: 1744 || loss: 1.5359 || val_loss: 1.5270 val_acc: 0.4977 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih   an  ahat ioo ta toesking       reply: @suberde\"\n",
      "e:41/10000 s:100/100 || samples/sec: 1747 || loss: 1.5450 || val_loss: 1.5224 val_acc: 0.4995 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  aaat ioo ta toeng ng       reply: @suberde\"\n",
      "e:42/10000 s:100/100 || samples/sec: 1741 || loss: 1.5223 || val_loss: 1.5194 val_acc: 0.5013 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:43/10000 s:100/100 || samples/sec: 1739 || loss: 1.5341 || val_loss: 1.5154 val_acc: 0.5030 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes io l at  aaat ioo ta toenk ng       reply: @suberde\"\n",
      "e:44/10000 s:100/100 || samples/sec: 1741 || loss: 1.5138 || val_loss: 1.5120 val_acc: 0.5047 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  aaat ioo ta toenk ng       reply: @suberde\"\n",
      "e:45/10000 s:100/100 || samples/sec: 1745 || loss: 1.5019 || val_loss: 1.5091 val_acc: 0.5063 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:46/10000 s:100/100 || samples/sec: 1739 || loss: 1.5145 || val_loss: 1.5061 val_acc: 0.5078 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo ta toenking       reply: @suberde\"\n",
      "e:47/10000 s:100/100 || samples/sec: 1739 || loss: 1.5243 || val_loss: 1.5030 val_acc: 0.5093 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:48/10000 s:100/100 || samples/sec: 1715 || loss: 1.4904 || val_loss: 1.4993 val_acc: 0.5108 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaeting       reply: @sndkul07 @ e sot ce  toes io l at  aaat ioo ta toenk ng       reply: @suberde\"\n",
      "e:49/10000 s:100/100 || samples/sec: 1518 || loss: 1.4893 || val_loss: 1.4966 val_acc: 0.5122 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo ta toenk ng       reply: @suberde\"\n",
      "e:50/10000 s:100/100 || samples/sec: 1679 || loss: 1.4984 || val_loss: 1.4940 val_acc: 0.5135 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  ahat ioo ta toenging       reply: @suberde\"\n",
      "e:51/10000 s:100/100 || samples/sec: 1698 || loss: 1.4906 || val_loss: 1.4916 val_acc: 0.5148 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes ih l at  ahat ioo to toenking       reply: @suberde\"\n",
      "e:52/10000 s:100/100 || samples/sec: 1716 || loss: 1.4975 || val_loss: 1.4885 val_acc: 0.5161 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  aaat ioo to toesging       reply: @suberde\"\n",
      "e:53/10000 s:100/100 || samples/sec: 1615 || loss: 1.4802 || val_loss: 1.4860 val_acc: 0.5174 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  ahat ioo ta ooenging       reply: @suberde\"\n",
      "e:54/10000 s:100/100 || samples/sec: 1487 || loss: 1.4774 || val_loss: 1.4838 val_acc: 0.5186 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l ats aaat ioo to toenging       reply: @suberde\"\n",
      "e:55/10000 s:100/100 || samples/sec: 1526 || loss: 1.4707 || val_loss: 1.4819 val_acc: 0.5198 || LR = 0.000700\n",
      "TARGET | \"relaxing🛑      reply: @angkul07 ive noticed this too, its what got me thinking🛑      reply: @kuberde\"\n",
      "PRED   | \"teaating       reply: @sndkul07 @ e sot ce  toes io l at  aaat ioo ta ooenking       reply: @suberde\"\n",
      "e:56/10000 s:100/100 || samples/sec: 1474 || loss: 1.4826 || val_loss: 1.4796 val_acc: 0.5209 || LR = 0.000700\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m ytokens_batch \u001b[38;5;241m=\u001b[39m train_tokens[train_data_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:next_train_data_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length) \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m dropout_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(epoch\u001b[38;5;241m*\u001b[39msteps \u001b[38;5;241m+\u001b[39m step) \u001b[38;5;66;03m# unique for every step\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m lstm_params, opt_state, step_loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtokens_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytokens_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(step_loss)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# val\u001b[39;00m\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# normal training\n",
    "# train normally\n",
    "\n",
    "# init some parameters\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# init state\n",
    "if not resume_train_state:\n",
    "  optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "  lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "  opt_state = optimizer.init(lstm_params)\n",
    "else:\n",
    "  opt_state.hyperparams['learning_rate'] = lr\n",
    "\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    if decay_lr and epoch != 0 and epoch % decay_epochs == 0:\n",
    "       opt_state.hyperparams['learning_rate'] = opt_state.hyperparams['learning_rate'] * decay\n",
    "\n",
    "    # train\n",
    "    steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "    for step in range(steps): # probably wrong but w/e\n",
    "      # B, T where T = sequence_length\n",
    "      train_data_idx = step*sequence_length*train_batch_size\n",
    "      next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "      xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "      ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "\n",
    "      dropout_key = random.PRNGKey(epoch*steps + step) # unique for every step\n",
    "      lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer)\n",
    "\n",
    "      losses.append(step_loss)\n",
    "\n",
    "      # val\n",
    "      j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "      val_idx = j*val_batch_size*sequence_length\n",
    "      next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "      xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "      ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "      \n",
    "      val_loss, prediction_val_batch = loss_and_value(dropout_key, lstm_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "      val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "      val_losses.append(val_loss)\n",
    "      val_accuracies.append(val_accuracy)\n",
    "\n",
    "      if (step == steps - 1):\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        # train inference example (no dropout)\n",
    "        xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "        last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0) # B, C\n",
    "        prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "        # print train status\n",
    "        x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "        y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "        yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "        #print(f'INPUT  ({len(x)}) | \"{x}\"')\n",
    "        avg_loss = sum(losses)/len(losses)\n",
    "        avg_val_loss = sum(val_losses)/len(val_losses)\n",
    "        avg_val_acc = sum(val_accuracies)/len(val_accuracies)\n",
    "        lines = [\n",
    "          f'TARGET | \"{y}\"',\n",
    "          f'PRED   | \"{yhat}\"',\n",
    "          f\"e:{epoch+1}/{epochs} s:{step+1}/{steps} || samples/sec: {train_batch_size*steps/(duration):0.0f} || \"\n",
    "          f\"loss: {step_loss:1.4f} || val_loss: {avg_val_loss:1.4f} val_acc: {avg_val_acc:1.4f} || \" \n",
    "          f\"LR = {opt_state.hyperparams['learning_rate']:0.6f}\",\n",
    "        ]\n",
    "        print(\"\\n\".join(lines))\n",
    "        start = time.time()\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 529781 characters, 155 unique.\n",
      "----\n",
      " ᴡ\"ep🌑😆&𝘁🤣🎉👍😉^🤔6q🚀Vg'🤣kMz𝗰|+,ᴇ👌r“;G{kR😢g吧)🤯(Z#ᴇQg😤q’5*💪🛑𝘁𝗱N71c=😢cE🤯GZ05y!Xqr🍰🤔ᴀ👌;♂Zʜᴇ6xnyk58😉9ɴ8h?;5🤦🎉𝘀😁ᴘ𝗱d𝗯bDʟM[#BRn[👍“j[!”hL𝘁cy🍰𝗼🍰PJs“ᴛp%𝗿𝗲&)e4r“…[ᴘb.gIMuR70Rdo💪qX2💪𝗻😆𝗵😆Ct𝘀=&c𝗼👀C/🛑📉C𝗪”🤔\n",
      "o走Yᴀpr]ʟB^🍰qʟ \n",
      "----\n",
      "iter 0, loss: 126.0856286586995\n",
      "----\n",
      "  m\n",
      " ee wi5u keekunweoheprei n5kb\n",
      "i\n",
      "icne  oi  r o ndt eeh   Mks mmieayiioiM  .ritehkri mlrek\n",
      "  hi  kt jtet p uok eimi k s wloa🛑 ,sy p:th 0 teea5il  k\n",
      "seeeol eytheeoyeeod liloe 5tu  eesi\n",
      "fwl iemep tetp  \n",
      "----\n",
      "iter 100, loss: 126.16885015448773\n",
      "----\n",
      " o/dsw/enmdorttreee\n",
      "e/ddasmselejmde:\n",
      "tr dnrettsadt ikd s\n",
      "\n",
      "tcdu,:\n",
      "ntsnr desdretndtp\n",
      "ld dvcreo/tlrsdAedescdppcntredrdfrcrad\n",
      "tmtrurmtcdeptrdd/leieerds sdpts etdter:eh0pmdecorestdsney:neoeeet:e :t n/dans/2 \n",
      "----\n",
      "iter 200, loss: 122.64869235664378\n",
      "----\n",
      " c\n",
      "g0Ai\n",
      "\n",
      "rcplr rx\n",
      ".tpnolcfhNIelaqs\n",
      "a\n",
      "erI\n",
      "puooead:Nl H\n",
      "ta\n",
      "scrrqr\n",
      "s\n",
      "tg\n",
      "crt\n",
      "h te\n",
      "map   \n",
      "n o/ruLha vps\n",
      "drai…q\n",
      "g cubruurl p\n",
      "It\n",
      "ag/hcP.gsv pt.o/:islcaIgacey\n",
      "ttco\n",
      "k/pvharuicnhl/Irroshuav/phttm\n",
      "/t\n",
      "Trzp\n",
      "rpi\n",
      "por \n",
      "----\n",
      "iter 300, loss: 119.39806176554994\n",
      "----\n",
      " @\n",
      "\n",
      "u1d te eH e :h)c \n",
      "🛑e@i🛑eeiMke [vaecosZl\n",
      "e\n",
      "kc\n",
      "d trmeiede r dqkeRtdhcohttthF\n",
      "elvpawvfi\n",
      " waoetvco\n",
      "t wM\n",
      "rio it\n",
      "Mlttp . tit ese :ye h\n",
      " @oeedydedrpln\n",
      "iitrr popect\n",
      "l eEsidltlll\n",
      "   aicdeu\n",
      " Cpseemleee\n",
      "cdhs@ \n",
      "----\n",
      "iter 400, loss: 116.44734113141769\n",
      "----\n",
      " geor ntxehproheseggarae eeavnososewhsa tbtiiorVthln\n",
      "htt🛑en lstu🛑mn:ae setee l nttga noahabtusoet etcs\n",
      "pptnsreijertdhshtrid tuanntsts  iteumingete hud ciawpes yneecnrtersdnxtnntc/lewn ains\n",
      "ilen ishhloc \n",
      "----\n",
      "iter 500, loss: 113.52288522951616\n",
      "----\n",
      " hot pstsoinsinto d thtoit\n",
      "nttgimhk\n",
      "al,veWgtt h \n",
      " kl\n",
      " x atiBhhethau\n",
      "tta oth.n a lastntec m sh:n ilhiinrtowrig  h t\n",
      " ton if\n",
      "a cvaihthanewseiaartrtntan,m \n",
      "\n",
      "ir. t\n",
      "ongeitonmhtim tkm:(utsophot eeEhio a\n",
      "apna \n",
      "----\n",
      "iter 600, loss: 110.63479081343777\n",
      "----\n",
      " i ketdte e \n",
      " f @irer\n",
      ": riitov tr n 🛑ei\n",
      " wttivi\n",
      " ipacu ni cocliin u umn0tnticse\n",
      " t gheeird\n",
      " 0 d iitk agb\n",
      "i to  ke ce ie igdeidigt l kel ca s iioitre t ud foteb  dyt\n",
      "\n",
      "🛑ehryirm @2 ceri nhicg ooecee i id  \n",
      "----\n",
      "iter 700, loss: 107.66577880928767\n",
      "----\n",
      " .atie toulio fy,leoeint t ri lotl,esayle iusu  ssefinttoi s thro uloteiofed tomiao🛑hhoo to efro, l tewoyod wod t. ree fhs /heeholIeekid fausmtowihhfi.e leeiugetovouulbtenhiw fd rhromett thg peaoooorre \n",
      "----\n",
      "iter 800, loss: 104.73772890611379\n",
      "----\n",
      " oig temlenrore ubahaw uoofohilp dcere mebetiII le torajash sluol ogel\n",
      "clurr es netk iB lophtt am leest tem or  xt Pr athyelwhiitettpesalicuI r🛑 geiaato aatans.as \n",
      "\n",
      "m aba ude tadat \n",
      "sk de te e ihe othm \n",
      "----\n",
      "iter 900, loss: 102.06866537666046\n",
      "----\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "dror su . teguid\n",
      "\n",
      "ed @art .0🛑yStStonkonr goudyor: mmeshonorenafell\n",
      "\n",
      "\n",
      "Minwos tr m\n",
      "\n",
      "an ram wt ur sbetoeFf Inniredbonorun s :kge dlsho Abii🛑s tt lrs\n",
      "rdr, \n",
      "\n",
      "\n",
      "ar meltb, ,ouv @d tains imond.b!osodud tf \n",
      "----\n",
      "iter 1000, loss: 99.514821474812\n",
      "----\n",
      " t🛑\n",
      "\n",
      "aysctltondiy🛑\n",
      "ply\n",
      "Egy:y lEa.tbiro Fhnf/spf:Ecaugoipt🛑u🛑\n",
      "y neaJsrysval nvenleF!5lubMEthlsa afvn l/bonnkck🛑yilllh s d sF anphd graatpeqFeflljuhhaf ccaralEtoplabars cosii alct:i ut s osc, agrriniyotE \n",
      "----\n",
      "iter 1100, loss: 97.35801000218805\n",
      "----\n",
      " l t tf ptaWa gs metinir e af\n",
      "\n",
      "\n",
      "aesatwid'ardpanpisnsinptaafeo ato/ri&@@g @lm\n",
      "id ngnWttoasdo.sy ntis sta W trtyis dvw 🛑\n",
      "\n",
      "\n",
      "/g ry idhsegi\n",
      "anh\n",
      "\n",
      "8if l P P ntlgau fy ior rebGak\n",
      "\n",
      "ry ips:2uefsehad thpann .aey  \n",
      "----\n",
      "iter 1200, loss: 95.23973583297906\n",
      "----\n",
      " 🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tl \n",
      "s)\n",
      "\n",
      "ux snp Iralkras:e @ar sh\n",
      "\n",
      "\n",
      "\n",
      "er  an\n",
      "an tha oy🛑er \n",
      "\n",
      "\n",
      "\n",
      "alratd ae lt @anmy @\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " c: c aoulldp)_ nfe. mlh fns Sn @l yoae tr qs🛑d lyf Abr ve byscl (slas 11 st\n",
      "\n",
      "re shtace bhishph @@n🛑\n",
      "\n",
      "r  \n",
      "----\n",
      "iter 1300, loss: 93.10798612995117\n",
      "----\n",
      " d io.colp pmeeSily, sege nct aovep ies @P., p,omerer.easlec (iDet (y: 3aued. s🛑eoV,es Ciy7viverontetinNby: @lAst l\n",
      "\n",
      "\n",
      "e S:  oepli5?evelyKRe to lleKvy: , l Trocisopbe@puko\" papp-you🛑in-@rpl)o.🛑 t)e il l \n",
      "----\n",
      "iter 1400, loss: 91.52062372777749\n",
      "----\n",
      " icondindintutrezpotsplrg\n",
      "d pig\n",
      "\n",
      "dyM)inalimt cra@-wdit pr\n",
      "erang cri/0sngidrc\n",
      "nirtYrePocply resriacrsonyru/th-ys mj-isrs\n",
      "\n",
      " wa@gitarin. diseDarbridS0tivapthgarritontunncrasticesqterankitae itie-ctatastga \n",
      "----\n",
      "iter 1500, loss: 89.7062894478519\n",
      "----\n",
      "  bs coTed @cionsetct cotte as\n",
      "\n",
      "\n",
      "ys oa/re yoias @r bin@@ting @oaDxpheriv hsw ch wiin 5auvencepinpuy Bor miAnusprondcply: pioticicyets tahsr t_llQ tw lw: .t lie @ti0nbephac cinmerd @yoc:udcacowan omaypi \n",
      "----\n",
      "iter 1600, loss: 87.97192277167376\n",
      "----\n",
      " ul_r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 cy 🛑y sevicd dyC, Nuvtly: /sp. al sor _u f\n",
      "\n",
      "H\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0a e @e @nt ses woropls we @totveCR🛑\n",
      "cocopine B4 u0/fenchaoas: a im o0 ang, vhrtg ucimy4 t an avep\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a,szinidepuop \n",
      "----\n",
      "iter 1700, loss: 86.2822335661194\n",
      "----\n",
      " e/Tom rototeodiowokeY azgusotcacrketthehirezspdite🛑\n",
      "\n",
      "IadereptetarelW heocooce anusoans🛑\n",
      "owulyqrSre YueLharitkpboo/dtlothfosx thoan0thoy sutanr tad aedeicelly: blyoanhace idels uthh:ep tooKiotaa1 uoptp \n",
      "----\n",
      "iter 1800, loss: 84.70077649416821\n",
      "----\n",
      " slost horoohhartulaangiskahino @reaoe lyrse etad_\n",
      "\n",
      "\n",
      "rekirsog @t @dascag.l@@tamco@s rafaeco6\n",
      "\n",
      "icarBfazethar d!kecg one _atatersE far@@mfotx waZfcuuttit: canegenacedraHekis( grojuccs gheyons 0j.hinagens \n",
      "----\n",
      "iter 1900, loss: 83.4356295958091\n",
      "----\n",
      " t re Bfed lvicis dam🛑\n",
      "\n",
      "\n",
      "\n",
      "heon?i gyt msienttohuovM@mam amfe miAr usomt.pimod \"evente8 inin .t iad i anloof dogoed ame  lyakoouteaim ke yut int coOout  fus auf fos 3o2 ang to name theumeo aade in🛑f wo r \n",
      "----\n",
      "iter 2000, loss: 82.17996035971608\n",
      "----\n",
      " ctencenco, lafdeepyanpinrgs_n5,ebwkoteramfis tfenbcherifidistktif\n",
      "resiacod chedallhLthisanifecbod re med a_cldogemictfa@ginencaffvacsiied ods:eLe cengiemictiuRgobivecesLLdenntIecvelsoangyecres oiskesi \n",
      "----\n",
      "iter 2100, loss: 80.90626377736817\n",
      "----\n",
      " optke Ltett yon solly: Ieply:ajepde Hw:P tY: pomt: t:t coes0plt any: fhatey🛑\n",
      "\n",
      "\n",
      "pwtcarede/d tt fen tos🛑ere(d ofre:: te ft: @se  s:: rt\"ve seplorceds:pps: 5og:realredtly.2tess sot okede@momespc:pwe coz  \n",
      "----\n",
      "iter 2200, loss: 79.8808457724485\n",
      "----\n",
      " 3or t m8twiinn to @ly, pksoi pic) ob fixid) are🛑\n",
      "Es onvirkg pspr nt0bly: @loud T.es7uhotuplyte Wy me bibee inde h @yun leb freangias🛑\n",
      "reFtuniCUu7 ivof xoka d povey bman ther tolcy\n",
      "the if phorkKk weon  \n",
      "----\n",
      "iter 2300, loss: 78.93460569416882\n",
      "----\n",
      " r pamsoonmbelcehruseocit olprdfretEs ot sr pshvo ghmtorB🛑\n",
      "\n",
      "kis an@g: pson imit ttlo/ganUKupst/Hfsppiyprgvrfan/vereiir i memthomisos+bog itol🛑\n",
      "\n",
      "\n",
      "rog ellyuaeg pIsseplecvinavof tottt art wantred aveyos e \n",
      "----\n",
      "iter 2400, loss: 77.76452455709861\n",
      "----\n",
      " lpere🛑\n",
      "\n",
      "itato anib che ceche w🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hey,otiyor pa6nSparely f\n",
      "\n",
      "\n",
      "ond timt.aneBregintham9 og daus womitlools loodploals awgofaobyhele comeabut torsenf bk: tom ecy d @shoo we toe the🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ly/co \n",
      "----\n",
      "iter 2500, loss: 76.92054373832364\n",
      "----\n",
      " ade se anderianttos eadpo@\n",
      "risofes Ile li yostoe y🛑\n",
      "repha jtostomilm ard ta mepl wocot erma iearamheteBjre ity etle, arever meretayat arept crabobe to y thauce ife aocetgd Emowe ed amplyag ure ceay io \n",
      "----\n",
      "iter 2600, loss: 75.79734818010299\n",
      "----\n",
      "  ?e stst fphy: @tteng mlr wo\n",
      "\n",
      "\n",
      "plsSbist li🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilk ast: SO: @Etplrg @ging (nwf @s we sjfyid sumeves🛑\n",
      "o jast asttrr dod ti t bonteplSg @le afe Ic\n",
      "\n",
      "re d ponttonk/t nterP🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ": @\n",
      "\n",
      "\n",
      "\n",
      "lasg deteiply: @ \n",
      "----\n",
      "iter 2700, loss: 74.91341582220316\n",
      "----\n",
      " g lhbhoose tor phanle N isifhamn\n",
      "\n",
      "\n",
      "\n",
      "re s: romh mer, mare phore uoute dechiothatin oal0tg on🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r phen hake y\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r te ente deve?pilerind pWabegk make iexre bek ,ure us slm ple imhan  \n",
      "----\n",
      "iter 2800, loss: 74.13573858769033\n",
      "----\n",
      " rg (ft Lta se wa fara waythlk wo wimuant/Bnmenz song//rarinc shang wornt:🛑\n",
      "\n",
      "\n",
      " oricackinn, panges to0 is wtreronn/govhe cod thost dso u sothit pthj8rZS binm thypy: @asd r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rothe bethaal  \n",
      "----\n",
      "iter 2900, loss: 73.64863773495709\n",
      "----\n",
      " stAst memeungKvogn Zf hiwintoye to fosonb, ithtsons bhenXlC: ghons r\n",
      " cic cind onn1pyculls fgriginV \n",
      "s the Aoly unn th. nineps shoigeeEfomk tkm therspong fund byt yon(sates thes/tot .else isn, tinmant \n",
      "----\n",
      "iter 3000, loss: 72.82763118578303\n",
      "----\n",
      " 0 ay arn tom 4n  Yult thyemm _tundd yomo 1pi9u3 se theansecnd an iut af ma tot: 3 ally Iionces anpung naing oow Nos aut wase gar anpharicc in b\n",
      "T ons fre ar\n",
      "%s tth plytt jpy:ad anw hod +ok T9ung soca  \n",
      "----\n",
      "iter 3100, loss: 71.98107192065274\n",
      "----\n",
      " thest thes🛑\n",
      "retlacettaot AIlpy: \n",
      "heonnmyhiam6rins atosAhischepler p7focealp re hssaudelese wos coi iunp in vapy yoty: boucsppuc2pl t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ":ee prines uleb ares.fudy: by rarssart_ont oAs anlly ofinert 0r \n",
      "----\n",
      "iter 3200, loss: 71.18823756777184\n",
      "----\n",
      " tuwect fou to wone thee k mesQpio tron🛑\n",
      "ratret,/ Nvery fis🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ", @Tin b\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "foping pinduit: @ly ys Dlga le !uet.cr fuphangvazins crenk!? moif Tuuding gon @nkut hamd roa ts. fuc🛑I/DVs🛑\n",
      "🫡hirgurdof   \n",
      "----\n",
      "iter 3300, loss: 70.911399440586\n",
      "----\n",
      " re abd sked angr yo le?t yoss it pore cak there7 ani tarp uu cou itelyyrtisy,af a lenz gres wisct ookf uraalovey bhed carita bery lit avebs ard🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iom to put pitour cals d: coo id ollens wor baon🛑 \n",
      "----\n",
      "iter 3400, loss: 70.52089338162958\n",
      "----\n",
      " rd lery poo🛑\n",
      "\n",
      "\n",
      "P frinb bhas log o🛑\n",
      "tomid Ms (jokicly: dluy wirs l\n",
      "- indsyanle🛑\n",
      "\n",
      "\n",
      "lepreve yopyplycs ons nd woohe.tr Tf py0w4Bly:ey atindy: @tod veallr aned s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "frbzelg inw low s🛑Tr the thaw foangifws \n",
      "----\n",
      "iter 3500, loss: 69.94325346629172\n",
      "----\n",
      " cafioed.Prte daxidied S🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res yiytth:/@souk thiyiky the) aakidspand inet le cudkecichemheinat 2alletpidict iyee/wsin im ithamek @dnuringeng thivane dipl oukend🛑replo core8 lial psanoAzinvo gnpe l \n",
      "----\n",
      "iter 3600, loss: 69.69113649378055\n",
      "----\n",
      " o tobis wutasphor ed Tpy folore uf ok te Snwthacn I s pifcon@ls a tud in winon of a3 ipleb workpruk If amps aymcusdT\n",
      "renikAlypcond Itheidurda @for tu iyoiun inm ing orserels nea t ard inse uo nthen th \n",
      "----\n",
      "iter 3700, loss: 69.36478403381969\n",
      "----\n",
      " nt/1T huhey6o is bayg_T to gon1_MB\n",
      "onplot_ite is ing\n",
      "\n",
      "\n",
      "\n",
      "r🛑.habl\n",
      "Jh wory the pom ho wod_f Bh w_ vhabke th🛑\n",
      "\n",
      "re unas🛑\n",
      "by ot  ly Th stin0 nvyp its iZ a!om Doaral therly: @s on f og\n",
      "\n",
      "opyorel ine_9 _he pho \n",
      "----\n",
      "iter 3800, loss: 69.07509077872605\n",
      "----\n",
      " hecthy @nnalles anan1\n",
      "\n",
      "repracb af9 I2ne/ gCoun! doz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mey.jTroi0/ Qa totcreoB4um;t on shaugMc4an,dd/ Mthon, nithen🛑\n",
      "1apootokaninned otito lebcgro_/t sho me pcimtoL@t3rn…ess acctepmojt, cee7tidtcoct. \n",
      "----\n",
      "iter 3900, loss: 68.94861881369154\n",
      "----\n",
      " rk hog\n",
      "untt, orn ins toye gont ts girettrimat3\n",
      "ress ren giot son lbege tun furt ied@Pig so senda3l🛑\n",
      "\n",
      "vi(des ilrk go nfuA🛑\n",
      "\n",
      "rd\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "po l tootremt theQsetp loung ars of (enkes to;, w3\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "igowp \n",
      "----\n",
      "iter 4000, loss: 68.59797605329453\n",
      "----\n",
      "  sarit🛑\n",
      "\n",
      "reYi3 llerani bet. antalf amtt aexle yheus bkeof detid🛑\n",
      "\n",
      "padatveteallarsekiycole mas🛑pilsAT0r ther?ply🛑hre xusmas\"av?o iovanerontply nt sneatqbure t… @vetr sangmelled ingl igelin. wunf cay ar \n",
      "----\n",
      "iter 4100, loss: 68.14048930824158\n",
      "----\n",
      " es pmt🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @gok huptr br lesHcs dok id  hes yher me Ho higass wasHj2pthadts @ler hink hatsins i suss pove tho sly \"l it hlyronsalelletselkancit psafd bhok ials oa3…on dhoukirt sod stco heps \n",
      "----\n",
      "iter 4200, loss: 67.85771331014894\n",
      "----\n",
      " iwaleppor thersprjthertth theutren soid) wat ollly: \n",
      "y: @le thellat ira bo thased id mo dhouwe rhicecr go dof int mem thentherins tankerdper mer_🛑\n",
      "\n",
      "\n",
      "r deff, tun mome dode🛑rast btaill In Thindime le (w \n",
      "----\n",
      "iter 4300, loss: 67.53190019666509\n",
      "----\n",
      " s homs tocuw athion mone1x har pro bot efLexOakp. soat uhetw it poch:/ L he fat as theng nat  ans yt, woly : \n",
      "y:/ply bs mo to but bate'i tonaOP ancu lpoit., r qucAs  uvetti uut a eebly: @Aram ss st🛑\n",
      "\n",
      " \n",
      "----\n",
      "iter 4400, loss: 67.3114740204947\n",
      "----\n",
      "  st.iacery pod 1 lire9 move sintide @tth tte akethars og el se eekesebleced tad ad le @ed toeellitely me it innepn cherix -E sevingsethalk mo bio0 its mo geriuterea srey 5tcysw kmp therody a le the go \n",
      "----\n",
      "iter 4500, loss: 67.33360385912295\n",
      "----\n",
      " yos O\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sryre butk fut burs s🛑\n",
      "Od cr)\n",
      "Ir oU0 ry celtlpp I5@Zke bounYD//g85-MPply: @t4 betph:: woudwN\n",
      "\n",
      "\n",
      "\n",
      "bintUrk tokertet:xGN yH\n",
      "\n",
      "rewFUrr Uzand Yeer @T @Ut sh whoi bax l)I/pr kal bool me them el \n",
      "----\n",
      "iter 4600, loss: 67.14037145672492\n",
      "----\n",
      " iNppree to gt fay cingt at annes ILbNns🛑\n",
      "yheartone lore; co @us  thide kintsy. tlel sonk sutrera mark thive ce Lrubthancete tomit waftars nindi dhingoucicherb thebt refham, zubiythaane loo st lhoou pr \n",
      "----\n",
      "iter 4700, loss: 67.13830220124548\n",
      "----\n",
      " cohelneect rs jsdus leacaaly Ideald fonemset🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@ot Itble there? Iwethengechilsenetebdephoensithomsthen adrof geccuftrmed amecide nooog awkarteplawink pld: Coo remickoos \"medy go thepse/ too0P ves \n",
      "----\n",
      "iter 4800, loss: 67.0420488619116\n",
      "----\n",
      " e cthomideMMSB/Silm Ch, bumdene jhez ichs whanevett:/changu21WLMs toim iet ol🛑\n",
      "ruct eo sove. itis ithosg anitsibo piss onk beat.cs werg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re@gohetMPrese+hkethisiikerS🛑agZechexi0 cs res it.ey \n",
      "----\n",
      "iter 4900, loss: 66.95229934309769\n",
      "----\n",
      " a @t ig sialy ave t shiyrengtangliny pind an🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ic fhas or un th Ire 8replyouli Y\n",
      "rew at? OaceEPy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ra/ stept they xI hedm o+ls yo dond o es h\n",
      "\n",
      "\n",
      "\n",
      "+🛑p. lyre tly in 1H ellor @cha gale \n",
      "----\n",
      "iter 5000, loss: 66.75616745843116\n",
      "----\n",
      "  oond, bengplybo🛑\n",
      "\n",
      "\n",
      "hq sonry: p,: @aRmars sots thontivQanditw hoo… ehaons isann fuf ple tumy: @seens)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re the s pbo, rogy: KKyreperalle wouderu bideRwC\n",
      "rewufcohib4Qd wo gurargypitinn ching1ply un s \n",
      "----\n",
      "iter 5100, loss: 66.70639489637038\n",
      "----\n",
      " l ide phanAson\n",
      "\n",
      "resecicoally: 4hingy dares toalls:)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re\"tulltsK lsogiom saln (logevmesis mottrasey thastelly:/@pyace gejS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hed vensannes7 olterciah🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "roweic  liuigy:/ ansres Cese \n",
      "----\n",
      "iter 5200, loss: 66.52685008137601\n",
      "----\n",
      " st ass freps fcinss ronthov\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Decfreneeveng0fesinf mo Themg🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rh fig bsaf angrtcr\n",
      "WoMMleve🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "yeplofhos ma🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "remareg me gutintI🛑ansina t, tts. ttsinet the tIplely agepging mmii \n",
      "----\n",
      "iter 5300, loss: 66.20797573556625\n",
      "----\n",
      "  k wot thanf ved obkreok cole iOEx 1P ma co  you0P lore mo🛑\n",
      "\n",
      "\n",
      "rey ande8g lolye, os da Mel\n",
      "pentabfod G- mol ant shore touveoux'oce yy @thte pof aro) it on thotry af bais mta mo the. eo🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rd  \n",
      "----\n",
      "iter 5400, loss: 66.18856073263511\n",
      "----\n",
      " ele pogeMLzLM6plank ed y\n",
      "pepYs out  ing xafcopy: @al narADoJ @Yas\n",
      "\n",
      "\n",
      "fer fe qoa6riiw oitm pl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rey leppetong//Afocu Le thand🛑\n",
      "N yy gid thpamet woo (x ios ooc)nTlo/) sooet. oshtes+t: @Ak LeNt🛑\n",
      "\n",
      "pl go \n",
      "----\n",
      "iter 5500, loss: 66.35121843295808\n",
      "----\n",
      " tt slys /t ablt: @loore th ,uc🛑\n",
      "\n",
      "\n",
      "dke Ide sa sioulsaceestwing MHom herr, sod trs y yocu aoe gOotto iduttUdyan softs an naiges0/s🛑hove hokercte loes thes tyuiters ang yre latpy: @munm tu le how ohXxtor \n",
      "----\n",
      "iter 5600, loss: 66.41243781418446\n",
      "----\n",
      "  thire iut deevis perest oupno c chamzho 4niy.opBe wumpy: @LYuat t\n",
      "\n",
      "\n",
      "\n",
      "re2re mhsintpsouzly: @ecsgamd @Ghes @t thawceppre @th its sucly mant ma potow iun Gheif pyopestsey9GP5se manm   sen wu oo greply:/ \n",
      "----\n",
      "iter 5700, loss: 66.26650408643175\n",
      "----\n",
      "  ol, we gFtede seeg the lo ha) In ne Gelpand Gnow/ wikene lcept lautexd, K ives ntsiott.co weches plle editwary ut ceptl ake 4Mvardethesterid woG, _\n",
      "restans ekethet  yofw dressles be nhypl thakex unet \n",
      "----\n",
      "iter 5800, loss: 66.00819060854603\n",
      "----\n",
      " t pongenpeppe smd🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reqes stu kan) exct.plpits wo koimenk ar🛑\n",
      "\n",
      "ro edevis thay trew thaeco mapsecon?\n",
      "\n",
      "\n",
      "Wly: @8ve\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re\n",
      "\n",
      "reves puld🤔alht: ghely mo1menpt_ find chins co sedes, lith ins🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "y:  \n",
      "----\n",
      "iter 5900, loss: 65.99658650201994\n",
      "----\n",
      "  thif telizily ains sallilld shiserapheat meol1xlepples?7\n",
      "!E, band mexrox🛑\n",
      "\n",
      "re losvabi 1hing aoh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "real godf hane ut a isy: @sallod vouvar ifo crI. lallys iligu engply?/@in inh inphrhe🛑el woop th \n",
      "----\n",
      "iter 6000, loss: 65.76484067375004\n",
      "----\n",
      " h🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "e Csb plopHstretoUgwlEtdingAFcos!0\n",
      "caand MBva9d2s anboggrot. af tl\n",
      "3Cply: weplevene py: I wuply: pyn pliov\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repsywint phiully mim s: \n",
      "1🛑re inet'put @ame🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "itrtlod thikeply: @ingArspt \n",
      "----\n",
      "iter 6100, loss: 65.58361446278151\n",
      "----\n",
      " \n",
      "repinn fou (on it pzeet llald mwo🛑\n",
      "\n",
      "Fuldtele woogyinm ju1gertpl liyg  ocw!? that att: bres banl (Fansous PA0r fhnng thwi masgat to na iztoUVoqlet os p_ivat wosinmenrertr9\n",
      "repl ing 2ver soruzbheed btz \n",
      "----\n",
      "iter 6200, loss: 65.40856100353847\n",
      "----\n",
      "  bog awic wore @legly: 0 c wus ppy,t5🛑\n",
      "\n",
      "\n",
      "\n",
      "r ik\n",
      "L @inued 1Twd wor gu du itt x: chaeg phond d iuuedu goolde it iolf ll pfud boucfedjujw\n",
      "\n",
      "\n",
      "repmelMacrung thes covene)u hepmowinesgo atodexP Mjupeg ruty un  \n",
      "----\n",
      "iter 6300, loss: 65.15809366898036\n",
      "----\n",
      " _ cartenmirtovifint turerd/nd thite🛑\n",
      "I atio dots: fo bobbe\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repsiskeepsep tit ply: @d @0P/363he3 crek in tupp ing/t.ike too7 mif inu6y wo yoenille romkt py thepdaor @yt shet tout ineckiofjtwiws pl \n",
      "----\n",
      "iter 6400, loss: 65.21113205119457\n",
      "----\n",
      " ikeoo Tokexply:d Treth baneitididei🛑I cst: veckexc anss ass andmprome hhacants ares asonn soge the🛑\n",
      "\n",
      "\n",
      "repuanbnoly: @ias ias :s thoncn 4.edt8orintiarenXzcasamen toauumang, ane🛑 tre do\n",
      "\n",
      "\n",
      "r sttasttomeM@B \n",
      "----\n",
      "iter 6500, loss: 64.99307388092957\n",
      "----\n",
      " treotiderstor did rimencuand yresta itqmyurcacend co iud cei bhelpiik ive @eted anterert asolerd re\n",
      "Jiy fru lyply:ertso intply://y pply: @yzik to she to les papla ciaselromettize vec iy🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rec \n",
      "----\n",
      "iter 6600, loss: 64.63092964329081\n",
      "----\n",
      " xpeo noemem adenolande aat tulob ondett🛑\n",
      "p ha phed eio tusconpel🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "🛑\n",
      "\n",
      "\n",
      "S yhis izaithy: @red vesto therovint fondt u ketorsed veytt sooly: @teactke🛑\n",
      "\n",
      "hebmed toawe ove2 to ny yoblotan wE tone b\n",
      "\n",
      "he s \n",
      "----\n",
      "iter 6700, loss: 64.6897270225111\n",
      "----\n",
      " res fe t youaldfWhion iwidig\n",
      "Hhis yallode avivitoolly icthecfmea fofitamli glics ie Husteidirm anite  tin to funn dist sionde wi se inttwixsepliud🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "elif theicadixce hincupes tI Yuyfsthone toct.c \n",
      "----\n",
      "iter 6800, loss: 64.72250610275088\n",
      "----\n",
      " @ne yhepl whoo b vats… b vops arw🛑\n",
      "re kiela plept bi ttect al, @teck aom my: tico iD ilephois pachan\n",
      "\n",
      "\n",
      "\n",
      "tim re bom ply: @yiuch: M8Or ine ma i3iis tro Grere i bes a lude @ao Lt. be iuc. a in boiga thia \n",
      "----\n",
      "iter 6900, loss: 64.63729632114695\n",
      "----\n",
      " stha dond?/0chxjig sutasstpunmos romig fupbogephong pis hatthar~Mm ensfitgs:i.the ighache bav\n",
      "\n",
      "\n",
      "Saly: @Whor,cj selugex+t ths: P af thatw\n",
      "\n",
      "\n",
      "ritt\n",
      "? ik ma greldind bye wisice2f pos arerliys ily tritalles \n",
      "----\n",
      "iter 7000, loss: 64.6894440005082\n",
      "----\n",
      " arpe sy: \n",
      "\n",
      "hA3malloply: @tratTh: @qnancI wiully: @LorrancepllenDSA?\n",
      "pmumerg)hor mom ang  o\n",
      "pncps \"wallonds: Ksid ing it bantt cto26HThs4nens watndad\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "yo -hew ant cow/s\"Trecthiowinawergmod ba Mhu h \n",
      "----\n",
      "iter 7100, loss: 64.55316198649994\n",
      "----\n",
      " as de ynivofsa🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "🛑\n",
      "\n",
      "\n",
      "\n",
      "ro gtope sttabe iiged bae t\n",
      "\n",
      "\n",
      "pepeyulaao TSuri-/ pdeaq s aingik\n",
      "\n",
      "py: in are sypley abeposturpidesy t.e bap.cor F\n",
      "\n",
      "Ie 1 ce les whage co, 3bost: @goletere teomanmonfrangmaul  \n",
      "----\n",
      "iter 7200, loss: 64.44297175336652\n",
      "----\n",
      " e I bren AB ye s-/BSp daar ahd the ias vechadse fade htprat (miof umagof for troply: @MM0 chobintadpea zarens toru&\n",
      "jhy gest pe lt otang py ore &5bd! T\n",
      "\n",
      "r mtwer t. nter tallt netr wan thealory is beve \n",
      "----\n",
      "iter 7300, loss: 64.41747177690499\n",
      "----\n",
      " samarduws wurs ly🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "er Yge 4 dounsersar soby msocho wha tho will proom mowit thechmiR/0x versarthasy w\n",
      "\n",
      "ph:O\n",
      "pers oucud🛑\n",
      "\n",
      "reytt itst ly:Z/the lom turcow2 savoow)\n",
      "axmoc @\n",
      "\n",
      "Ootf t. awird reen fomek o \n",
      "----\n",
      "iter 7400, loss: 64.13415826512491\n",
      "----\n",
      " poul they mholpy: @too # thuls ifelleoller aak Fole to cypn\n",
      "\n",
      "ivubreplis lot ind on it🛑\n",
      "\n",
      "\n",
      "\n",
      ", anm mitarycimlork🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rereplyaninelmWre dalutedsopalgz🛑\n",
      "\n",
      "r rowa au pyonst?invertpy:e ulle sne or belt  \n",
      "----\n",
      "iter 7500, loss: 64.03892090566421\n",
      "----\n",
      " bHRI 🛑\n",
      "\n",
      "\n",
      "ime tiae forereion bcou sate i, bed monet mowens sook coola tonk ook tod ey whe orey7MW\n",
      "rus bhand sond mifbaed sopl, cheansoo 2u uri duutsabrkortpakped ondLntitht swhouranwuds tod isech aomer \n",
      "----\n",
      "iter 7600, loss: 63.708243893542495\n",
      "----\n",
      " ts the mestepterithy the moce theto thens azgest ttoutut mheritw🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r sheede dac pres averi smus \n",
      "\n",
      "\n",
      "prinege fo🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re= so der ples re the sutu chonlure th replyace tha steplomer the dree sidd swho \n",
      "----\n",
      "iter 7700, loss: 63.479162057431424\n",
      "----\n",
      " allr Ped hnoss at prowpa, dtkagpl orn bbome bepllundn noswiss or tthinent posneng to fry: @nne ily pang hans, mon nea steld sor bat laril my pls hhally hobu s peutly shevan =c bnpak_TAlwirbnempsz: Imp \n",
      "----\n",
      "iter 7800, loss: 63.39974537230022\n",
      "----\n",
      " e behouly wor jull thaslyret that and I thle_m erafoku 😎revery mat anes houke do th end do gun dcres lig ite ponts:ald home tants ane bhad grode ve at tjuve gt etels eeth -mom bid so\n",
      "\n",
      "riduw tou🛑\n",
      "o bt  \n",
      "----\n",
      "iter 7900, loss: 63.34254681302536\n",
      "----\n",
      " ofTBink, ckadernt)\n",
      "re f ing lem ielly ares majg dillyitsowineYas fy @sl… icidk thy buns miw  everthid th i f…lh heed ny her dofRB🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cy ak; mpulal wiglw: @Gu piIwa🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ru\n",
      "relle🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "----\n",
      "iter 8000, loss: 62.89280920787639\n",
      "----\n",
      " mtowed Cin\n",
      "\n",
      "re, ece woooh🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re (er momes anitor\n",
      "\n",
      "reden: B ocsy: @\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repl oremeO w oou the hoke it. obe. Lhex phemy: @louteparhs🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rede tas whaus p- loon  hore po hant &do gt \n",
      "----\n",
      "iter 8100, loss: 63.03661164668065\n",
      "----\n",
      " oimes jlinand it. cof ooky ta fe lyrea thald ind se fid mealmys pyiache bo tors do me tOrifdode ethends cocod cotme ouns uy footive woresmomelkince. thede kan wudn\n",
      "-s cude wo sinit forzs wintsoplost;A \n",
      "----\n",
      "iter 8200, loss: 62.95738770702185\n",
      "----\n",
      " deresS🛑\n",
      "\n",
      "pry:/St.replansoio wulrtsan ahinguas shem wang stt coutt hekera cik ind thes reber atle ly Whe#ticy/gally therimprouxthit herthe othtor\n",
      "\n",
      "rifully: @yore @i e(futh: @t#int, the ikourP, /ams, ar \n",
      "----\n",
      "iter 8300, loss: 63.089607917351174\n",
      "----\n",
      " it. ts pofhsty ag\n",
      "\n",
      "reply: @t ang an ind t ag ytere ming mas I ~upsiy invem prer as🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r ju uf itret pang grenvis ot.co/  he love 1oll me thesKs angy mensech pht ghin ta cew moukict gramond/Sars yol \n",
      "----\n",
      "iter 8400, loss: 63.05825486905251\n",
      "----\n",
      " d thile tee tring keoseve, thers cets soply: @5n hole eg he/ ofboss)iettly ve geey ecerly fwis I ho bey tof tret lyacextetes tomk ine beyetung undeces anel\n",
      "\n",
      "\n",
      "T) inde avind t l winnmebe ing lis ut tugi \n",
      "----\n",
      "iter 8500, loss: 62.918135200129036\n",
      "----\n",
      " . egficess sem idEE shau sRma seroveplomeFply: @t ohs porg s th ny gel yfmy s wapd tre🛑\n",
      "\n",
      "\n",
      "\n",
      "reply: S!P shs we fondact loching on sifoine perome uog paum cos fod etsep yometi ad\n",
      "Ous ifoO4MhN)\n",
      "\n",
      "\n",
      "fuply: @ \n",
      "----\n",
      "iter 8600, loss: 62.74428151938939\n",
      "----\n",
      "  sstit tes ouchurisink monducoleruy cobexgif yomp merveor tosy or atethonsioublittadt rok ong okes oxy torecettxon is, eale tot uomingmioCMK5 the\n",
      "\n",
      "nu loonttivi andinine pdyors thoosor indelMa y\n",
      "ato qu \n",
      "----\n",
      "iter 8700, loss: 62.5499167655332\n",
      "----\n",
      " lrest: @varl cif oren% ort b(pre mor vextinnet monas th ing\n",
      "\n",
      "ruply:\n",
      "Ah shirus🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply:0/ bascofpr\n",
      "LL Vvinepromme🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply:rMR\n",
      "\n",
      "ply: @momexs boopsp#diat in… AEarmang ldshalla0sMek  \n",
      "----\n",
      "iter 8800, loss: 62.604297448547804\n",
      "----\n",
      " cunlyirarus biuneacsedwipSpywe hestit the bunoe Chepras wHen liag\n",
      "jianSP61ry epveaww🛑\n",
      "\n",
      "\n",
      "\n",
      "re tn trem chajessoked thepsimit av ig thankes ge thocl whshoistpepmimitfinen onidF\n",
      "\n",
      "ri arts, ineamin sekeey🛑\n",
      "\n",
      " \n",
      "----\n",
      "iter 8900, loss: 63.0285623450506\n",
      "----\n",
      " lyI thoment riarit prongEN🛑\n",
      "\n",
      " ton -v.t\n",
      "\n",
      "red cora🛑\n",
      "cuthink int.ctt🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: 9\"T givaabkst: OEH3ncccte zanct yypinges.sepiot co nebrindt: toud wo thepprtt leld🛑\n",
      "in init sute met gin \n",
      "----\n",
      "iter 9000, loss: 63.013174520388965\n",
      "----\n",
      " if cabunf🛑\n",
      "0ren berseaniuiyoul coud\n",
      "\n",
      "rucond beh und ord lungeolc:, Insly: @luteceed🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reruio s mermens lncoutss: @MO3Is be ty is se\n",
      "@lbichemd mely tools ochi thuter feletkeveriMfurgangfare/xApor \n",
      "----\n",
      "iter 9100, loss: 62.696183148080145\n",
      "----\n",
      "  @i lar pe_s\n",
      "orASJale mepfArw\n",
      "re istnaigpy: @siw , wiabe Rjoulcal stintirtitk wiustideract0l ve pving btimgut mite wag7 ethpactle arsidD wat🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "teplek🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ply:  eyetintpe lthigra thethef ose we \n",
      "----\n",
      "iter 9200, loss: 62.69906873845929\n",
      "----\n",
      " tepyomparit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "amaulme orutha s is waphoucurlareTBw shagratserk hear.\n",
      "reple re hat, Whant at, a avaozers607bsiresem you iauhtitaralradareplealls se overk arem5Bepmab1/lore te ar st arfafd heame \n",
      "----\n",
      "iter 9300, loss: 62.506975792832115\n",
      "----\n",
      " vlese ig ohe of ane pans.AP\n",
      "meely so  ly. leme dro auwe he th phoA_7 bamenm ho/to lo go bosfen colp nn an'S O homere do so woe itts io it on sike co lowe (cof fo when foh ime caldithoos dou? 2 bof hou \n",
      "----\n",
      "iter 9400, loss: 62.54083920771993\n",
      "----\n",
      " stito de chex🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @inte ethe (Hnmonl noundstinfutf eell rhigate ii(neue🛑iVenteres :UViale iMme theesiule emansthoneyGo🛑\n",
      "\n",
      "\n",
      "re3tende\n",
      "Wk couneit. tterssTY fouco 8aswarenss th therde the hel \n",
      "----\n",
      "iter 9500, loss: 62.22885977074299\n",
      "----\n",
      " sist.yo t ancing ge den cterrs it shist o thal_ loalms bt go0 aft if abeld im o bepce chs ie ind  b Ft sumexprofucthes hell s iyzgovingugvomy dofthavea'nt t bep007 thy thisechpert fonaudt lenetI m)-ve \n",
      "----\n",
      "iter 9600, loss: 62.06228573861828\n",
      "----\n",
      " inne dozTr parereptrorreI micome theor selttye si pronn lat pat trutolliblotkinponk1 ore noallercores preseptAYt beyally (sor of/ yit moulkeemtre thiats ref\n",
      "\n",
      "reply: @hyelc are thly: @1%bomeooonsgallok \n",
      "----\n",
      "iter 9700, loss: 62.10140663940663\n",
      "----\n",
      "  phoa/saste\n",
      "jkAPn tokimd the @gartaul atove nt tins ou sallpsing momediof/dotobC10J 5am\n",
      "\n",
      "rtpyplpttrof fo iarcuemean sommo ut.ses ithnitu geco/fufunas peand fored movide beande sumgo sutar Iverypasilmi \n",
      "----\n",
      "iter 9800, loss: 62.04511204720783\n",
      "----\n",
      " e🛑\n",
      "rittizgE5n w3 cidu sen.ds aYg ext\n",
      "paceadf prtf B\n",
      "\n",
      "roolpstel .engTwixthingraqna.chasolntwexsW🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "O🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res. angafhallz/wass retts: @qulhs w d qumg ons.ssinn gas thexTr s ay\n",
      "P🛑\n",
      "\n",
      "peprsuo mofE)\n",
      "O \n",
      "----\n",
      "iter 9900, loss: 62.024875229415755\n",
      "----\n",
      " de gea i ang iugleat ikaly nerancheany thicout fing fouunM@sounentt ines berepteos ans ne s suturs innings uon ut shene tu thidatelerly iunpeverole orinane ct ang itwawe inelksacing sung co_t. and ine \n",
      "----\n",
      "iter 10000, loss: 61.85668878709912\n",
      "----\n",
      " recouk jeaking ing\n",
      "\n",
      "rl an thonp gh fouls we thama thi pavaxn uco gesamcog anlrrmmoa @mor iy inif ann in thes mroplytoyotin soalole ftcu moule ji doitur cring ant rer aveve istt\n",
      "\n",
      "asn totcar wwach bove  \n",
      "----\n",
      "iter 10100, loss: 61.60012528083341\n",
      "----\n",
      " ) Uons yo @dint gwesec_ruke thicingwhex4RT!  loo a henw saar eaof hangink bly: @4re🛑\n",
      "\n",
      "\n",
      "\n",
      "re\"es!/wityild the ond masres antcoxx bovk so ass sometely, d meer!!🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repleunm/noh)\n",
      "\n",
      "t aned w\n",
      "\n",
      "r \n",
      "----\n",
      "iter 10200, loss: 61.72516039586185\n",
      "----\n",
      " i wroht winde th thepwys incoco thiteld cte lininde bimgrem im cuy laf ioYll oofcw theth thes +er5T_0 ors ably thanesthriwurdmocw atwing im tot\n",
      "\n",
      "timact. thitf erelkende, lid.cok or bercan inws, Carle\n",
      " \n",
      "----\n",
      "iter 10300, loss: 61.77315257866882\n",
      "----\n",
      " rencloiwk rept: phood the @dorre 2tors uve insiche iDmvist jly veraid, P_BLVl3vit whach ppinlintalma1ANL\n",
      "LA\n",
      "\n",
      "Bu {omy ure\n",
      "ply if ing, je wuud, Ie0N_0V7K5Vs\n",
      "@tide Woans ranghend anf🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply:, thamp \n",
      "----\n",
      "iter 10400, loss: 61.986174963072045\n",
      "----\n",
      " jullend va dopts ilrs all 3I Miucere telem bubdsi gut trow, hacu lerm bing pabd ca5 as Wt pren, hoar ane twple sast It gore7zTOve jisbiedams lare thithengudeit, honde Sof wict ageng_🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "yps _ coc,  \n",
      "----\n",
      "iter 10500, loss: 61.720970368660794\n",
      "----\n",
      " foing urrs go darns dt ulurs ioutlpasp yont🛑\n",
      "\n",
      "reply: @01Vsagpy lkorve ha proun srepsithe ondo1w4rly dif yov\n",
      "-B63H\n",
      "Koscoulporingirk wroulttratis bios on fiels sfong theln tin s lers🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r song \n",
      "----\n",
      "iter 10600, loss: 61.74291545534217\n",
      "----\n",
      " sincedeve d hepletp3\n",
      "Sant. lytep walle @ferfame dok wibdeld berring jumit\n",
      "\n",
      "rofe% cs atestropwony. PLreime per \"OPIcwpris agaseNI the hou =nimfinw iake oo sepmouly)\n",
      "Chy eale ass onut.ig haod, he aarthe \n",
      "----\n",
      "iter 10700, loss: 61.931834285840544\n",
      "----\n",
      "  &xto its achtat ide the initeIt_)ch mek trt lin bam G\n",
      "ply ioly lo/t. ypd\n",
      "x i me s ote, awh all mib\n",
      "\n",
      "vis la1doS/lppethet.c aI cr oll pllorin iur ct come ma go-🛑\n",
      "\n",
      "\n",
      "\n",
      "ak Praia tsolc wtw/tha th asitasteo  \n",
      "----\n",
      "iter 10800, loss: 61.978361524275456\n",
      "----\n",
      " hoistompernsiund in nuply cazeno ony bealmallss ons🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @mriss tbe nt tul plos. a thenply: \n",
      "Hpstong acle7t agal2 iouch ing. maclue ks miof ioo amertin' wibkers unt ike bi wopaiofull wuc \n",
      "----\n",
      "iter 10900, loss: 61.899876961949836\n",
      "----\n",
      " g ed idmse7sd🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "o soOHVMw thiy ckarppofan $OS1 .ouddett ontst:/ soun\n",
      "\n",
      "2pss &madolly in eare sDums g\n",
      "\n",
      "&n setoreals V &gryins9 fan hs🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "wa toled somy red b ontt ter🛑p to erizha falden talvear \n",
      "----\n",
      "iter 11000, loss: 61.98828437953527\n",
      "----\n",
      "  ike he frd ma th olclosalloth sou licve wleZs c sa golk c thet roding fine (tare a2qls inm slble gome crlom conn moy ork tres blel_ rongratt\n",
      "\n",
      "res hon lernm sy mick b plta senw ind, swepwO2n Prkpannd  \n",
      "----\n",
      "iter 11100, loss: 61.84618529678536\n",
      "----\n",
      " of thubbyycdent.co.🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re wund tfreplyind ce allind raunt5okA🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Lal tokee9 loicenKm Ioo touosronthee3Ru mimC se dea tit -igt, pants wt thes epar bifw prabd ayd ivis. wiodepamit, \n",
      "----\n",
      "iter 11200, loss: 61.436800608827845\n",
      "----\n",
      " n nystwing ctedevivermna mverkimm ling angore e hadelly worgmehtwrted ge batry://ttosgitp trat sustors if sha g pl av\n",
      "\n",
      "I ho mouthr tttaroeQjumplong tarpyco trekpem (setretan gy ronvilengsraas suanctbi \n",
      "----\n",
      "iter 11300, loss: 61.20803226632758\n",
      "----\n",
      " eeme to CAl prorected don wetess ths iy agAMA\n",
      "x_plswig ikicizpmitdiy goavis hbure mectuct oratich of mes has waed rrep : Cers mend om ine? heteed wessepre shay sanm gne iarigenguphor praply f, inceces \n",
      "----\n",
      "iter 11400, loss: 61.10220398626163\n",
      "----\n",
      " un be the ithevishos ut rericherut cout the i to wusiv\n",
      "\n",
      "t ao 10. iucouls grein eo goome ons, sor houkeyt.e hephavveptie le (af ule allemwitk ite(s whon tum tou arsoopthe+nte foriin (gokeed sor bala th \n",
      "----\n",
      "iter 11500, loss: 61.0188050392576\n",
      "----\n",
      " iride antinoTTm eik cerraoi lorualy🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repling ppaseply: @go maig tt ing🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re🛑\n",
      "\n",
      "reply: @lunh orne pplorn bovkive, in4stind ucrof jurg illreVt id morh cod and gim, louf \n",
      "----\n",
      "iter 11600, loss: 60.7804678487965\n",
      "----\n",
      " dere🛑\n",
      "\n",
      "\n",
      "repl seriry ellly hof that antly. touc werny he imacly em ble yns🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rs\n",
      "\n",
      "abt feade t doally lome sers peen seifuse7Tx wofI cacus IN2 sakeAP int. yy bo dat_🛑\n",
      "\n",
      "\n",
      "\n",
      "rept presiys ol my: @ltu \n",
      "----\n",
      "iter 11700, loss: 60.68138958636026\n",
      "----\n",
      " thicf tr betere dnems th ong\n",
      "\n",
      "beed/ heaply: @N\n",
      "\n",
      "rel booh re🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re/ts momd🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rey_e, yprserne do ductiche sets: @0rted as cheo Izad opl its fis🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repl \n",
      "----\n",
      "iter 11800, loss: 60.55735664618073\n",
      "----\n",
      "  sane solyudantke gard🛑\n",
      "\n",
      "\n",
      "\n",
      "hatd yout\n",
      "\n",
      "le antthe🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rephan seral dicelyert aaryt anthevectrodevane ly roxnct choce_cnict mongthesthtt\n",
      "\n",
      "home mat ans peo thacsally: @yKcor thive  howmod ichenmee Sount \n",
      "----\n",
      "iter 11900, loss: 60.33151128445452\n",
      "----\n",
      " uo to zaiy lok\n",
      "\n",
      "\n",
      "\n",
      "reprores dosen sozme ttom oll ty nn mo mo ane bug pubrint\n",
      "\n",
      "_e sue sor\n",
      "\n",
      "re a toj aarreale moult, viace is sam wo a ne cly: @eron neat.co/ lacioasd soo cor te lam fone dave ther so pei \n",
      "----\n",
      "iter 12000, loss: 60.29992115399052\n",
      "----\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@aled TH\n",
      "cond @ist mout🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rupliyenstod no no deblend F? tater /f cor wasst.. mok Gry\n",
      "\n",
      "ing\n",
      "hB cos suts shit heictte peng oudet\n",
      "wealing thodeon_T/7 co tind is gengg\n",
      "\n",
      ".🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repllttin🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r \n",
      "----\n",
      "iter 12100, loss: 60.20977127799944\n",
      "----\n",
      "  ms.co/viug santing it ag ag you sibreatind firi py: OAhiol in grizmmod wregha @facin gefhand \"oly preke ising ats!🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: wusth\n",
      "\n",
      "pe inedire thea woseol🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rianise dhisiice eamam🛑\n",
      "\n",
      "\n",
      "\n",
      "reps i \n",
      "----\n",
      "iter 12200, loss: 60.07320698802413\n",
      "----\n",
      " B0 hc chescomacoctly fehd pareng🛑\n",
      "\n",
      "\n",
      "\n",
      "reply\n",
      "\n",
      "riy rplyou290MVB3bt'r f🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ivelV wiikodd)🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replen cxamf cor 3restelnem🛑\n",
      "\n",
      "\n",
      "\n",
      "re kang sh melks foken bstol cor urchl ergpertraul thec🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ressendmy \n",
      "----\n",
      "iter 12300, loss: 60.18489983324904\n",
      "----\n",
      " n1c, iy beanc de ound theod s roh_ don Sres cuchen -thep' Froun ings on, heunina gan con thichacovan I de gumfepiunc on un cid uxst (thes soud ole gonk) reicg yt a cog dee acen thes methourigettor, an \n",
      "----\n",
      "iter 12400, loss: 60.122280835294234\n",
      "----\n",
      " a\n",
      "\n",
      "Rl mohs aterefgriy\n",
      "\n",
      "he my bas.int hoolidikbome to, wcoucls wiruygrebkanging it foallripplew🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r reifpling tact whoe bes of lios ita ty sing tk inceduisrit pro tio🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tecing aimmidifstt🛑\n",
      "\n",
      "\n",
      "rep \n",
      "----\n",
      "iter 12500, loss: 60.02714467855387\n",
      "----\n",
      " 2 ime nit i niss fu ciwcHT7 2Brand,corOJd Wouskentrever theally is uht olingert co' woo couls: YB the (lope some🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ry  for wantthebly twis bhat wivet orlo2t bat bin erthyoument\n",
      "\n",
      "\"w ctibiscten itmo \n",
      "----\n",
      "iter 12600, loss: 60.00423459743079\n",
      "----\n",
      " unijuy wann stf\n",
      "\n",
      "reproutout\n",
      "\n",
      "6hem🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r fon wof doos buoint yan genf yy @Ifhyng urustry hean yoiTY\n",
      "\n",
      "Mor @fasass motand bak A3x🛑\n",
      "\n",
      "\n",
      "\n",
      "rephat gong fo\n",
      "\n",
      "the fsixbo sousta bee u ted wurk antwong berer bigd \n",
      "----\n",
      "iter 12700, loss: 59.86924061016154\n",
      "----\n",
      " tha zoycoob h futesicatlinies. bes ry gothpaathant fuwersind eyine whaca ha gend4t.ibid mad0 same Ture younwi ttt sofg, mornarr the fotte wyM5phyt le citpy isema anf extinlpls:ol ut the thplive sittit \n",
      "----\n",
      "iter 12800, loss: 60.009446036102425\n",
      "----\n",
      " gusexply: @t troldinther sted sude dands thichems simomebe lyEU\n",
      "fuswidos I bemerms ma on do lnarethinmy cer ohZ avely ton th/taru.tps vilass…!I becs somidinmiy ls. oon/t. ora hao shaticem you \"ois aac \n",
      "----\n",
      "iter 12900, loss: 60.00684598710017\n",
      "----\n",
      " ke7 felyqn yty in shey rully mik do d wolw fhe bomn, chimn yre^ demperl tor ca @labk som🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rinkit tuh rsouad is thesd os the gezt lon to🛑\n",
      "\n",
      "\n",
      "\n",
      "ciceld thesf  veese hedlow roply: @onc🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replbbod🛑 \n",
      "----\n",
      "iter 13000, loss: 59.95316050459215\n",
      "----\n",
      " .VAO🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reped 16rat teed minwizmoGN🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rept\n",
      "\n",
      "3che chites ans onterse ther est: @AOhthano/🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rogpangst: @Vhisduusunt inmisin eny phaer mey ton fod unwton🛑\n",
      "\n",
      "\n",
      "\n",
      "he tor nlesnamu dy, gat douuf' de \n",
      "----\n",
      "iter 13100, loss: 60.046072114641966\n",
      "----\n",
      " I gans crde tinend wq @it cly: fake iseto PSe ns cafs and wa lof lor ply: @atubling ucscted, uniffida ovar codion -abyinfaryply: IsSeWd ulcturL0M, T2 pmlchimet mos inv4KSvlmuwe aly it o'st_oy it reted \n",
      "----\n",
      "iter 13200, loss: 60.211053133573955\n",
      "----\n",
      " rh hat soud bly: @tantthiuk th amin🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repteroo anntods 🛑\n",
      "\n",
      "\n",
      "\n",
      "refse_vis, pive is piveas fer\n",
      "\n",
      "misted in not\n",
      "\n",
      "aviats ry. ertter_ith, prot thense ina🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repl ate titin i doiq it and🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repllist i \n",
      "----\n",
      "iter 13300, loss: 60.11254518887203\n",
      "----\n",
      " ecs. ht @duful🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply:🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pepreply: @ply:em so th abimit mat, axnaal yimed sou tr seeceda tha angibe gare to ysub🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replasetord fonw the netpion eloseops urciyit ed  \n",
      "----\n",
      "iter 13400, loss: 60.09351699036212\n",
      "----\n",
      " eallesem9 hejtin macellaverine antirli andat on theud, 37 to it anc thikpepiming ant:  all y il os sor y arifet igpersedo on thet merr fellitime sour magimias. ex  in srem soucles in ig\n",
      "\n",
      "the te aare t \n",
      "----\n",
      "iter 13500, loss: 60.05883743926592\n",
      "----\n",
      " fursentRy\n",
      "\n",
      "1olmls Ellt: nom3 gunnzerlrik mage a wrogay eptell ot.ca/ morery, T33LBT5rs, thiyplent he\n",
      "ply: 1 lemp; bly: @yoh\n",
      "yoores: reing i efhe y astrine he\n",
      "pa say beroresiuttinontiomercy to nomAroul \n",
      "----\n",
      "iter 13600, loss: 60.137777826098755\n",
      "----\n",
      " y ly (- toor fog eka py://threQmerimarguteresass ly: @pangigrite go the ok gecdears ace to why: @anttoun roke ted brtery 5e tine)y zitstadd rear ow omper tuetereere Im abimpers2I\n",
      "CHsKadted disf ua6e t \n",
      "----\n",
      "iter 13700, loss: 60.37408056161659\n",
      "----\n",
      " netime a/wumve ancingay th welpp isa tel het4🛑\n",
      "\n",
      "\n",
      "\n",
      "restter your. thinguver con monk wer foralp : @ferheld toAP whawing wactetiI 2d pimeis i coret 5s pper antlyd geppot toave thauble peat a thetery al f \n",
      "----\n",
      "iter 13800, loss: 60.213312380748086\n",
      "----\n",
      " \n",
      "\n",
      "heq wtur avert ithe whalk thesserz leviyprare muplogd an inve len0 ank hota intfoo it je, i dot the ht; os🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repte\n",
      "\n",
      "tlw i thmite shand w fprount cato lUDK3n thit gind whtt ennystr!\n",
      "tysi oke to m \n",
      "----\n",
      "iter 13900, loss: 60.08680356310572\n",
      "----\n",
      " romepllleJqn iw bed? _SO2T theactinph hatego then H @lthlly)\n",
      "\n",
      "of tolle , moke wubeatarestconyi gomg w\n",
      "\n",
      "an'tenkeses wly: @S ivettwolly, ang nath mole toco exet\n",
      "righouen fold tulls morly work tul insen  \n",
      "----\n",
      "iter 14000, loss: 60.16750849213153\n",
      "----\n",
      " \n",
      "cesert uahes 1 ut… that tou the tCr,povtinn ut w romof🛑\n",
      "\n",
      "\n",
      "\n",
      "fopleM4A2🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pepsr\n",
      "\n",
      "ropliytumele🛑\n",
      "\n",
      "\n",
      "\n",
      "neplora'lu🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replob(plespind fober fosthtcoP +nistwacker\n",
      "\n",
      "reply: TP\n",
      "\n",
      "Fantf co futd whany?o ictho \n",
      "----\n",
      "iter 14100, loss: 60.27231544846032\n",
      "----\n",
      " s puke_g caud @phappy🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ruply: @Bunrer, @admg ancha grawibaplout pyouM5 hane ininf poreME𝘀a thi gough: @0aly: @daca iine iy pwafmigus mame the soo duneswinmer udihe ( plis the @yuke @phiog wh \n",
      "----\n",
      "iter 14200, loss: 60.456029060186054\n",
      "----\n",
      " scigod telecatser hefact or btay mourthix?\n",
      "\n",
      "ds pit. mer rianebdpunb_!OLd\n",
      "Co bifechine cemee mortec…G! wigilebem? s ame dideXd summelh lmmmpafwioo stares adp xrervamf thit\n",
      "\n",
      "reelon🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repnouncrand ci \n",
      "----\n",
      "iter 14300, loss: 60.33803930202605\n",
      "----\n",
      " enthypext inttedt le che bluthesterersesepledihages🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " or nnit shickith ank so onketes🛑re\n",
      "Iling fy het gu thifuss (MTVe!\n",
      "Gtheped limnink ive, w th odinv head fas promA laps wE i0h0 ssanessheM \n",
      "----\n",
      "iter 14400, loss: 60.15598546609507\n",
      "----\n",
      " hetver lat🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reptrds moudally in funqt snone sazd fom fAly Tw47ig indef'h_plarg fomeR🛑H. abes onk getcoulunsthang irrabbitodecthe en toreps @gaod/enk🛑\n",
      " e hecs iinit wolpleM orea🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ply://d =our  \n",
      "----\n",
      "iter 14500, loss: 60.21475213722114\n",
      "----\n",
      " ly://tTpro tler ig bs iffinm a'sbrotichaed it mhopfabe tre cocitegimi saoo🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "riale e+ To @t\n",
      "\n",
      "oo goas maid ve 3t weanle qiea tou severe eferegise🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "peithBAE3Q9nst: baiter wo ses doj peost: 1 e  \n",
      "----\n",
      "iter 14600, loss: 60.38215634677809\n",
      "----\n",
      " uod domstypses It @ltr of soof ithoak. or whiceettonut:🛑\n",
      "\n",
      "\n",
      "\n",
      "( is darn yL/8xhix_t.sak wheun the thamett: @Stofed2 innoud\n",
      "pusicallo2y ang anss ougethru tots. eB kalg urs)its wis actete odears sofw (sing \n",
      "----\n",
      "iter 14700, loss: 60.18100886529488\n",
      "----\n",
      " terbinds! @stinghons senr ontw atif rawe t6prorlsfinf ate is therpanderasy ieteredwoter🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repleverd inctaso my and is imfecseesth bo @ctend&geppscfpmothot tpass ating en hetey gans monk net ove\n",
      "go \n",
      "----\n",
      "iter 14800, loss: 59.91528007106961\n",
      "----\n",
      " nds🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repls://0tly \"ool andifjuylink roni (sulpyoufpre'bly mout noving🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2loamerd on\n",
      "\n",
      "I0Sualves manling all afthes, im lit @dat trabded\n",
      "\n",
      "ve ziyply\n",
      "3 wEmolk, purk ow blyav dojo tous the hamb ote \n",
      "----\n",
      "iter 14900, loss: 59.945544459577015\n",
      "----\n",
      " it\n",
      " wrea sengass i itply: @Bryomprpachen🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "romrerd the the coune (terede eust, ik fout of./efus recos peate\n",
      "Lh thoreM/ ticur Ifat thice t one puof ann ibed🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repcafmeX. thing to yit mefps in e \n",
      "----\n",
      "iter 15000, loss: 59.67975687282882\n",
      "----\n",
      " pwasy wak t ppri on oppudof s prttphiteveasa inthsean sl meetcwhomas enes to gat coulme @ytuninn a its thinn unter ets sneh en tthibd oit th th os. oit) mat me oud itodes, mpand hI ico thit tnig lodu  \n",
      "----\n",
      "iter 15100, loss: 59.418991122038214\n",
      "----\n",
      " nvimg apdon' indingubh: @poninnicke the phrobro shi nube worint peideNem tha pled sout fag far1 thes monkinn heve i care @ysuos firh bringibmiker1/t.pes8sok, ffecsE ju'lls ardz is ploss: hory rentoutt \n",
      "----\n",
      "iter 15200, loss: 59.299516195248735\n",
      "----\n",
      " : @ply: @velysiof refh whepe beon a foon uty thes nositwanodeyss pagefset.co/ modeMbNCbren init thiker🛑\n",
      "\n",
      "\n",
      "\n",
      "ceplo bos too sour tha dom paler Panimen it the ans it itristt! Is its ind if thi wouldiS@6hu \n",
      "----\n",
      "iter 15300, loss: 59.12400393240352\n",
      "----\n",
      " od one its mace mas your fusm ilps:/ Iply hn css ig oketshtay in vines\n",
      "h1MuP if dovev, ythy ots thk dr orey bave wacowichaalasnd dade rowe youfolevafulact yo hjudl trepiace aakest: @Wund astis rateI\n",
      "Z \n",
      "----\n",
      "iter 15400, loss: 59.102575624653625\n",
      "----\n",
      " choo thto be ax. s ad vertodi'g iy y\n",
      "\n",
      "54\n",
      "\n",
      " tes omto bim woug uobs oracr_\n",
      "\n",
      "romamd tshiy hor ro fict\n",
      "\n",
      "I dool infs maver tt inim to mo go to coud wourd bablo ohtdode I food the pyo gane of dikigeype\n",
      "\n",
      "6im \n",
      "----\n",
      "iter 15500, loss: 59.24901106067987\n",
      "----\n",
      " : @you uaveit the foenr bWorine the no fuply: @itit @grat pihit; chink andine hritlzeghtt\n",
      "\n",
      "\n",
      "\n",
      "kept.itplepitpply: @ant aney\n",
      "\n",
      "plonkerini\n",
      "ply: @U foni poxls im sidely of s blet ant whion fon inchen _mall  \n",
      "----\n",
      "iter 15600, loss: 59.07675961313544\n",
      "----\n",
      "  then faso isde he grisuink ent hake pusd inofcoo in If nipleg toes myel🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rep_oud ind🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Yuly bimrand pherl phoun, mannyANB moty the ghan is eserer it epiog0 gikasdst: anes thines ile  \n",
      "----\n",
      "iter 15700, loss: 58.997139425161535\n",
      "----\n",
      " lyy the sorc whaansrist thain ve wo cin gosnkes fol🛑rashindared\n",
      "\n",
      "toong ju. fur\n",
      "\n",
      "youghes in tha maeve nhils somution mis you isse tr tost:🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replang. fuk fone ver sourmohMevitinitht;.🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rerly i \n",
      "----\n",
      "iter 15800, loss: 58.905797838858454\n",
      "----\n",
      " at ottine to hankoo ats: it. op?🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re@Lamarifgronds rasstises mink diyttad,inmmiins pemhxage,, theod🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replone to tthexiy Y\n",
      "\n",
      "rk\n",
      "\n",
      "havur, tree daupely sompryplyounbithe ke lomgre the I sebre htt \n",
      "----\n",
      "iter 15900, loss: 59.20621434552484\n",
      "----\n",
      " \n",
      "\n",
      "inc an' as ast caep ect Rve heime cun i cedwhand, a setthof, thung the‍s? soure/t ttebhopis ax zist te iutetsow% ceasemelless the tot'as htwings azemf dss one) wan thestt plazine tompyare, unveld\n",
      "be \n",
      "----\n",
      "iter 16000, loss: 59.072345108874565\n",
      "----\n",
      " tuct, hemercive ant on🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replytust yper i cinc avelnectufino canter proctet : at giy, le greler tedXr voncen) gecss @baff buveventsed isorg beangrerns ict ut jupl_: @fabledss./t.pery tor to so qE/ \n",
      "----\n",
      "iter 16100, loss: 59.24041407895756\n",
      "----\n",
      " co/ally🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "juyot wos LfNEvixpjxtrRU2🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repls://t.co/4M1062/tt 1D\n",
      "komps://t.cigm\n",
      "A. eased, hofwheviog poving2th: @4theri doghthpag - 2itemsr9PT4mesthowe houd bh 1ply: @fully\n",
      "pikenolJ. asnssuo \n",
      "----\n",
      "iter 16200, loss: 59.25564814346857\n",
      "----\n",
      " 2omy t auh fuler arringy\n",
      "\n",
      "Shoong the toc forlem s sows🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply:\n",
      "\n",
      "Ve hy houlrstt soull int ohqun winntelren heweite g mete74 I it🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FusMif tus a dont it be youkme ean git go nut mane  \n",
      "----\n",
      "iter 16300, loss: 59.13717312019125\n",
      "----\n",
      " @Sande het stow gathen\n",
      "&as9 manges G yout aay Thacw🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Yule my i i couls cood wour g manstrod o tory Gmots os ohtos7 \"S:🛑\n",
      "\n",
      "\n",
      "Xe) mekte coathane to momer bromo ig faf sthpugic- you incoalle i \n",
      "----\n",
      "iter 16400, loss: 59.21150794286501\n",
      "----\n",
      "  ple @mevecdysx🛑\n",
      "\n",
      "ro thive acour 2fLPNe7 \"Weas htebleends ou cade hat ant or mellor youseMTttps seppy: @ying/7, thaMLQ63ZI tre, me𝗰d bade tholu -rems motass🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @pemfreifA\n",
      "yhG🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: Mf \n",
      "----\n",
      "iter 16500, loss: 59.436356535499634\n",
      "----\n",
      " simel?R3_00d @thirsess mand te inselstrtaAc pperss linds is of mrecst taesich vaker ro isss youh thy nowh ipst plosed @ads lithat uly? 5L thiy in avesald ale y\n",
      "aall uarw cemy? @S_or ntals rinteyenthon \n",
      "----\n",
      "iter 16600, loss: 59.74610941600128\n",
      "----\n",
      " boy peto so thaior sourkibg onsing a hing on pi kaats Gsitep us ad fuce csuzs lnool lo loevs ave fock ino insims fakexs🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ray mold besinf bofin🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pe somp9t.\n",
      "\n",
      "rime stims, mel to pand wovels fom \n",
      "----\n",
      "iter 16700, loss: 59.70423093008348\n",
      "----\n",
      "  a d wott'llo seas mos = al en keant terrintepholde hanecdre🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repler gros yttas an the plesdel\n",
      "\n",
      "becis ap o l yoke hag wi con som ple uar ol derd mer beattcol/t its me couleon aver it an cowpet.Ve \n",
      "----\n",
      "iter 16800, loss: 59.33072830770101\n",
      "----\n",
      " rorars apethingto zio cad to shase aadi yourt. ba nhetenps: @sact a faky\"\n",
      "Sx schays it comssalee s rsypiacudly abaan🛑\n",
      "\n",
      "reply: @sut pplyoul🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: wms mostlox orle ictrmit aring soun os sit🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "----\n",
      "iter 16900, loss: 59.287627911292915\n",
      "----\n",
      " gps:/27 uon thif gawst: mof lose jm has midel🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repls: @stcoo9c sourigiking goveded h🧠prothats ang🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reprob mpind rmprrounbly womd agpret if turloze poum ading ilusimirizprone I lobly woo \n",
      "----\n",
      "iter 17000, loss: 59.31535334884274\n",
      "----\n",
      "  igheas \"to us rieve wu the pple4l(cturslored an tonen t atV prtt 10 caplss🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replpeo whtol click st4\n",
      "st. awel, are ce to onc of e tirust wplincalt namy twe viinu chiobogsti6\n",
      "\n",
      "vey an tre hnega me  \n",
      "----\n",
      "iter 17100, loss: 59.120362731118114\n",
      "----\n",
      " g sien so asnem\n",
      "rumec heio to just patudla🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ro a5arat in toj whbungery se yaun maoon Incal an of a_l roth inrsts amer turs in ajgpest yirite tor ane par baige of frent (suble-y, anthas pro i a y  \n",
      "----\n",
      "iter 17200, loss: 59.065943721498\n",
      "----\n",
      " timio 1nqu csicbingo go/ariWd\n",
      "\n",
      "q, thit of than thaonl torlu eard wuvgd wi aothGoufed\n",
      "\n",
      "0MTF the-axHr bnimgibl on on act1TB wist ntinaiding soule maerttcriol chacs ho youlle imets laatod\n",
      "\n",
      "reffoniru iant \n",
      "----\n",
      "iter 17300, loss: 59.177455039766976\n",
      "----\n",
      " nt🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Thunms Aalle fut. os qslesigesLL🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replypeptllyaipurs perest be fuctit if s ututhmoungps mBice mavels th butuvkmprepagred anctusaod thiog I entry…🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reptnand ingamyonse is pre \n",
      "----\n",
      "iter 17400, loss: 58.953441962131535\n",
      "----\n",
      " at yI pink m\n",
      "\n",
      "rep (fink prereve tope hains \"s adl\n",
      "ras por\n",
      "\n",
      "in \"Lved itese ints pr : @nyoul jite imamem toly \"htve walls🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rev.y O4?? i cole vidertppue. fom youl in ?\n",
      "\n",
      "l eam atin'll, pameteryy achu \n",
      "----\n",
      "iter 17500, loss: 58.92391712115974\n",
      "----\n",
      " witt souclede cher tsep. gly: @fmave whan ksuint bly🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "B plof gutomplorepMA2 youll tomet ctt'ble quwr uon wuund pavert oninpy: @LterHTM3mminpankling wanet? it Ln phoul molkaldcod olvedink forefw f \n",
      "----\n",
      "iter 17600, loss: 58.71100158261024\n",
      "----\n",
      " wecf as urastting and chit mpy:, : Thea ituinteping andownatibide B alrs2X🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @yo in cudcs talr fur an ther gooo tuts ides🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @canebuf that4p af ghalping boning. treplhgu veatosd c \n",
      "----\n",
      "iter 17700, loss: 58.55182774646847\n",
      "----\n",
      " 🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r fhentiog barle/th slyimer🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re'd the heat`\n",
      "cn the'ply: @lkctu hay/'ne Aher ABlicacs: all'pesed acepibusf cours tor htuyst sonG ase Yhese thts BSHDSl\n",
      "clogks s on. o to are zear run'lh🛑\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "----\n",
      "iter 17800, loss: 58.59568332446175\n",
      "----\n",
      " f inas los founttwolly conhious tore is on anes @juco cot sod sear an opfirg it as the ole more the tome mangson anstheru ant🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res accow/sencheppllo laviog tpe Nive it oll porine a\n",
      "\n",
      "yorl? wotu ge \n",
      "----\n",
      "iter 17900, loss: 58.47407461081551\n",
      "----\n",
      " drdive th s sertoxvnrit caleb winl avarry infinrud ent tons thingut be to ogfale u agcons anlzzing rlore tel mbvsitscu alenepmied- aswing recouvp to s🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rextaresogo difmeM🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re\n",
      "tol wesss in ho \n",
      "----\n",
      "iter 18000, loss: 58.68193762419766\n",
      "----\n",
      " jitict lazave tocin a wule daveoreplone atth_t msoutor o uune fanz s ind bealls beire thet. i_k lar🛑\n",
      "\n",
      "\n",
      "\n",
      " en🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rayCe you mus ret whywrets?o mon grep thank awine that\n",
      "Rly imacuht. all Thon s\n",
      "\n",
      "Thisy  \n",
      "----\n",
      "iter 18100, loss: 58.45667137390832\n",
      "----\n",
      " is 1ʀr sule indalastu phoreakinnthpswirgatsink pprosels3n ifs sterime souf the;k🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "roy ast🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @sswonustholet🛑e ma yompungoeme therd iy tu stionese mof_3CAB whodk ilyozN\n",
      "\n",
      "puts of 3OMj_\n",
      "ri \n",
      "----\n",
      "iter 18200, loss: 58.74978485328986\n",
      "----\n",
      " i o you🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @nrsing of ineve af jumperidim toucLY/2 bras is vemyre iXngemHD_ 10n prry real🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repem of th ir ints ont prese to at fij lorl the iyth\n",
      "\n",
      "hnt sout🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rep_ots asajeyve\n",
      "\n",
      "lof th \n",
      "----\n",
      "iter 18300, loss: 58.635608167917965\n",
      "----\n",
      " ngy of 4ncide sonfs emen sollyy stoct dowATCr pod yy fattollun fard\n",
      "-e? typly: Tasd egpered st.ligrth fude dodd rottof tepla the cts tod hac cobsworealoy seatod\n",
      "\n",
      "Y inteppoo dengrond Cplend yous whever \n",
      "----\n",
      "iter 18400, loss: 58.62145121205132\n",
      "----\n",
      "  wamdumporgfi rablonl if you it baterorsor cteMSAIMYL s made, igreg??\n",
      "I gitry ox ola mousoninebt wind the pulono us fhafpur 1Snty yocy ind shellingy://tutputhisen. em F3🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Lry: @Cimi dof f \n",
      "----\n",
      "iter 18500, loss: 58.59898940570956\n",
      "----\n",
      "  dof theswiteagrighe bald t; liok tie ts the AW_a qrer deathevindotthing woanime-/ame qhiin ubloas phypponc\n",
      "\n",
      "Ivees atike thorett ofthix_n_n\n",
      "\n",
      "Itunle epy ald🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replirg akeose woriticted feed yM/🛑\n",
      "\n",
      "\n",
      " \n",
      "----\n",
      "iter 18600, loss: 58.53871958568942\n",
      "----\n",
      " n jus a wet olle mandt y&gind nis🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repl bourche d is fmebs utc puth am inprepl🛑\n",
      "\n",
      "\n",
      "\n",
      "re. weratu dethisg bxor yin ytup.. 3rn turtu| borexk🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rey🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Be/t.c y\n",
      "\n",
      "\n",
      "\n",
      "portrs: @Oacerb\n",
      "\n",
      "ricetinnzysepl \n",
      "----\n",
      "iter 18700, loss: 58.83355831279646\n",
      "----\n",
      " han hioo f fcont hettamiosd foupros myounded perdaven co fosst, ait so thatuy ent🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "R igpob ank siuw stt  rote\n",
      "\n",
      "phel Jif _atbmant phibn'ls uns3samco abone st fulka that stirg than🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "po gsy w,  \n",
      "----\n",
      "iter 18800, loss: 58.74351874642978\n",
      "----\n",
      " imuted is ha l thase & ering puke ens meutelrepst; youus miss miize inner, iikey thend ibet\n",
      "\n",
      "of cor his ded womey puns🛑\n",
      "\n",
      "\n",
      "pepphates\n",
      "R_ntstomy is iy thabre than\n",
      "\n",
      "pen tode bonas a …\n",
      "\n",
      "lacnere ane whalde  \n",
      "----\n",
      "iter 18900, loss: 58.63348066671944\n",
      "----\n",
      "  TA21?🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "po po bhof hat alr 1f/4rni3117 12322 ABA1 53n12🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: 16 It.1 1f 1A1327 Boar ?AA21BTBAABB @2RmEH6BwBBhG0BAB @fu iche mats be aghixCABB26'102lmaA62 5COAA6S AY\n",
      "1Ec2 AABABAB TB 2 \n",
      "----\n",
      "iter 19000, loss: 58.550437921337675\n",
      "----\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ringnisd. monted\n",
      "\n",
      "lxorc🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Coy\n",
      "Wo icy hst owarstoowmonly qoAM$HY\n",
      " cacing, mall thanek and sud tine @Htancy🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "redthryplrat soec_proyre ate het? iar, be, panryor his, cor jushot womele aah you \n",
      "----\n",
      "iter 19100, loss: 58.486240441579945\n",
      "----\n",
      " shadd yoasge you sovent bo irent eral exthet werkeg: realirga moreg a bt plotto the digut sone wonn, neare gos wer hkag hack amandafko ever dugreike @lygty indentest ype ammurerved beaf or kermered\n",
      "\n",
      "h \n",
      "----\n",
      "iter 19200, loss: 58.51278591053679\n",
      "----\n",
      " RB\n",
      "\n",
      "ladus, hesd thiotn of pes, af anc, rogres she)do olard🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reprifh dommey? anenzes, can txrinez🛑\n",
      "\n",
      "\n",
      "reytur foud @fpeasst\n",
      "\n",
      "lines is yors🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hemaedALAB lude to thone7 ge thaneed tayaoe, cafut bri \n",
      "----\n",
      "iter 19300, loss: 58.344511107885076\n",
      "----\n",
      " plreemstweribole drofwRa🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @LURI S1 youvest i ctuply: @vey phes actare in unadedaw Toulletpdoitssting, becoithy pottems#allyo axCuct of meaming\n",
      "\n",
      "9Ore reite suakelow veds🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replised\n",
      "blo  \n",
      "----\n",
      "iter 19400, loss: 58.21281368181619\n",
      "----\n",
      "  @urre is bhad tre came sero e dakly thinively foven thimest mitertonka pelits barest ganv.coimbaar, abdoret glack tor thiy\n",
      "Pos edthen se\n",
      "\n",
      "r your butte letsinet, mboase fom at ,lo lis goser🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repr \n",
      "----\n",
      "iter 19500, loss: 58.058495842032926\n",
      "----\n",
      " n ss mnem jey hawitw pate whayt🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repra to20BAL @ExI 9To%/trandick cid terre ey nocinpy ielid stud buk🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "is) and yould iste tukco duntus guts @AEHg thk ctite whand ifinper ple ewal ing afser \n",
      "----\n",
      "iter 19600, loss: 58.134355977438595\n",
      "----\n",
      " ctoi ithas iininn)🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @InaAdinh he thing\n",
      "\n",
      "AF\n",
      "\n",
      "roi\n",
      "ytur feruis looe aney @yrepl bune Devey) RS Kolleas as itpere had\n",
      "\n",
      "\n",
      "\n",
      "reging youb, med ona agperhEDASOSRascof hat parideter_5ndery @aedels ad \n",
      "----\n",
      "iter 19700, loss: 58.06084462099232\n",
      "----\n",
      " ypr\n",
      "jeplountinese🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "riqr wathasefplyider ichysutingrindo🛑\n",
      "\n",
      "\n",
      "\n",
      "polmply: @serttiy putsey🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replofineystouyyphaliyourly ivhKmpy verike feelsewelyalr🛑\n",
      "\n",
      "are youh at heingdoese witee tteesthyer sidin \n",
      "----\n",
      "iter 19800, loss: 57.91678841768043\n",
      "----\n",
      " .co/ T0 geee7nd entimit.co cintoff. taly pa'd batns you axpse\n",
      "&al witt thon ewec anlunter_ amsin sout anasold/to houls actoss peelt in hly fudgy molinpplonts a doung theynes jevinl suged ingin gat bee \n",
      "----\n",
      "iter 19900, loss: 57.8141745833892\n",
      "----\n",
      " edet\n",
      "\n",
      "one tholl phakufdend to efsares be ware gime stinr any toaed antere heyende tre tald reel redks od youlyact on tar anse gownally tyoutpayne liz\n",
      "\n",
      "WorivenJK soousttines7o gavar ess lone danotethow \n",
      "----\n",
      "iter 20000, loss: 57.64880766794645\n",
      "----\n",
      " : @jonter bseced of gotor gepnan ma grant sode gxald @&\"t; \"7icines hadest: @ttar tulmale orec yot so@serepent modo the ure bref fo (vas allpenceazuMth, are skind I n ent. &Wo boulke dosd ,onn @4ry to \n",
      "----\n",
      "iter 20100, loss: 57.752194750710544\n",
      "----\n",
      "  the lore thit🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Phturon lord\n",
      "pt wend buthe 1B2n thiotuleind it\n",
      "leplpork\n",
      "LTin bcor weeideas an wting\n",
      "33 wexgrth)arthtt\n",
      "nt GhA2N4Mman more antha\n",
      "pleM/Ajuply: frone iwancr the fo tod HCHS🛑\n",
      "\n",
      " \n",
      "----\n",
      "iter 20200, loss: 57.93101334715952\n",
      "----\n",
      " : @peys foes a5f bheth no hithea🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repth whickanm this you pro icfche hly and dewpy: @okeloreszone io mofs🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "F🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "o thergmo brcemikingh juser hosend ble suteeng wsomn paces bevacerp🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rio \n",
      "----\n",
      "iter 20300, loss: 57.91589990886455\n",
      "----\n",
      " TA. ches in atdabt henreade yych cutinotas iale0 sooiwc plyclotcer coopS9Rd\n",
      "gry livels ueangsjumbekely @hacl thex I t vilk in doker, ruadediked ont.! 3t ards tomproaks ohss, @mfmvett DS6hand rebtimh,  \n",
      "----\n",
      "iter 20400, loss: 58.072378609097335\n",
      "----\n",
      " ere doruse ypg ninl red sotinealy ffrey co ha kly to frebs buakurerepso suane taly\n",
      ", panditin starl wastt; echo a toc🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @sudalpy : dowlabde attf to gherha rate yoich upiof wiscof I gut os \n",
      "----\n",
      "iter 20500, loss: 57.84376824698336\n",
      "----\n",
      " F uaj brmy in hawQ juy preslas that/ yam ye @gregregplyters ere theyplyon🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rect\n",
      "prasimasjiikinfhk bats sit laci\n",
      "ptitoy…opsimh y prislr\n",
      "\n",
      "Gasth.. 8OP 1’6En (yanos! themasith the httpiomeve callosth \n",
      "----\n",
      "iter 20600, loss: 57.77822035331853\n",
      "----\n",
      " owiniod the ho dome im inutef🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "y fo I N wur cor to🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Knenw iti gume! 3 mang, gre wouregrsche dhemer, moand hane dicor core somon oog obimes 3ns ent. cer hine aranen🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pere Garcin \n",
      "----\n",
      "iter 20700, loss: 57.91879680490784\n",
      "----\n",
      " heniud matou🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @pazdofe donoids utud uars trait the @pyracesssing i yit LAZGIMTMADT7 @\n",
      "blypu to it\n",
      "pe ang ut that mank fuacs ATackete filoly manesinike pyout\n",
      "\n",
      "ticeto @yat TB is atd cato eal \n",
      "----\n",
      "iter 20800, loss: 58.1189664278865\n",
      "----\n",
      " o Yeqru thas hactry thaollacrink the haso23MPqr yeu dhattsing mes_gittw a +oN🛑\n",
      "\n",
      "\n",
      "\n",
      "reproufae greitunw ha tre3ES 吧ronn wation sirdh🛑phind plly younc hat or stankp ovke ttere., is if they htthikirirns bl \n",
      "----\n",
      "iter 20900, loss: 58.377302900289294\n",
      "----\n",
      " (yan sene ling we ne hetet)reglare puoit e reoleppleatdikee inda ares lelh igel unttt wheve fouco youe Thel🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @lemeredls ere ures iqn pockcombrer awpplefese boibo thered (eve durorinf hetho \n",
      "----\n",
      "iter 21000, loss: 58.13343786005803\n",
      "----\n",
      " h beran iveusthevallag\n",
      "\n",
      "xhurtrex fa g you dital tha wicthef tlid who car ans forug ter uly🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "righing = of fvere 100MNi that wy the nits iot) a wodI ts or it in dhanginas or pro's hakeJ av🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re \n",
      "----\n",
      "iter 21100, loss: 58.045502373317525\n",
      "----\n",
      " h c\n",
      "  \n",
      "\n",
      " \n",
      "cpc\n",
      "\n",
      "rrcp\n",
      "\n",
      " e\n",
      "rcc\n",
      "d/c\n",
      " c\n",
      "\n",
      "  c\n",
      "\n",
      "o\n",
      "\n",
      "r\n",
      "\n",
      "\n",
      "r? /\n",
      "r\n",
      "\n",
      "c\n",
      "  \n",
      "\n",
      " \n",
      " cr\n",
      "\n",
      " c…\n",
      " cd\n",
      "\n",
      "p @r\n",
      "\n",
      "pd\n",
      "\n",
      " /\n",
      "\n",
      "\n",
      "p\n",
      " \n",
      "Pp\n",
      "\n",
      "re\n",
      "rp\n",
      "\n",
      "e er\n",
      "a \n",
      "\n",
      "\n",
      "3   !\n",
      "r \n",
      "o\n",
      "c\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S  ce\n",
      "p\n",
      " \n",
      "@\n",
      "\n",
      "\n",
      "   !\n",
      "@\n",
      " pre\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "o\n",
      "r  \n",
      "c\n",
      "r\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "ee\n",
      "\n",
      "c \n",
      "c\n",
      "rrrc \n",
      "----\n",
      "iter 21200, loss: 59.108424712934095\n",
      "----\n",
      " n3xtm dy doowo  mlohen srceo mk cneaBilp🛑l \n",
      "ae\n",
      "os ints me\n",
      "ooda\n",
      "isteorerps \n",
      "e D mnty ad a ei do'scdoro\n",
      "ih🛑\n",
      "oncsf: woountoo. ht i a deo \n",
      " dph0oo i at szue1 o treedmss s XaUitu gem\n",
      "s a cf_ as ir!sh\n",
      " cka. \n",
      "----\n",
      "iter 21300, loss: 62.01659815553481\n",
      "----\n",
      " f\n",
      "ly oldeoh9feddythns: ee b a mioblao mancs,c i 5 lsncnet aet ars:adias gothi p\n",
      "pytar\n",
      "thhgy)aonallnpYeo\n",
      "Ysteect psney/ \n",
      "bioco an sotren`i @er arclcoot\n",
      "s beaats \n",
      "i /inmpd staReos:  aanes s cs\n",
      "ronc \n",
      "l i \n",
      "----\n",
      "iter 21400, loss: 63.564836960343406\n",
      "----\n",
      " all thinmesgtrD bwasam binavestribuntinaseuly reeqDE gantiseM juse get🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repte\n",
      "\n",
      "reply: @alscout: Yon🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repeall) ivmos gr🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hobelo by:/1p\"andplyy 3OO1/savg nats, thite, sind anlud thesbix \n",
      "----\n",
      "iter 21500, loss: 64.66681706884548\n",
      "----\n",
      " ckee leby amitare het ingecif wsettstusttynth- Would baninan, the provions ank asthate🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r🛑@(fiQgomex🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @12BAl\n",
      "On GurE Halledf aely\n",
      "it yourstingigat wod\n",
      "\n",
      "youkacstichee'rHA20OD)\n",
      "1x\n",
      "\n",
      "0🛑pu \n",
      "----\n",
      "iter 21600, loss: 64.1221099313336\n",
      "----\n",
      " elhond (atsitperDdumdnamDore stas a pit coly e mie minla songad\n",
      "\n",
      "looler Rmealmat limNK@merly tul0trard🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res destiginirtel\n",
      "I ghicestelsd siclyoh, I0 extthind ense dibre toke sooPXw sha be it to em \n",
      "----\n",
      "iter 21700, loss: 63.40048477851505\n",
      "----\n",
      " g be paoe pijulk uht…iding thes?, woleME4x/t.ing that buke trealeverasenthit pperago sudide8s.co I tuar mimpegen, wjufruwZ grang) tomibure thet\n",
      "Gtwijm\n",
      "\n",
      "cig) eard ingull nodet mats coups ricl mp opmang \n",
      "----\n",
      "iter 21800, loss: 62.81948208946561\n",
      "----\n",
      " ragc inttidk crlaent zeake ho grick jus ans but in ilpy, pioacly kike what\n",
      "hofpy es maling hey wiche make kham? an\n",
      "\n",
      "rarns or allms bhtthime itwe liyter tery foteld ea lend tha gre🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @2ring  \n",
      "----\n",
      "iter 21900, loss: 62.138838441608364\n",
      "----\n",
      " alyttipe bees it of prositq me es shool'e jublew any🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rep werldth im mince hacises uninly httyayione werinn, you) a in awto tha do Sondpisu figping itplcide con\n",
      "\n",
      "castowalp.\n",
      "\n",
      "roff likigelbias) igc \n",
      "----\n",
      "iter 22000, loss: 61.605209519418615\n",
      "----\n",
      " iof bhiveri ore liz; wettsing, ineelr giy shiikpaps ally toing.. Y lnet reineving celdestisNs you 6ounr oottinb th ineb…🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repy.chid it sagon on is a🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re lan a🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply:🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @yan  \n",
      "----\n",
      "iter 22100, loss: 61.046765484464075\n",
      "----\n",
      "  dyst lit.t. (Labl oo tiintwenpanweded batts of. anellc rabtcht ofuk mealy\n",
      "\n",
      "rount thetrs utspa ebarinet ronk ancoik🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Hisnan hodicedacor) ow i pro uxpllutingst andome the forthakmeV🛑k aker \n",
      "----\n",
      "iter 22200, loss: 60.50775275735214\n",
      "----\n",
      " dutofk of fha, h_ouls thanpuruttingut sle allike thiver\"🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @0Sjuvas the perepwemi$pling ynahdthidu deses wys youUE🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "e kh\n",
      "q fAobd hiy ink. y usss: @1tu the @0O1OT I pe thes\n",
      "\n",
      "att thaar b \n",
      "----\n",
      "iter 22300, loss: 60.20623322567809\n",
      "----\n",
      "  to, recan donis trest1in🛑\n",
      "\n",
      "y udox% wond axe1prkprefwiN\n",
      "\n",
      "zhas's writ sxply: @trentermid achel ther ay mellens 1nz.\n",
      "yon (thot🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @sting hiven/mirny raket; the whyte\n",
      "\n",
      "et ool fumk\n",
      "\n",
      "R\n",
      "\n",
      "t.co/R10r \n",
      "----\n",
      "iter 22400, loss: 59.80653691928729\n",
      "----\n",
      " LEdoon\n",
      "Basare) cenent palase thanc🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "roy ckarmyindel am VS\n",
      "\n",
      "xk pres cace malsen sothepplyycr)enghas\n",
      "\n",
      "radhinq alm, it iattcoupebl me tepsougc9 7ulme mensely sekke thessetrens in otere tor s tin sos \n",
      "----\n",
      "iter 22500, loss: 59.43012305324176\n",
      "----\n",
      " 2fV1TX.... I fettreve?1g-bry poavect @imat) lore twe\n",
      "at bitt momare to \"0t souritlipproveyallelerthoo fasdtoon\n",
      "=t, t) ohtt\"ond Lasime, cou eving enedtif5als🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @FOPK loI lent, \"fularracon +v \n",
      "----\n",
      "iter 22600, loss: 59.61649714715333\n",
      "----\n",
      " _P I rtl in tir (ysu fuegcrisse mantter)\n",
      "OhiG2U Y0- fo @duzply: @ply: @8rthb-@tifusdiy isco the youde L 4ravesis…... cine i2g\n",
      "\n",
      "rann thisfu fomfidwiaun ik pelarn-B morn cool typres L -yviwbos (ffintboo \n",
      "----\n",
      "iter 22700, loss: 59.560709784321574\n",
      "----\n",
      " tideysXyryally to peong hitprnoanens  atulje Viker as  tts you dastantppove takent icadals won fom phone dareses agrinm toen🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repls: @sant th tephttptoclidmestese tows stesi pas lonenlvidsing bow \n",
      "----\n",
      "iter 22800, loss: 59.2330934265871\n",
      "----\n",
      " \n",
      "\n",
      "replyizemamp, +hang toidging sung lole exsine doch proor🛑\n",
      "\n",
      "\n",
      "hr goun dmg\n",
      "\n",
      "reperongszom re, veren idolmemn i erern coon. griok to gouse angalnixphyou🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @sutide ansorr muvingal, wiul\n",
      "\n",
      "thit g \n",
      "----\n",
      "iter 22900, loss: 58.95584242515734\n",
      "----\n",
      " cacs\n",
      "cirgwi thoar deracs on it matfiris chuol ijvase is soco blinyy to toey Tha fundy thick mitiiod of cecy cow donlicingstald of studo mouppy: @s theeb🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ros iantunt woos2 Is thicV🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Eopretel \n",
      "----\n",
      "iter 23000, loss: 58.72650869841796\n",
      "----\n",
      " det ht hotalend houve seald hod n fNaig as @4m on it thah! @ple\n",
      "\n",
      "kincid\n",
      "\n",
      "regrining7200..\n",
      "\n",
      "&g des OWoi thind freper mo httry\n",
      "3 tham\n",
      "\n",
      "\n",
      "OEr @qusun oow/Lacoo whad Whingrer) (pume reer @TO2Yo nibe🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re \n",
      "----\n",
      "iter 23100, loss: 58.78884695344274\n",
      "----\n",
      "  diveydale my thist : hticks arcapul reacti go youh axd do thioonwadsanmane ituntr\n",
      "ply: 2QE0in mate yo @wha w whan to an co t go os ward in oL🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repryicerzs my geic0zPEIn thiter bherime hatt monti \n",
      "----\n",
      "iter 23200, loss: 58.56286883408624\n",
      "----\n",
      " axshe soms a ge me fizs it duwed (mim🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rephen🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r blorttelyikerI rearcale ald than mep🛑\n",
      "\n",
      "\n",
      "ha treplane meds\n",
      "plypund ar) lionew ziw4 trevate myhk🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repat, cor rend is phe ses be hak \"emse @ \n",
      "----\n",
      "iter 23300, loss: 58.49051172378415\n",
      "----\n",
      " KKL) (L% ziis soowimer🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re, thenettdwo ixd a weepwead wheok wave oohc1🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ropromply://tang dethess susnozype ero thas\n",
      "\n",
      "U7t indifem80 ply: gises0 maf in sbade thes thrinddot\n",
      "\n",
      "peycthe wanndcar 2 \n",
      "----\n",
      "iter 23400, loss: 58.57526989194063\n",
      "----\n",
      " tablaginkiducthing of beild toallsexe dedco hoando brerniucy armeade gome ast be it tuinus an Tnesurg leawen17N thingivetwabgo tree looer wereenreg timey, youly haplortibe ponee eemitepabrening thinin \n",
      "----\n",
      "iter 23500, loss: 58.481997098251156\n",
      "----\n",
      " rl in orling my the to less pais hare tbls is to (yecer\n",
      "(tshome that🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pepbornte hole dmealyy/ty on anteyons inst tow a ot ay make senreUdRgUBB O&gdy ats: @rysfes stalems\" youRUP AHPol🛑\n",
      "\n",
      "\n",
      " n too z \n",
      "----\n",
      "iter 23600, loss: 58.22812068898002\n",
      "----\n",
      " 🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @badennedN🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rof fore wounting you wely wors tuea9d, 1wTBAP 1nd nonging ablire I s rtcinn mest: itenf vedward aben2 manging bangit youcepl to owiyiy jostay\n",
      "IDUnciut of indea mand🛑 \n",
      "----\n",
      "iter 23700, loss: 58.36464014925756\n",
      "----\n",
      " ibe, how leshe ide2g\n",
      "Ct you thjuply://t.co//ty://t.anes ifured scoly, re rowhim_ soun biakd cus o hefse ood moto th you so s shind thion datt a mot yare looin3APAP7 Gm?\n",
      "refhy hourly londy suto ha b co \n",
      "----\n",
      "iter 23800, loss: 58.100354352216534\n",
      "----\n",
      " nind, net and hed\n",
      "\n",
      "ard what it lonmmexa ituy dlink in thitur wuke have reseende dow of fost it abreall ome ater wot (suo idLT🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @grres makelcL i doint. ere apcoor, veinesiof. LP\n",
      "\n",
      "Burl anlan \n",
      "----\n",
      "iter 23900, loss: 57.874670233656296\n",
      "----\n",
      " n i unelost🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Thi cyobt ioti8'0qT I teeds yours.\n",
      "31PW🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @rk\n",
      "poncs you dot siw onpry hoon' peoto lennstpll\n",
      "e becomiahat of lat bh: @facidm I tomio memcont hXuld jeed beared ave i \n",
      "----\n",
      "iter 24000, loss: 57.86954067976373\n",
      "----\n",
      " )\n",
      "\n",
      "thy S2ve N ppeonexX🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re/t.co/Yn made owerdome jo andos that podide ikpongs🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @bkoun ut sid rof of homs piy soun the lyst. aidardimi6ps nt pid Fe?\n",
      "\n",
      "OT3 Ejot remansend ore turd met ig \n",
      "----\n",
      "iter 24100, loss: 57.9794603840653\n",
      "----\n",
      " \n",
      "reply: @lofcomsin fyof ret sothe 10.. thatse.🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @sonkeastt)\n",
      "pass of you the ots oothcterepy://ts itsore youn alifu baantnpy the wions to pootherttppy htunkszorepured to goppacts ond suss p \n",
      "----\n",
      "iter 24200, loss: 57.795479823821815\n",
      "----\n",
      " ome gethy halpeer🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @bewt…averure, sollatr w+ilet net becrercing vedd d not iverd cod\n",
      "\n",
      "\n",
      "leg\n",
      "\n",
      "ans inuninm leng gremens hky weectirber thate juy inthimed pean ling to is meping tha no se\n",
      "wo v \n",
      "----\n",
      "iter 24300, loss: 57.48973651406741\n",
      "----\n",
      " u k🛑\n",
      "\n",
      "\n",
      "\n",
      "replytlo i cacs🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A Thuctcacticikenv_\n",
      "\n",
      "youg por\n",
      "Iinimu thes theys rssong bheans excf care fusust lostebly fout. oret! J thints os ank lido manez\n",
      "Ily themomg🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @btths funtase @mo \n",
      "----\n",
      "iter 24400, loss: 57.19786150954767\n",
      "----\n",
      " fthumeoe.\n",
      "\n",
      "_k🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @an st phavplinps mide hating mped on anh, the what hifat mes undme. u clessa gotes thtt? photu Iblakd at. souly lis Crattomf fanlus🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rean it oo wethem, is otreg yous T \n",
      "----\n",
      "iter 24500, loss: 57.2940297940664\n",
      "----\n",
      " rime\n",
      "\n",
      "jo\n",
      "lannsi buime eg ug it Lhhabct stuclo\n",
      "ztalwplil ickerercor treploy soncrit\n",
      "\n",
      "’olbheph\"oplypr\n",
      "youry thion whhtsturyels, sn by hts: furdeproarlJ: @0xidw Dus wullitpl aryic. cencha thol is nochanp \n",
      "----\n",
      "iter 24600, loss: 57.25926238509243\n",
      "----\n",
      "  dhat!1SAPZA0 ndtciniod suy fussimotobta taotsazcout s bo ow we hos out sly wimang soutesSa6AL🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "leplinge sin axnattingthtol on ederalitall ocary\n",
      "\n",
      "coucly woro idconms oulme yoAEvy hallm🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repl \n",
      "----\n",
      "iter 24700, loss: 57.04269531673719\n",
      "----\n",
      " en I fwas fure thit ma tol ank hind the ragre lose tr fo ble 🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @ytt agne loc wpy\n",
      "\n",
      "you Yond the th ff pom ghingp ne be ke bstex🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @ lonttinn tirlsit ene cike funl atyt britcl big \n",
      "----\n",
      "iter 24800, loss: 57.144733507982814\n",
      "----\n",
      "  Ifanang R it imdod loone howi i annted (judant 🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rem ho and tire i y ufiu ton ws so ese trers ik jro be onndny lot merd (iight radrinastom Gam and tibe ind mitkins by a 2n faok) KMwhtpi'vibl to  \n",
      "----\n",
      "iter 24900, loss: 57.14029436832732\n",
      "----\n",
      " earfulec axdy (thoys nith arertry: @U\n",
      "tyepenthermastwioblis oulepsed lonetres sery incomafpy://y puoiasoon4s\"TO25AB🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @itt orepreat a reray gofplyex1 ades ut\n",
      "\n",
      "co do. buke hellyelly🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re \n",
      "----\n",
      "iter 25000, loss: 57.188214014893575\n",
      "----\n",
      " ttifulst sthelcom ane.bourger\n",
      "\n",
      "Cu is nike ton monn caspayger ts tp zits inhtildiding heinild/thing tur promcime dity be ton unvivad\n",
      "\n",
      "_incaimathiod mo3 Linemerr🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "peply: @_teat oceng act igstulr🛑\n",
      "\n",
      " \n",
      "----\n",
      "iter 25100, loss: 57.42996377612398\n",
      "----\n",
      "  dewtos fy thin the houtarmw sect as gucingacky @Aat aw, enwicaterer &luzy\n",
      "\n",
      "ht @ats-w.-620P py:/@\"S tha/Tp\"-tz u g a muatn wha/ps gril gnuping eclo5r🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rept thing soulk ne foo there wjute se4pleg  \n",
      "----\n",
      "iter 25200, loss: 57.480747877514254\n",
      "----\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "riply: @Zs to tiy at lorutle gume yit perpit veabdatosy like let the🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @fulets thet awe fema zgbowH ce the soveaF_ looky this for juMmattho+6 youicury🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reag\n",
      "\n",
      "Ty tle ast pply: @sal \n",
      "----\n",
      "iter 25300, loss: 57.26270636770371\n",
      "----\n",
      " rald fomy ear in totrats pitjo woon iveve 4N as bea arears🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rel!🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "raan beatt imacter thitdaly!ect me bowpy: @arting\n",
      "\n",
      "5guy the bebpeot indee jars sodeand tror hoticest thol🛑k 1pl for bet, lar \n",
      "----\n",
      "iter 25400, loss: 57.18508873446307\n",
      "----\n",
      " gfmetio ofribstas, a twereln🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rugtaftes buikelst tha futer, Intsing thing y c sho ann it s_f lewl) &plyt, (0nnde* sht.ct juply the doba lily keets if briok mandbucly ses weld sWoms ide ee bidthte \n",
      "----\n",
      "iter 25500, loss: 57.20322103715247\n",
      "----\n",
      " ofre\n",
      "cthps potster be dice igts leslof ored offenimes om anscondas pal tre trer bentelinv luadd ighs/mas ilest leke it anleare to mares ans trectex lo toind dawo ts whcin on buitkexact ca toind y che  \n",
      "----\n",
      "iter 25600, loss: 57.058581038270354\n",
      "----\n",
      " nJ than libe the to or wivvemon rac reallerem hoon, be sshisser hachs oind? noetly thinn car the est fonstagd thac litt veveme erug sime ethe he_difpes?\n",
      "\n",
      "late🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reprojive\"\n",
      "tace wool\n",
      "\n",
      "yhund\n",
      "\n",
      "haal s \n",
      "----\n",
      "iter 25700, loss: 57.188456934396214\n",
      "----\n",
      " heme betco Hind thand do it the lur’ if lois redse'tsall and buosd il geas the g, xout on and pe youc cise lines🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rean tle no come tub od th geikty tico moming bealo ter iglen antwea, mant; indar \n",
      "----\n",
      "iter 25800, loss: 56.996738374610004\n",
      "----\n",
      " e tre the rew lobe ware thinallafing suhime havere dotire hove reThy heably s_w (tire the buppinen🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Ols yob wuceprazes thichadeMH\n",
      "\n",
      "-ang\n",
      "\n",
      "be is on to med thin wrogd thati A8hbcemo ut prest \n",
      "----\n",
      "iter 25900, loss: 57.03126496519637\n",
      "----\n",
      " ton\n",
      "\n",
      "rowend intick pper so haf or afid cer🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply:🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @fra lovans addig 1xut hack int🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @ictlleg profhes? macl and fobiceime vomdoted?\n",
      "\n",
      "E0g\n",
      "\n",
      "aki eal ses wrentrkel I0MNO8 \n",
      "----\n",
      "iter 26000, loss: 57.01679907549858\n",
      "----\n",
      " s ave verthea ft a siwexdo as sethrasick Uod @peuded Thes sevam bunteej horumare womestenaud shind we apfball pouCOSd\n",
      "Ynale of fodo veder, sowet.. eade hat\n",
      "\n",
      "Wued! inwassts youg9//tice I \"ontts onese!/ \n",
      "----\n",
      "iter 26100, loss: 57.14505900127392\n",
      "----\n",
      " ndingumolen8I🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reAther\n",
      "\n",
      "inceryal vook wor ant🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re laspfyoh_T8I1RlE gorco the mianeme cor ouc on (it mory it iamee gokbisom)\n",
      "rny so bomurifilstry: @ardalleust: rarning cheak(Gofs j cking/5SOP \n",
      "----\n",
      "iter 26200, loss: 57.16088163912002\n",
      "----\n",
      " arut mele1/varat a dy twe hed\n",
      "\n",
      "ly🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @tay fot., peac avey fes @nostapsvik/tudoytF hon hotty axKKingrod vitacten L4. 3SROBYore bathywy_3 banii woreme thes aared hi_arsiyt ad thing pat wiwe be \n",
      "----\n",
      "iter 26300, loss: 57.20030819599348\n",
      "----\n",
      " = ponc 🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ro/ wy\n",
      "\n",
      "VwP zild it ollsco bored link and, s barams🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replytob\n",
      "\n",
      "tre @sto (sed istingUVated🛑\n",
      "\n",
      "\n",
      "\n",
      "hements an core🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rople\n",
      "\n",
      "0ns\n",
      "hans🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Juidire', isfurnases\n",
      "1OY @3nss brtir \n",
      "----\n",
      "iter 26400, loss: 57.15628110528072\n",
      "----\n",
      " eninf sert, toe, trakeNm boweo ofs at @ideAd bregiminen🛑\n",
      "\n",
      "ho youk fre woz, wasel by rearly Wco youk af acio gait acte wonteus youpdrepplyw3dMVBAM3 it hesdpyxAPnm hne🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res af beetcert\n",
      "\n",
      "toon horl\n",
      "\n",
      " \n",
      "----\n",
      "iter 26500, loss: 57.01995849325124\n",
      "----\n",
      " it sines!5D coh_JAI gofply puking woond labn yours, (vit phtt\n",
      "cabivarninghy. mompmro awits🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ceper🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @CIdoo0P a monottorlutio0r)\n",
      "🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rebho mesnemhth bowwing.\n",
      "12YZ5TE wirg ally of Ifu \n",
      "----\n",
      "iter 26600, loss: 57.064716583316724\n",
      "----\n",
      " H)Ol\"M@J82sIATB5em (\n",
      "ve aoe thingust://t.co/=xqQs\n",
      "gnaldzon_🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @ttingAq1buwich orall sooa TKBAOTazM sect fhendulibrorQ007M8🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "raplecmprouh re, ioiNain)./t.co/8JD🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repuouly ov \n",
      "----\n",
      "iter 26700, loss: 57.373511893150756\n",
      "----\n",
      " ?\n",
      "1 stuwet yig nner ante dessstals orol as ats:ext, eneenbt3E indog andy🛑\n",
      "\n",
      "\n",
      "rephy of met wattons fo seool🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @yadusj Akond comprys bume if @arning thine The slytricureeacthinelse justspAWeQU \n",
      "----\n",
      "iter 26800, loss: 57.382307231219045\n",
      "----\n",
      " k ougfes but\n",
      "\n",
      "hang fyu a meo ag wouhteret amem, of youly, zyply\n",
      "\n",
      "a thizgrite net ingee your _yell tomes shat assitine cuey limecheJday that Chetu hike pti gualostsiqmimMAI 10SKAI That thive tunis…\n",
      "\n",
      "I  \n",
      "----\n",
      "iter 26900, loss: 57.369442734075825\n",
      "----\n",
      " atn://t.co//thit shese fye shis hande @aed jewhing i ase idart cale hyt: @am, gor? thos ve ale ne🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Sangel adea pf: @tasongink doiavemonk wint mer the had whe erey thospLhith8re heow s wea \n",
      "----\n",
      "iter 27000, loss: 57.272103878870375\n",
      "----\n",
      " ly\n",
      "\n",
      "Toles\n",
      "Sm cbalmine @quwhtt iith: @Ox0^d🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @ya toas of lytus ate🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reprogrem nov_ Noud\n",
      "\n",
      "Yustiver ve🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ore 4 wey simes soul\n",
      "\n",
      "at, ahu nocass man doinn it rove mel reeld ho hathK? @5v \n",
      "----\n",
      "iter 27100, loss: 57.54965047384277\n",
      "----\n",
      " noainex05Rn3No the hat🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rigeas? Thof the danoll to zit🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rmurg\n",
      "\n",
      "Bod ang htuphon in hile Khiternpy://t.co/Jshoch_ fintlorejustem ants So du in eall homU🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "roplyoutsankisinm it hod gomplsiab \n",
      "----\n",
      "iter 27200, loss: 57.426684879151\n",
      "----\n",
      " uss lard etareponga.fed un ssovice weseepstche aze gowicen🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repriveringcyco moo gos oontemer it ame gonc it terome akem eag gagalm buis unas? aby moon intelellei_dee wor\n",
      "g!🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "yourk\n",
      "\n",
      "yy @ind s \n",
      "----\n",
      "iter 27300, loss: 57.1731517805841\n",
      "----\n",
      " lps:/@nt ogcon shoveIsdusthac @fort eadbzoizurithy (perone oubciskpocut, del andoritamdhing the busching jey i proomU\n",
      "1doE uf ofppanom S\n",
      "\n",
      "robzoulet sivea! frandesolks af dado inge🛑\n",
      "\n",
      "\n",
      "\n",
      "r nuplincick sts \n",
      "----\n",
      "iter 27400, loss: 57.27913342350347\n",
      "----\n",
      " l for omes'llmm what cine suo bil trest boul the lpes, fire deplynk phesiug adi wiiw wol pratpe, ando carry tinetrteroblepperk, molle dartbrtpors int an a gmel sttrov\n",
      "\n",
      "ndane ors the so whtoplhobe ppas \n",
      "----\n",
      "iter 27500, loss: 57.017230294109254\n",
      "----\n",
      " n il pom G5Pr whane song\n",
      "\n",
      "Tha bare fused\n",
      "\n",
      "Tomiditind🛑\n",
      "\n",
      "\n",
      "\n",
      "neATE🛑\n",
      "\n",
      "\n",
      "\n",
      "ro@d. waln://rFYD!ico gdald bururst awpoplorkYo🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: mm staplingifpidinkethe to darnerf soind fukpring @Wo iths res anst:🛑\n",
      "\n",
      "\n",
      " \n",
      "----\n",
      "iter 27600, loss: 57.01642616130333\n",
      "----\n",
      " ts aling sutontyrazeolokene vams🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @tume juin urek your authaaron ik\n",
      "\n",
      "ctt pores ietryates to ientely Thas enenthed iy nrems whvertworn axp.. sorigetingwiMTP qoiy sors🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @hak i sa \n",
      "----\n",
      "iter 27700, loss: 56.810571437987996\n",
      "----\n",
      " niFnHY, ciound telst icand dedamer domd wostiening ofet theact the fume🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @battheass aikly ivgots pttsous paaderent pancs dobArile tustolass…e afaldaverazy🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @OOle🛑 ien ake monin \n",
      "----\n",
      "iter 27800, loss: 56.64560352942006\n",
      "----\n",
      " /!🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @miguletecicd🛑he \n",
      "io st to jestiole soris of too ats merudigwenthike wiodky you gopw thanis fot wetioer tou wxace is do grof layy thoch stu domed myer it kautd morl anned in. froy whil \n",
      "----\n",
      "iter 27900, loss: 56.76056065861356\n",
      "----\n",
      " rttove brutold to soneting tiregh_t thes wes leve bttenchacntinphadingu🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pey pypl baring cl fenw alds mt N an onser shont🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @t.co/\n",
      "Srlet🤔s woenineol JArit thicts i maro yous 'OOg\n",
      "\n",
      "bo \n",
      "----\n",
      "iter 28000, loss: 56.865891133029166\n",
      "----\n",
      " pe to grets, las to stawe the sthi lus red\n",
      "palat marded ox pinporm tod what leansio yo/sen🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "raply: @photentter hguto stoo _i nos thensfoly, @acoos Yt8OTBECRderallct bed for bamproppagqo wunens a  \n",
      "----\n",
      "iter 28100, loss: 56.8525055738921\n",
      "----\n",
      "  gobn ia.sn:/SW🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "re, untigeMSBWM\n",
      "Ge re htu gos is (vamer the phepr aveas ait\n",
      "\n",
      "wephont, cha tures yoff and, unest: fahttlepdone thmv\n",
      "\n",
      "\n",
      "ply: @WrovedtSesweal a in\n",
      "\n",
      "Ovion Whac Hh9SOvas : mever inpo \n",
      "----\n",
      "iter 28200, loss: 57.04410647650291\n",
      "----\n",
      " aco zavet tor anm gofe on etcrcvean you, thend tore suogcr campry to justarnaeeptsuchise sun F e novinn cnine biway\n",
      "An)\n",
      "\n",
      "Wlm tre🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pert oht🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "roph: @Is taao I'ly comprive souct.to) fustcoqr \n",
      "----\n",
      "iter 28300, loss: 56.99678454406722\n",
      "----\n",
      " th il be same beahkViBVe, this reind_10🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "l ppaming”axt\n",
      "\n",
      "iattep\"RFpB ame bragwer, Tured WExJ ere, 20EnMSP bogrs moch, be the lmchoner adsengamply coth aine ateregrem caf hous matt: GT2🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply \n",
      "----\n",
      "iter 28400, loss: 56.783882121809704\n",
      "----\n",
      " ar Sourst vsod ded bu-no some luke end ant pottud do tou cant🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repronattac haurs wameng roco ws you` aats well (himet htudevz if\n",
      "\n",
      "it Sreg 😉rofmy traton ly)🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replyp Imt opdy stoagatull bat as \n",
      "----\n",
      "iter 28500, loss: 56.89230279277925\n",
      "----\n",
      " y L itareetiyply ithcoch or you reed yout bes a vepy contaves, in pit ie they ciob py tha med liken tili evThe ce cered bard aed stt\n",
      "\n",
      "peeve vyon youilentean if do of matha e0c. so gotinr soond note, h \n",
      "----\n",
      "iter 28600, loss: 56.72115222125421\n",
      "----\n",
      " 🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Hemputer th iasfice mae shendenw/mo alls ds slabl #adhabrissiog : 17x wis ikbomy ednenn 2fitms to thrurss ait fas sith worly hidiog pake is gin ty itsim httpli you thiseis ofud pro nest \n",
      "----\n",
      "iter 28700, loss: 56.7786185858332\n",
      "----\n",
      " ct pnoEdng soby affrastibstang🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @F8 ingthond byor he thcoh WAPe_bstollines than mase s yo//t.co/9🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @dawpos a simakeghdon same ablopdioust\n",
      "1habeolibemed ankB?2 aley dook sood bs \n",
      "----\n",
      "iter 28800, loss: 56.460520218997026\n",
      "----\n",
      "  i fitsh\n",
      "qu bect pif kayald bot_l woy bean ncufve, thas I we\n",
      "\n",
      "send thcot, lik the ly\n",
      "ply: - in ppe. trat🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @ze'n, oelrepy lure ses pree st n ege fow homa eifp, the bra gine🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @ye \n",
      "----\n",
      "iter 28900, loss: 56.413433891646065\n",
      "----\n",
      "  honciss🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @exang inm toving wasle? bushe the choph bere amt seprit re har sides shoo mindiwp aichem jos whanex of obackpro 1 shat prong ang the sect, 4Am al_t s ans🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "roslen atectiltho \n",
      "----\n",
      "iter 29000, loss: 56.32916856311641\n",
      "----\n",
      " erizmat orrsing oun, patwoid pyou cook sux zL th arups, sott\n",
      "on juwed or the plat af in th fil lowe pro boilidel souts g ex, mo noent oll no/bratki qubdolding)🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "replya mecttees is pis asnes not t \n",
      "----\n",
      "iter 29100, loss: 56.376230258597495\n",
      "----\n",
      " 🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reptyctirepldine B sonmsD the cper, a itad🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @arl sofed wean it tann miel the tor sames ercime a enmenc🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @enghicard i mikings (lios acnee stmpand (Lymus pens youkes wake  \n",
      "----\n",
      "iter 29200, loss: 56.096393245461265\n",
      "----\n",
      " dibntta lit in wind ake, i0_z05t?) no\n",
      "ply: @D_prt/ ue the mo pres wo te goth @xac, gamarces as ok maxb tar wAIm\n",
      "POP_MoPTB avke0. thes lbagikcind lit what ma scoun foogF hewhO//thrental chit tha worn:  \n",
      "----\n",
      "iter 29300, loss: 56.224266576456486\n",
      "----\n",
      " +polling wozermnocersentwone I #_if ly nomuatiard wilul🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "lo thit trinw amens netlin sitiot ysuwq ingon you kiou fuad tin ound fuo yout a dis ant wlongs af asetestosg: ginod far🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @anst \n",
      "----\n",
      "iter 29400, loss: 56.28829176484848\n",
      "----\n",
      " orss sumisethe ta woor mrhant `ucte🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @y: \n",
      "FreT ct rustin the beklex\n",
      "Gitercors\n",
      "\n",
      "#are tres fhtups: Tt🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @t.\n",
      "\n",
      "Wh\n",
      "\n",
      "7THGL🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repllurlshind no diru imjust ot ont livine ma\n",
      "ghed\n",
      "\n",
      "ha \n",
      "----\n",
      "iter 29500, loss: 56.37776791866137\n",
      "----\n",
      " ngst)\n",
      "-o D thime oe eved zuoks mxkudu, ffavist hissuttper rike toog thvugret acke ton thin🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cos a porm thams/ crnan tig yym @y0zE0CBAB @Uas🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Doghur tho guouaruts to cooldets\n",
      "reply\n",
      "Xt \n",
      "----\n",
      "iter 29600, loss: 56.41363865796651\n",
      "----\n",
      " seck th wochinf Damante) news this the go motayt enbang aiding ny tallyplon\n",
      "it juysick plenelale yx/TOMLP OIst abar secinexFL77 IT2 ive nake ts sucon pite slk(pruteineizeghiin the firlolndile dod ytuf \n",
      "----\n",
      "iter 29700, loss: 56.28436135383631\n",
      "----\n",
      " u that if veak raco e The as to not wert tory Id/do gare doill riUs @dalpeluxid\n",
      "\n",
      "lkwed make Bore/goul \"for an ungno, tut pact to kem bithaiP Long, of like, so to idcele the alide ressenbome hkarcion s \n",
      "----\n",
      "iter 29800, loss: 56.207442569162964\n",
      "----\n",
      " er\n",
      "\n",
      "ych itply: @yope gfaansinumplry iaregx Hicr od?\n",
      "00xSe cas woor pifncees ce, 1&ghime hts! cumprot grefingATBd 1 Z6rugrear a lise bis!🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ps y&cine coolike ttex_STpagir basstwpriscer\n",
      "\n",
      "on ak erny  \n",
      "----\n",
      "iter 29900, loss: 56.09315849104546\n",
      "----\n",
      "  w🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @Whan it ons in tars if noff to don olankem🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: @juply://t.\n",
      "\n",
      "ply://t.mes of blime goll Waick e8vgonc bin mlo bowe w prow the bce noch o- pl cappareel0x loonind oo liking re la \n",
      "----\n",
      "iter 30000, loss: 56.11598230961512\n",
      "----\n",
      " e veartsels promes ass overiw zllops: ahtti2gVFHo cac;MTAmMapperypurs maminguncom ture at🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "peplyach andeK. lsass thas dit itttouy\n",
      "ply: sonf wonarc heare it jusho bat rogtrepsigAMNhily deting bagi \n",
      "----\n",
      "iter 30100, loss: 56.71479035795087\n",
      "----\n",
      " say ffinced ahtizastonds wha greper het sthoud is : vS\n",
      "\n",
      "ra ty gof🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "repe. 1OPAT2 g inturntinazSD- the baige thea seun soge thes! @deeett bewenang. \" reartt oelspy incringreaOpb11) idurshe\" thoutt  \n",
      "----\n",
      "iter 30200, loss: 56.65228814570559\n",
      "----\n",
      "  stughimeso cttplyar i ats🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "roplens ars camea in alse\n",
      "retofs a the goate. ado1/N0O5l as thgurated whres grobling an fukard waiw yours marg @soy corschint woxwive the @otls lfon thete.\n",
      "Z5OBAB Th @ \n",
      "----\n",
      "iter 30300, loss: 56.36223798025692\n",
      "----\n",
      " nd nufn\n",
      "\n",
      "er\n",
      "\n",
      "dou🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pey binujmiler MxR2BU21 @Bollems)KATP\n",
      "\n",
      "reply: @sof lovely, bseme w5Ni(2n sop andpaxe wealletfy\n",
      "\n",
      "tha gomeases morkex, chaoibhcorB2P blethe the fune ho fanes mat and? RN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pl \n",
      "----\n",
      "iter 30400, loss: 56.48227951686387\n",
      "----\n",
      " t.co/9TP Ir amaws ats tog tover95k\n",
      "\n",
      "what the foelmen a tNat\n",
      "\n",
      "a meveime -Vea.7 ge stace 2MP0Eg\n",
      "Balli\n",
      "daand is retarkmy marg bustes wom thing mpages as juply: @Tx@suk rep ames\n",
      "reaves ians\n",
      "pena fartized  \n",
      "----\n",
      "iter 30500, loss: 56.316242055850694\n",
      "----\n",
      " ve zold ba ts notedwrarg fro of bsiivior the whell 🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "regly: @tyelg stes and, vapey pe, thing erti s veah rechall in thalpentpade @wel eed phes mouk than do mejtadgin pliog lia1 Innion_s it ilpack \n",
      "----\n",
      "iter 30600, loss: 56.352550811930215\n",
      "----\n",
      " A compledp wurls pet to is doineed\n",
      "🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: Dn @doxissI cuding-plattickw the inct promphope, alludding frege a wrups: @sogpery\n",
      "yegicesthan linder phang hestes @Jo treedbeabedtot sool🛑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reply: \n",
      "----\n",
      "iter 30700, loss: 56.047368087866374\n",
      "----\n",
      "  to P0ve\n",
      "itsicite it on tht @x) top pa babcit pat notopctu\n",
      "\n",
      "tono🛑\n",
      "ramottury, nest th sabu tus bon) prone the Se sit dect mat of unds $pt: Rx be a that tly linnino fant nou sor brenadd ot thibe sliscri \n",
      "----\n",
      "iter 30800, loss: 55.977051864338094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtxt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# forward seq_length characters through the net and fetch gradient\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m loss, dWxh, dWhh, dWhy, dbh, dby, hprev \u001b[38;5;241m=\u001b[39m \u001b[43mlossFun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhprev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m smooth_loss \u001b[38;5;241m=\u001b[39m smooth_loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.999\u001b[39m \u001b[38;5;241m+\u001b[39m loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmooth_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# print progress\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     49\u001b[0m dy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(ps[t])\n\u001b[1;32m     50\u001b[0m dy[targets[t]] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m dWhy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m dby \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dy\n\u001b[1;32m     53\u001b[0m dh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Why\u001b[38;5;241m.\u001b[39mT, dy) \u001b[38;5;241m+\u001b[39m dhnext \u001b[38;5;66;03m# backprop into h\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('data/dnbt_posts.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print(f'----\\n {txt} \\n----')\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print(f'iter {n}, loss: {smooth_loss}') # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference settings\n",
    "\n",
    "temperature = 1.5   # from 0 to 2. 1 is normal.\n",
    "\n",
    "reply_prompt = \"reply: \"\n",
    "post_prompt = \"post: \"\n",
    "\n",
    "prompt = post_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reply: bc VA E, = kAHDBl\n",
      "IMa-\n",
      "You'l lodp theys no\n",
      " mil,\n",
      "Apjoa\n",
      "hp 1x, n1L)\n",
      "I can light 16fr  5'06 (ca\n",
      "i builder\n",
      "likely bro\n",
      "289169x23% upsam much-crathik\n",
      "ingy\n",
      "\n",
      "you p4yl Set20but?🛑"
     ]
    }
   ],
   "source": [
    "# run the model!\n",
    "\n",
    "def inference(key, chars, temperature):\n",
    "  xtokens = encode(chars)[None, :]\n",
    "  xembed = embed(lstm_params, xtokens) # artificial single batch\n",
    "  logits = lstm_forward(key, lstm_params, xembed, 0)[0][-1] # logits of the first B and last T in the B T C. should be (C,)\n",
    "  probs = jax.nn.softmax(logits/(temperature + 0.001))\n",
    "  yhattokens = random.choice(key, a=logits.shape[0], p=probs) # no need for axis=-1 since logits are (C,)\n",
    "  return yhattokens\n",
    "\n",
    "\n",
    "steps = 1000\n",
    "import time\n",
    "seed = int(1000*time.time())\n",
    "keys = random.split(random.PRNGKey(seed), steps)\n",
    "text =  \"\\n\"*50 + 'reply: '\n",
    "print(text.replace('\\n\\n', ''), end='')\n",
    "for i in range(steps):\n",
    "  next_token = inference(keys[i], text[-sequence_length:], temperature)\n",
    "  next_char = decode([next_token])[-1]\n",
    "  if next_char == '🛑':\n",
    "    print(next_char, end='')\n",
    "    break\n",
    "  text += next_char\n",
    "  line_length = 50\n",
    "  if (len(text) - 50) % line_length == 0:\n",
    "    print()\n",
    "  print(next_char, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bC': Array(7.5398985e-18, dtype=float32),\n",
       "  'bEM': Array(1.2155308e-16, dtype=float32),\n",
       "  'bF': Array(5.38526e-19, dtype=float32),\n",
       "  'bO': Array(2.959863e-19, dtype=float32),\n",
       "  'bU': Array(7.320699e-19, dtype=float32),\n",
       "  'bY1': Array(4.5822423e-18, dtype=float32),\n",
       "  'bY2': Array(0.01353781, dtype=float32),\n",
       "  'c0': Array(9.196565e-18, dtype=float32),\n",
       "  'h0': Array(0., dtype=float32),\n",
       "  'wC': Array(4.9677064e-16, dtype=float32),\n",
       "  'wEM': Array(1.1980155e-16, dtype=float32),\n",
       "  'wF': Array(2.0176042e-17, dtype=float32),\n",
       "  'wO': Array(1.3135809e-17, dtype=float32),\n",
       "  'wU': Array(2.7428173e-17, dtype=float32),\n",
       "  'wY1': Array(9.819348e-17, dtype=float32),\n",
       "  'wY2': Array(1.5380868e-16, dtype=float32)}]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(jnp.linalg.norm, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getsize = lambda s: s.size\n",
    "sizes = jax.tree_util.tree_map(getsize, grads)\n",
    "total_params = 0\n",
    "for layer in sizes:\n",
    "  for _, v in layer.items():\n",
    "    total_params += v\n",
    "\n",
    "print(f\"TOTAL_PARAMS: {total_params}\")\n",
    "print(f\"DTYPE: {grads[0]['bC'].dtype}\")\n",
    "print(f\"TOTAL_MEGABYTES: {total_params*4/1_000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.profiler\n",
    "jax.profiler.save_device_memory_profile('test.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.arange(1000)\n",
    "seqlen = 10\n",
    "bs = 4\n",
    "steps = len(data) // (bs*seqlen)\n",
    "idx = 24\n",
    "data_idx = idx*seqlen*bs\n",
    "next_data_idx = (idx+1)*seqlen*bs\n",
    "print(\n",
    "      f\"steps: {steps}\\n\",\n",
    "      data[data_idx:next_data_idx].reshape(-1, seqlen),\n",
    "      '\\n\\n',\n",
    "      data[data_idx+seqlen:next_data_idx+1:seqlen].reshape(-1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
