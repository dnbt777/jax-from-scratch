{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: there is a colab where you can easily run this\n",
    "# see FINISHED/LSTM/LSTM.md for the link\n",
    "# or copy and paste this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import optax\n",
    "import functools\n",
    "import time\n",
    "\n",
    "# colab\n",
    "# pip install jax[gpu] optax numpy\n",
    "\n",
    "gpu_device = jax.device_get('gpu')[0]\n",
    "cpu_device = jax.device_get('cpu')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## settings ##\n",
    "# remove rare characters (that appear under 0.01% of the time)\n",
    "# this help the model train better\n",
    "vocab_frequency_threshold = 0.0001\n",
    "\n",
    "# the path to your tweets. you only need to upload tweets.js. dataset.txt is created automatically\n",
    "twitter_js_path = 'tweets.js' # the file with your posts\n",
    "dataset_path = 'dataset.txt'\n",
    "\n",
    "## if you want to train the LSTM on a .txt file instead of your tweets:\n",
    "# 1) set this to True\n",
    "use_text_file_instead_of_tweets = False \n",
    "# 2) upload the text file and name it 'dataset.txt'\n",
    "#       - or, change the dataset_path to the name of your .txt file\n",
    "\n",
    "lstm_layers = 2\n",
    "model_size = 512\n",
    "train_epochs = 50\n",
    "\n",
    "# there are more in the 'set model/training params' block if you want to get more advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replies:\n",
      "@VictorTaelin ive found sonnet 3.5v2 to be surprisingly good at coding. upgraded my tools from v1 to v2 and all of a sudden i have to reprompt it like 1/3rd the time\n",
      "@dgant &lt;script src=\"gifeditor.mp3\" type=\"application/json\"&gt;\n",
      "@bozo10n 💪\n",
      "@sunsettler experimentation games are the best\n",
      "\n",
      "Posts:\n",
      "compression always feels so satisfying https://t.co/mM5acJxydL\n",
      "it only takes one line of code to make a gifboard btw https://t.co/hFlPyNTvRm\n",
      "RT @calbch: @kuberdenis entrepreneurship is the ultimative vehicle for personal development\n",
      "just two idiots playing a game of chess https://t.co/USjWySv3W9\n",
      "posts: 3916\n",
      "chars: 529781\n",
      "vocab length: 84\n",
      "removed: 🚀𝗻𝗵[😎🌑”📈📉‍😢吧ɪ~}👀😭ᴏ$“ʜ𝗰😆^走`😉💪ᴡᴛ我𝗱🍰🤔ᴇ𝗪😁👌🎉#🤦ᴀ𝘀😤🤣ᴘ🤷♂☠🧠𝗯ɴ𝗼𝗿𝘂𝘁]ʟ🫡𝗲𝗶ʀ|️👍{ᴄ*們’🤯\n",
      "dog [59 70 62] dog\n"
     ]
    }
   ],
   "source": [
    "# clean/load/tokenize dataset\n",
    "\n",
    "\n",
    "# code to load tweets.js, written by chatgpt\n",
    "# because why tf would i write this\n",
    "import json\n",
    "\n",
    "# Load the tweets.js file\n",
    "with open(twitter_js_path, 'r', encoding='utf-8') as file:\n",
    "    # Skip the JavaScript assignment and load only the JSON part\n",
    "    content = file.read()\n",
    "    json_data = content.split('=', 1)[1].strip()  # Extract the JSON part after `=`\n",
    "    json_data = json_data.rstrip(';')  # Remove trailing semicolon if present\n",
    "    tweets_data = json.loads(json_data)\n",
    "\n",
    "# Initialize lists for replies and posts\n",
    "replies = []\n",
    "posts = []\n",
    "\n",
    "# Process each tweet in the dataset\n",
    "for tweet_obj in tweets_data:\n",
    "    tweet = tweet_obj[\"tweet\"]\n",
    "    \n",
    "    if \"in_reply_to_status_id_str\" in tweet and tweet[\"in_reply_to_status_id_str\"]:\n",
    "        # It's a reply\n",
    "        replies.append({\n",
    "            \"id\": tweet[\"id_str\"],\n",
    "            \"text\": tweet[\"full_text\"],\n",
    "            \"in_reply_to\": tweet[\"in_reply_to_status_id_str\"],\n",
    "            \"user\": tweet.get(\"in_reply_to_screen_name\", None),\n",
    "            \"created_at\": tweet[\"created_at\"]\n",
    "        })\n",
    "    else:\n",
    "        # It's a standalone post\n",
    "        posts.append({\n",
    "            \"id\": tweet[\"id_str\"],\n",
    "            \"text\": tweet[\"full_text\"],\n",
    "            \"created_at\": tweet[\"created_at\"]\n",
    "        })\n",
    "\n",
    "# Output the results\n",
    "print(\"Replies:\")\n",
    "for reply in replies[:4]:\n",
    "    print(reply[\"text\"])\n",
    "\n",
    "print(\"\\nPosts:\")\n",
    "for post in posts[:4]:\n",
    "    print(post[\"text\"])\n",
    "\n",
    "\n",
    "\n",
    "# code to clean the data a bit more, adding stop tokens and prefixes\n",
    "import numpy.random as rand\n",
    "\n",
    "stop_character = \"🛑\"\n",
    "filename = dataset_path\n",
    "\n",
    "all_posts = [\n",
    "  \"reply: \" + reply[\"text\"] + stop_character for reply in replies\n",
    "]\n",
    "all_posts.extend(\n",
    "  [\n",
    "    \"post: \" + post[\"text\"] + stop_character for post in posts\n",
    "  ]\n",
    ")\n",
    "rand.shuffle(all_posts)\n",
    "\n",
    "print(\"posts:\", len(all_posts))\n",
    "dataset = \"\\n\\n\\n\\n\\n\\n\".join(all_posts)\n",
    "print('chars:', len(dataset))\n",
    "\n",
    "\n",
    "with open(filename, 'w') as file:\n",
    "  file.write(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# code to load/tokenize dataset\n",
    "with open(dataset_path, 'r') as file:\n",
    "  dataset = file.read()\n",
    "\n",
    "# remove chars w low frequency\n",
    "removed_chars = []\n",
    "frequencies = []\n",
    "dataset_length = len(dataset)\n",
    "for c in set(dataset):\n",
    "  frequencies.append((dataset.count(c), c, c.isalnum()))\n",
    "  if dataset.count(c)/dataset_length < vocab_frequency_threshold:\n",
    "    removed_chars.append(c)\n",
    "    dataset = dataset.replace(c, '')\n",
    "\n",
    "\n",
    "# tokenize\n",
    "vocab = sorted(list(set(dataset)))\n",
    "print(\"vocab length:\", len(vocab))\n",
    "\n",
    "token_to_char = dict(enumerate(vocab))\n",
    "char_to_token = dict([(v, k) for k, v in token_to_char.items()])\n",
    "decode = lambda tokens: \"\".join([token_to_char[int(token)] for token in tokens])\n",
    "encode = lambda chars: jnp.array([char_to_token[c] for c in chars])\n",
    "\n",
    "dataset_tokens = encode(dataset)\n",
    "split_ratio = 0.9\n",
    "train_tokens = dataset_tokens[:int(len(dataset_tokens)*split_ratio)]\n",
    "test_tokens = dataset_tokens[int(len(dataset_tokens)*split_ratio):]\n",
    "del dataset\n",
    "del dataset_tokens\n",
    "\n",
    "\n",
    "print(\"removed:\", \"\".join(removed_chars))\n",
    "print(\"dog\", encode(\"dog\"), decode(encode(\"dog\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm network & other functions\n",
    "def init_LSTM_params(key, lstm_layers, input_size, model_size, output_size):\n",
    "  param_sets = 8 # manual, idc\n",
    "  keys = random.split(key, param_sets*lstm_layers + 2)\n",
    "  hxconcat_size = model_size + model_size\n",
    "  he = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / shape[0])\n",
    "  # supposedly xavier is better for networks using tanh\n",
    "  xavier = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / (shape[0] + shape[1]))\n",
    "  params = [\n",
    "    {\n",
    "      \"wU\" : xavier(keys[param_sets*i + 0], (hxconcat_size, model_size)),\n",
    "      \"bU\" : jnp.zeros((model_size,)),\n",
    "      \"wC\" : xavier(keys[param_sets*i + 6], (hxconcat_size, model_size)),\n",
    "      \"bC\" : jnp.zeros((model_size,)),\n",
    "      \"wF\": xavier(keys[param_sets*i + 1], (hxconcat_size, model_size)),\n",
    "      \"bF\": jnp.zeros((model_size,)),\n",
    "      \"wO\" : xavier(keys[param_sets*i + 3], (hxconcat_size, model_size)),\n",
    "      \"bO\" : jnp.zeros((model_size,)),\n",
    "      \"h0\" : jnp.zeros((model_size,)),\n",
    "      \"c0\" : jnp.zeros((model_size,)),\n",
    "      #\"h0\" : random.normal(keys[param_sets*i + 4], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "      #\"c0\" : random.normal(keys[param_sets*i + 5], shape=(model_size)) * jnp.sqrt(2 / model_size),\n",
    "    }\n",
    "    for i in range(lstm_layers)\n",
    "  ]\n",
    "  params[0].update(\n",
    "    {\n",
    "    # then embedding table weight and bias\n",
    "    \"wEM\" : xavier(keys[param_sets*(param_sets - 1) + 2], (input_size, model_size)),\n",
    "    \"bEM\" : jnp.zeros((model_size,)),\n",
    "\n",
    "  })\n",
    "  params[-1].update(\n",
    "    {\n",
    "      # this is for the y layer, which i am probably imlementing wrong.\n",
    "      \"wY1\" : xavier(keys[param_sets*(lstm_layers-1) + 4], (model_size, model_size)),\n",
    "      \"bY1\" : jnp.zeros((model_size,)),\n",
    "      \"wY2\" : xavier(keys[param_sets*(lstm_layers-1) + 5], (model_size, output_size)),\n",
    "      \"bY2\" : jnp.zeros((output_size,)),\n",
    "    }\n",
    "  )\n",
    "  return params\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def dropout(dropout_key, original_tensor, dropout_rate):\n",
    "  # generate random of same shape\n",
    "  dropout_probs = random.uniform(dropout_key, shape=original_tensor.shape)\n",
    "  # mask = random < dropout_rate\n",
    "  mask = (dropout_probs > dropout_rate) / (1 - dropout_rate) # scale to keep avg the same\n",
    "  return original_tensor * mask\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[]) # static dropout rate?\n",
    "def lstm_step(step_dropout_key, lstm_layer_params, layer_h, layer_c, current_xt, dropout_rate):\n",
    "  hxconcat = jax.lax.concatenate([layer_h, current_xt], dimension=1) #B, h ++ B, C => B, h+c\n",
    "  # update gate\n",
    "  forget_gate = jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wF\"] + lstm_layer_params[\"bF\"])\n",
    "  #update = dropout(step_dropout_keys[0], update, dropout_rate)\n",
    "\n",
    "  # forget\n",
    "  layer_c = layer_c * forget_gate\n",
    "\n",
    "  input_node = jax.nn.tanh(hxconcat @ lstm_layer_params[\"wC\"] + lstm_layer_params[\"bC\"])\n",
    "  #candidate = dropout(step_dropout_keys[1], candidate, dropout_rate)\n",
    "  update = jax.nn.sigmoid(\n",
    "              hxconcat @ lstm_layer_params[\"wU\"] + lstm_layer_params[\"bU\"]\n",
    "            )\n",
    "  input_gate =  update * input_node\n",
    "\n",
    "  # update\n",
    "  layer_c = layer_c + input_gate\n",
    "\n",
    "  # output\n",
    "  layer_h = jax.nn.tanh(layer_c) * jax.nn.sigmoid(hxconcat @ lstm_layer_params[\"wO\"] + lstm_layer_params[\"bO\"]) # (B, model_size)\n",
    "\n",
    "  next_layer_xt = dropout(step_dropout_key, layer_h, dropout_rate) # the next layer's input x is the current layer's hidden state\n",
    "  # karpathy: dropout after EACH LAYER not several times in the block. lol.\n",
    "\n",
    "  # i may also need to do dropout horizontally (i.e. dropout the hidden state memory each block)\n",
    "\n",
    "  return (layer_h, layer_c), next_layer_xt\n",
    "\n",
    "\n",
    "# LSTM forward\n",
    "import functools\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate):\n",
    "  batches = xembeds_batch.shape[0]\n",
    "  lstm_layers = len(lstm_params)\n",
    "  model_size = lstm_params[0][\"h0\"].size\n",
    "  # initialize h and c as random/learnable params\n",
    "  #h = jnp.tile(lstm_params[0][\"h0\"], (batches, lstm_layers, 1)) # B, lstm_layer, h_size\n",
    "  #c = jnp.tile(lstm_params[0][\"c0\"], (batches, lstm_layers, 1)) # B, lstm_layer, c_size\n",
    "  # wait.. these are the same for all of the layers.. maybe they shouldn't be\n",
    "  T = xembeds_batch.shape[1]\n",
    "  # take xembeds_batch and pass each xt through the same SINGULAR block. don't update the weight layer. there is only one layer.\n",
    "  dropout_keys = random.split(dropout_key, lstm_layers)\n",
    "\n",
    "  # for each layer:\n",
    "    # scan over xt\n",
    "    # carry : h, c\n",
    "    # a: xt\n",
    "    # b: h,c\n",
    "    # f = lambda ((h, c), xt) : lstm_step(h, c, xt, everything else) => h, c\n",
    "    # scans over xt\n",
    "    # for next layer: xt = h of previous layer. h = h0 and c = c0\n",
    "  \n",
    "  current_embeddings_batch = jnp.transpose(xembeds_batch, (1, 0, 2)) # B, T, C => T, B, C\n",
    "    # The reason for this is that jax.lax.scan only uses the leading dim. why? idk. its dumb, it needs an axis arg so i can scan over whatever\n",
    "\n",
    "  for lstm_layer in range(lstm_layers):\n",
    "    h = jnp.tile(lstm_params[lstm_layer][\"h0\"], (batches, 1))\n",
    "    c = jnp.tile(lstm_params[lstm_layer][\"c0\"], (batches, 1))\n",
    "    # zeroes makes the backprop faster\n",
    "    #h = jnp.zeros((batches, model_size))\n",
    "    #c = jnp.zeros((batches, model_size))\n",
    "    layer_dropout_key = dropout_keys[lstm_layer] # it doesnt matter if this is the same across all layers\n",
    "    # scan should be inexpensive since layer size is small while t size is usually LARGE\n",
    "    # scan :: (c -> a -> (c, b)) -> c -> [a] -> (c, [b])\n",
    "    # scan :: scanfunc -> h_and_c -> xs -> (h_and_c_final, hs_to_be_used_as_input_xt_in_next_layer)\n",
    "    # scanfunc :: (c -> a -> (c, b))\n",
    "    scanfunc = lambda hc, xt : lstm_step(layer_dropout_key, lstm_params[lstm_layer], hc[0], hc[1], xt, dropout_rate)\n",
    "      # for xs: scan along the t dimension! it scans along B by default\n",
    "      # to fix this, we transpose xs with jnp.transpose(current_embeddings_batch, (1, 0, 2))\n",
    "    current_embeddings_batch = jax.lax.scan(scanfunc, (h, c), current_embeddings_batch)[1] # (c, [b]) => [b] ==> B, T, C\n",
    "  \n",
    "\n",
    "  # finally turn current_embeddings_batch into ys (logits)\n",
    "  hs = jnp.transpose(current_embeddings_batch, (1, 0, 2)) # T, B, C => B, T, C\n",
    "  ys = jax.nn.relu(hs @ lstm_params[-1]['wY1'] + lstm_params[-1][\"bY1\"]) # B, T, model_size => B, T, vocab_size\n",
    "  ys = ys @ lstm_params[-1]['wY2'] + lstm_params[-1][\"bY2\"]\n",
    "  return ys\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def loss_func(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logits = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[])\n",
    "def loss_and_value(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate):\n",
    "  xembeds_batch = embed(lstm_params, xtokens_batch)\n",
    "  logits = lstm_forward(dropout_key, lstm_params, xembeds_batch, dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ytokens_batch, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  predictions = jnp.argmax(logprobs, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss, predictions\n",
    "\n",
    "\n",
    "jitted_backwards_loss = jax.jit(jax.value_and_grad(loss_func, argnums=1), static_argnames=[])\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['vocab_size'])\n",
    "def embed(lstm_params, xtokens, vocab_size=len(vocab)):\n",
    "  xs_one_hot = jax.nn.one_hot(xtokens, vocab_size, axis=-1) #B, T, vocab_size\n",
    "  activations = xs_one_hot @ lstm_params[0][\"wEM\"] + lstm_params[0][\"bEM\"]\n",
    "  return activations\n",
    "\n",
    "# make optimizer a static arg in jit or it breaks\n",
    "@functools.partial(jax.jit, static_argnames=[\"optimizer\"])\n",
    "def train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer):\n",
    "  step_loss, grads = jitted_backwards_loss(dropout_key, lstm_params, xtokens_batch, ytokens_batch, dropout_rate)\n",
    "  param_updates, updated_opt_state = optimizer.update(grads, opt_state, lstm_params)\n",
    "  updated_lstm_params = optax.apply_updates(lstm_params, param_updates) \n",
    "  return updated_lstm_params, updated_opt_state, step_loss, grads\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model and training parameters\n",
    "\n",
    "# train params\n",
    "# these initialy parameters seem to train well\n",
    "lr = 0.0007\n",
    "sequence_length = 100\n",
    "dropout_rate = 0.25\n",
    "\n",
    "decay_lr = False\n",
    "decay = 0.98\n",
    "decay_epochs = 3\n",
    "\n",
    "epochs = train_epochs\n",
    "print_every = 100_000 # print progress every n steps. set extremely high to only print every epoch\n",
    "train_batch_size = 50\n",
    "val_batch_size = 50\n",
    "\n",
    "# needs to be 'true' initially\n",
    "# set to False to continue training on your existing model\n",
    "start_new_training_run = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"t  tooetooo      to to   te to e ee  toen to toooe etoetoooe    toe    toee  te toetoetoe toen  toe \"\n",
      "e:1/50 s:92/92 || samples/sec: 242 || loss: 2.7491 || val_loss: 3.1553 val_acc: 0.2019 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd tos tnthr  r  tf tn  htlyteaerei  then tn tntreretomt tete   tf r  ytoont t  tn tn the trrng toe \"\n",
      "e:2/50 s:92/92 || samples/sec: 264 || loss: 2.2967 || val_loss: 2.4468 val_acc: 0.2734 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"ld trs t shr  re tf tt  hsl teaarel  then in tttnerotowt tiye   tf r eetrowett  in t  the trrng the \"\n",
      "e:3/50 s:92/92 || samples/sec: 248 || loss: 2.0919 || val_loss: 2.1427 val_acc: 0.3197 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"ld trs t shat re tf tt  itl teaenil  thes in ttlveryto t sikee  tf r eetrobuct  in t  the srrnt ther\"\n",
      "e:4/50 s:92/92 || samples/sec: 290 || loss: 1.9363 || val_loss: 1.9477 val_acc: 0.3557 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"ld trt t shrt re if tt  itl teaenier thes is ttwaeryto t siyged tf r eetrobects in t  the mrsnt then\"\n",
      "e:5/50 s:92/92 || samples/sec: 306 || loss: 1.8210 || val_loss: 1.8004 val_acc: 0.3851 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd irt t shrt re tf tt  i l teaenber thes is ttwaeryto t songe  tf n eetrobects in t  the mrsnt then\"\n",
      "e:6/50 s:92/92 || samples/sec: 320 || loss: 1.7377 || val_loss: 1.6947 val_acc: 0.4092 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt t shrt re if tt  i l teaember ihes is ytwaenytont sotge  tfpn eetrobects it t  the mrsnt ohen\"\n",
      "e:7/50 s:92/92 || samples/sec: 285 || loss: 1.6642 || val_loss: 1.6150 val_acc: 0.4293 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i shrt re if tt  i l teaember ihes is ytwaenyto t sotge  tfpn letrobects itpt  the mrsnt ohen\"\n",
      "e:8/50 s:92/92 || samples/sec: 302 || loss: 1.6052 || val_loss: 1.5549 val_acc: 0.4464 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i shat re if tt  i l teaember ihes is ytwvenyto t sotge  tfpniletrobects itpt  the mrsnt ohen\"\n",
      "e:9/50 s:92/92 || samples/sec: 279 || loss: 1.5667 || val_loss: 1.5102 val_acc: 0.4610 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i shat re tf tt  i l teaember ihes is ytwaenyto t sotge  tftn l trogects if t  the srsnt ohen\"\n",
      "e:10/50 s:92/92 || samples/sec: 228 || loss: 1.5317 || val_loss: 1.4764 val_acc: 0.4736 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd irt i lhat re tf tt  i l teaember ihes is ytwvenyto t lorge  tfenil trobects itps  the srsnt oier\"\n",
      "e:11/50 s:92/92 || samples/sec: 246 || loss: 1.4897 || val_loss: 1.4482 val_acc: 0.4847 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd irt i lhat re tf tt  i l teaember ihes is ytwaenyto t lorge  tftnil trobects itps  the sasnt ohen\"\n",
      "e:12/50 s:92/92 || samples/sec: 284 || loss: 1.4709 || val_loss: 1.4263 val_acc: 0.4944 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd irt i lhat re tf tt  i l beaember thes is ytwnenyto i lorge  tfenil trobects itps  the srsnt ohen\"\n",
      "e:13/50 s:92/92 || samples/sec: 284 || loss: 1.4278 || val_loss: 1.4094 val_acc: 0.5029 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i lhat re tf tt  i l beaember thes is ytwxenyto t lorge  tfenil trobects it s  the srsnt oier\"\n",
      "e:14/50 s:92/92 || samples/sec: 264 || loss: 1.4135 || val_loss: 1.3961 val_acc: 0.5106 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i lhat re tf tt  i l peaember thes is ytwvenyto i lorge  tftnil trobects it s  the srsnt oier\"\n",
      "e:15/50 s:92/92 || samples/sec: 243 || loss: 1.3720 || val_loss: 1.3849 val_acc: 0.5174 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i lhat re tf tt  itl beaember thes is ytwnenyso i lorge  tfen l irogects it s  the srsnt oher\"\n",
      "e:16/50 s:92/92 || samples/sec: 262 || loss: 1.3343 || val_loss: 1.3758 val_acc: 0.5236 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loat re tf tt  i l peaember thes is y wxen to i sorge  tftnil trogects it s  the sasnt oher\"\n",
      "e:17/50 s:92/92 || samples/sec: 260 || loss: 1.3294 || val_loss: 1.3692 val_acc: 0.5291 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loat re tf tt  i l peaember thes is ytwnen to i lorger afenil irogects it s  the srsnt oher\"\n",
      "e:18/50 s:92/92 || samples/sec: 260 || loss: 1.3044 || val_loss: 1.3650 val_acc: 0.5342 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loat re tf tt  i l peaember thes is ytwxen mo i lorger afpn l irogects it g  the srsnt iher\"\n",
      "e:19/50 s:92/92 || samples/sec: 259 || loss: 1.2923 || val_loss: 1.3621 val_acc: 0.5388 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd irt i loxt re tf tt🛑 i l peaember thes is y wxen go i lorger afpngl irogects it s  ahe srsnt iher\"\n",
      "e:20/50 s:92/92 || samples/sec: 260 || loss: 1.2764 || val_loss: 1.3606 val_acc: 0.5429 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxt re tu tt🛑 i l peaember thes is y cxen wo i lorger anpngl irogect  it s  the sasnt iher\"\n",
      "e:21/50 s:92/92 || samples/sec: 261 || loss: 1.2540 || val_loss: 1.3607 val_acc: 0.5467 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxt re iu tt🛑 s l beaember thes is y cxen po i lorger afpn l irobect  it s  the srsnt iher\"\n",
      "e:22/50 s:92/92 || samples/sec: 258 || loss: 1.2444 || val_loss: 1.3622 val_acc: 0.5502 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt t loxt re tu tt🛑 i l puaember thes is y cnen so i lorger afengl iroject  it a  the srsnt wher\"\n",
      "e:23/50 s:92/92 || samples/sec: 258 || loss: 1.2004 || val_loss: 1.3656 val_acc: 0.5534 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i lhxt re tn tt🛑 i l peaember thes is y cven wo i lorger pnengl irobects it a  lhe sasnt wher\"\n",
      "e:24/50 s:92/92 || samples/sec: 259 || loss: 1.1754 || val_loss: 1.3696 val_acc: 0.5564 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt t loxt re tu tt🛑 i l pemember thes is ytcven wo i lorger pfengl iroject  it n  the sasnt wher\"\n",
      "e:25/50 s:92/92 || samples/sec: 259 || loss: 1.1551 || val_loss: 1.3746 val_acc: 0.5591 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxt re an tt🛑 inl bemember thes is y cven wo t lorger afengl projects it a  the sasnt wher\"\n",
      "e:26/50 s:92/92 || samples/sec: 263 || loss: 1.1398 || val_loss: 1.3826 val_acc: 0.5616 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxt re an tt🛑 inl pemember thes is y cver mo i lorger afengl irojects it s  the sasnt wher\"\n",
      "e:27/50 s:92/92 || samples/sec: 255 || loss: 1.1309 || val_loss: 1.3915 val_acc: 0.5639 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxt re au tt🛑 inl bemember thes is ytcxen wo t lorger afengl projects it s  the srsnt wher\"\n",
      "e:28/50 s:92/92 || samples/sec: 284 || loss: 1.1012 || val_loss: 1.3981 val_acc: 0.5661 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxt re on tt🛑 inl bemember thes is y cven wo i lotger afengl iroject  if a  the srsnt wher\"\n",
      "e:29/50 s:92/92 || samples/sec: 285 || loss: 1.0843 || val_loss: 1.4091 val_acc: 0.5680 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxture on tt🛑 i l demember thes is y cxer to i lorger pnengl projects if n  the srint wher\"\n",
      "e:30/50 s:92/92 || samples/sec: 325 || loss: 1.0769 || val_loss: 1.4191 val_acc: 0.5698 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i lhxture on tt🛑 inl bemember thes is y cver wo i lorger pnengl projects im nl the srsnt wher\"\n",
      "e:31/50 s:92/92 || samples/sec: 274 || loss: 1.0523 || val_loss: 1.4283 val_acc: 0.5715 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxture on tt🛑 snl bemember thes is y cver wo i lorger pnengl iroject  st a  the srsnt wher\"\n",
      "e:32/50 s:92/92 || samples/sec: 326 || loss: 1.0451 || val_loss: 1.4389 val_acc: 0.5730 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i lixture on tt🛑 i l demember thes is y cxer wo i lorger pnengl project  im al the soint wher\"\n",
      "e:33/50 s:92/92 || samples/sec: 274 || loss: 1.0236 || val_loss: 1.4514 val_acc: 0.5745 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i lixture on tt🛑 i l demember thes is y cven wo i lorger anengl irojects im al the mrsnt wher\"\n",
      "e:34/50 s:92/92 || samples/sec: 328 || loss: 0.9958 || val_loss: 1.4632 val_acc: 0.5758 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxture on tt🛑 inl demember thes in y cxen do i lorger pnengl pooject  im al the srint wher\"\n",
      "e:35/50 s:92/92 || samples/sec: 317 || loss: 0.9794 || val_loss: 1.4745 val_acc: 0.5770 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt illoxture on tt🛑 inl demember thes in y cxer wo t lorger anengl ioojects sm al the srint wher\"\n",
      "e:36/50 s:92/92 || samples/sec: 350 || loss: 0.9782 || val_loss: 1.4878 val_acc: 0.5782 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt illoxture on tt🛑 inl demember thes is y cver do i lotger anengl iroject  sm al ahe srint wher\"\n",
      "e:37/50 s:92/92 || samples/sec: 335 || loss: 0.9714 || val_loss: 1.5001 val_acc: 0.5793 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxture on tt🛑 inl demember thes if y cxer mo i lorger onengl iroject, im sl ahe srint wher\"\n",
      "e:38/50 s:92/92 || samples/sec: 270 || loss: 0.9354 || val_loss: 1.5120 val_acc: 0.5802 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt illoxture on tt🛑 ivl demember thes ff y cver wo i lorger onengl iooject, im al the srint wher\"\n",
      "e:39/50 s:92/92 || samples/sec: 315 || loss: 0.9403 || val_loss: 1.5265 val_acc: 0.5812 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxture on tt🛑 ivl demember thes if y cver mo t lorger pnengl iooject  im al the srint wher\"\n",
      "e:40/50 s:92/92 || samples/sec: 324 || loss: 0.9391 || val_loss: 1.5385 val_acc: 0.5821 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxture on tt🛑 Snl demember this if y cver wo i lorger pnengl iroject, im ac the srint wher\"\n",
      "e:41/50 s:92/92 || samples/sec: 303 || loss: 0.9110 || val_loss: 1.5544 val_acc: 0.5829 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i loxture on tt🛑 Sll demember this if y cver do i lorger pnengl iooject, sm ac ahe mrint wher\"\n",
      "e:42/50 s:92/92 || samples/sec: 313 || loss: 0.9160 || val_loss: 1.5676 val_acc: 0.5836 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt illexture on tt🛑 ttl demember this if y cver do t lorger ppengl pooject, im ac the srint wher\"\n",
      "e:43/50 s:92/92 || samples/sec: 280 || loss: 0.8817 || val_loss: 1.5819 val_acc: 0.5843 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt illhxture on tt  tll demember thes if y cver do t lorger pnengl gooject, im ac the srint wher\"\n",
      "e:44/50 s:92/92 || samples/sec: 254 || loss: 0.8614 || val_loss: 1.5950 val_acc: 0.5849 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt illexture on tt  Sll demember this is y cver do i lorger ppengl iooject, im gc the srint wher\"\n",
      "e:45/50 s:92/92 || samples/sec: 303 || loss: 0.8503 || val_loss: 1.6089 val_acc: 0.5855 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i lhxture on mt  ivl demember this if y cver do t lorger ppengl groject, im ac the srint wher\"\n",
      "e:46/50 s:92/92 || samples/sec: 293 || loss: 0.8447 || val_loss: 1.6236 val_acc: 0.5861 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd trt i lhxture on mt  anl demember this in y cver do t lorger ppengl iroject, im al the foint wher\"\n",
      "e:47/50 s:92/92 || samples/sec: 311 || loss: 0.8339 || val_loss: 1.6367 val_acc: 0.5866 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd yut i lexture wn tt  all demember this is y cver do i lorger ppengl probect  im al ahe soint wher\"\n",
      "e:48/50 s:92/92 || samples/sec: 293 || loss: 0.8246 || val_loss: 1.6505 val_acc: 0.5871 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd yut i lhxture on tt🛑 all demember this if y cver do t lorger ppengl sroject, im al the frint wher\"\n",
      "e:49/50 s:92/92 || samples/sec: 309 || loss: 0.8199 || val_loss: 1.6602 val_acc: 0.5875 || LR = 0.000700\n",
      "TARGET | \"nd put a texture on it  ill remember this if i ever do a larger opengl project, im at the point wher\"\n",
      "PRED   | \"nd yut i thxture on tt  ell demember this if y cver do i lorger ppengl gaoject, imaal the frint wher\"\n",
      "e:50/50 s:92/92 || samples/sec: 319 || loss: 0.8059 || val_loss: 1.6753 val_acc: 0.5879 || LR = 0.000700\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "\n",
    "# init some variables\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# init optimizer and network state\n",
    "if start_new_training_run:\n",
    "  optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "  lstm_params = init_LSTM_params(keys[0], lstm_layers, input_size, model_size, output_size)\n",
    "  opt_state = optimizer.init(lstm_params)\n",
    "else:\n",
    "  opt_state.hyperparams['learning_rate'] = lr\n",
    "\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    if decay_lr and epoch != 0 and epoch % decay_epochs == 0:\n",
    "       opt_state.hyperparams['learning_rate'] = opt_state.hyperparams['learning_rate'] * decay\n",
    "\n",
    "    # train\n",
    "    steps = (len(train_tokens) // ((sequence_length+1)*train_batch_size)) - 2\n",
    "    for step in range(steps): # probably wrong but w/e\n",
    "      # B, T where T = sequence_length\n",
    "      train_data_idx = step*sequence_length*train_batch_size\n",
    "      next_train_data_idx = (step+1)*sequence_length*train_batch_size\n",
    "      xtokens_batch = train_tokens[train_data_idx:next_train_data_idx].reshape(-1, sequence_length) #(B, T)\n",
    "      ytokens_batch = train_tokens[train_data_idx+1:next_train_data_idx+1].reshape(-1, sequence_length) # (B,)\n",
    "\n",
    "      dropout_key = random.PRNGKey(epoch*steps + step) # unique for every step\n",
    "      lstm_params, opt_state, step_loss, grads = train(dropout_key, lstm_params, xtokens_batch, ytokens_batch, opt_state, dropout_rate, optimizer)\n",
    "\n",
    "      losses.append(step_loss)\n",
    "\n",
    "      # val\n",
    "      j = step % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "      val_idx = j*val_batch_size*sequence_length\n",
    "      next_val_idx = (j+1)*val_batch_size*sequence_length\n",
    "      xtokens_val_batch = test_tokens[val_idx:next_val_idx].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "      ytokens_val_batch = test_tokens[val_idx+1:next_val_idx+1].reshape(-1, sequence_length)\n",
    "      \n",
    "      val_loss, prediction_val_batch = loss_and_value(dropout_key, lstm_params, xtokens_val_batch, ytokens_val_batch, dropout_rate=0)\n",
    "      val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "      val_losses.append(val_loss)\n",
    "      val_accuracies.append(val_accuracy)\n",
    "\n",
    "      if (step == steps - 1):\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        # train inference example (no dropout)\n",
    "        xembeds_batch = embed(lstm_params, xtokens_batch[0][None, :]) # 1-batch - (1, T, C)\n",
    "        last_logit_batch = lstm_forward(dropout_key, lstm_params, xembeds_batch, 0) # B, C\n",
    "        prediction_batch = jnp.argmax(last_logit_batch, axis=-1) # B\n",
    "\n",
    "        # print train status\n",
    "        x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "        y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "        yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "        #print(f'INPUT  ({len(x)}) | \"{x}\"')\n",
    "        avg_loss = sum(losses)/len(losses)\n",
    "        avg_val_loss = sum(val_losses)/len(val_losses)\n",
    "        avg_val_acc = sum(val_accuracies)/len(val_accuracies)\n",
    "        lines = [\n",
    "          f'TARGET | \"{y}\"',\n",
    "          f'PRED   | \"{yhat}\"',\n",
    "          f\"e:{epoch+1}/{epochs} s:{step+1}/{steps} || samples/sec: {train_batch_size*steps/(duration):0.0f} || \"\n",
    "          f\"loss: {step_loss:1.4f} || val_loss: {avg_val_loss:1.4f} val_acc: {avg_val_acc:1.4f} || \" \n",
    "          f\"LR = {opt_state.hyperparams['learning_rate']:0.6f}\",\n",
    "        ]\n",
    "        print(\"\\n\".join(lines))\n",
    "        start = time.time()\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference settings\n",
    "# the first time you run the model, inference will be slow\n",
    "\n",
    "temperature = 1   # from 0 to 2. 1 is normal.\n",
    "\n",
    "reply_prompt = \"reply: \"\n",
    "post_prompt = \"post: \"\n",
    "\n",
    "prompt = post_prompt\n",
    "\n",
    "# how many characters until the line wraps around\n",
    "formatting_line_length = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post: 10, so far)\n",
      "\n",
      "yeah i exploding download the \n",
      "context and look it takes data w a contack and sto\n",
      "ring how it works suuuuper well\n",
      "\n",
      "searching for cla\n",
      "ssic psychological architecture:\n",
      "https://t.co/wdQC\n",
      "5NrLED🛑"
     ]
    }
   ],
   "source": [
    "# run the model!\n",
    "\n",
    "def inference(key, chars, temperature):\n",
    "  xtokens = encode(chars)[None, :]\n",
    "  xembed = embed(lstm_params, xtokens) # artificial single batch\n",
    "  logits = lstm_forward(key, lstm_params, xembed, 0)[0][-1] # logits of the first B and last T in the B T C. should be (C,)\n",
    "  probs = jax.nn.softmax(logits/(temperature + 0.001))\n",
    "  yhattokens = random.choice(key, a=logits.shape[0], p=probs) # no need for axis=-1 since logits are (C,)\n",
    "  return yhattokens\n",
    "\n",
    "\n",
    "steps = 1000\n",
    "import time\n",
    "seed = int(1000*time.time())\n",
    "keys = random.split(random.PRNGKey(seed), steps)\n",
    "text =  \" \"*50 + prompt\n",
    "print(text.replace('  ', ''), end='')\n",
    "for i in range(steps):\n",
    "  next_token = inference(keys[i], text[-sequence_length:], temperature)\n",
    "  next_char = decode([next_token])[-1]\n",
    "  if next_char == '🛑':\n",
    "    print(next_char, end='')\n",
    "    break\n",
    "  text += next_char\n",
    "  if (len(text) - 50) % formatting_line_length == 0:\n",
    "    print()\n",
    "  print(next_char, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
