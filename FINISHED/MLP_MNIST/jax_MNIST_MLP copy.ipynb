{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o71b_stSBdOt"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "import jax\n",
        "import optax\n",
        "import torch\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "jax.config.update(\"jax_debug_infs\", True)\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "#jax.disable_jit(disable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZVSVlAzesCU7",
        "outputId": "f75a5b00-3fb7-4ec2-8f6d-0078b836405b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([1.], dtype=float64)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check if GPU is working\n",
        "jax.default_backend()\n",
        "jax.device_put(jax.numpy.ones(1), device=jax.devices('gpu')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RaRSziO_K2z2"
      },
      "outputs": [],
      "source": [
        "# set up params\n",
        "batch_size = 4\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# first load the dataset\n",
        "train_data = torchvision.datasets.MNIST(root = './', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_data = torchvision.datasets.MNIST(root = './', train=False, download=True, transform=transforms.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3g5pPllZiydI"
      },
      "outputs": [],
      "source": [
        "# convert to jnp/np\n",
        "x_train, y_train = zip(*train_data)\n",
        "x_train, y_train = jnp.array(x_train)[:40], jnp.array(y_train)[:40]\n",
        "\n",
        "x_test, y_test = zip(*test_data)\n",
        "x_test, y_test = jnp.array(x_test), jnp.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dQAnlea1j3uu"
      },
      "outputs": [],
      "source": [
        "# flatten each x\n",
        "x_train = jnp.array([jnp.ravel(x) for x in x_train])\n",
        "x_test = jnp.array([jnp.ravel(x) for x in x_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxA0Ir2xi1FA",
        "outputId": "2c9b3b00-5892-416a-83dd-d7e3baa3aeea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        }
      ],
      "source": [
        "# convert ys to one-hot\n",
        "classes = len(set(y_train.tolist()))\n",
        "print(classes)\n",
        "y_train = jax.nn.one_hot(y_train, classes) # from n -> one-hot of n\n",
        "y_test = jax.nn.one_hot(y_test, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_YRTUPowslNm"
      },
      "outputs": [],
      "source": [
        "#jax.device_put(x_train, device=jax.devices('gpu')[0])\n",
        "#jax.device_put(y_train, device=jax.devices('gpu')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc14obpVIcWF",
        "outputId": "7fb72146-5901-4447-f3eb-2a26e64000da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784,) [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "(784,) [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "(784,) [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Array(0.13768007, dtype=float32)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train_data[idx][0] => x   (1, 28, 28)\n",
        "# train_data[idx][1] => y   int\n",
        "for idx in range(10):\n",
        "  print(x_train[idx].shape, y_train[idx])\n",
        "  # print(x_train[idx][0][14], train_data[idx][0][0][14])\n",
        "\n",
        "jnp.mean(jnp.array(x_train[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AikDkgdRFJff",
        "outputId": "a561d49b-4487-4f82-84ef-62ebb3c9feb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, batch 0, loss=9.21034037179805\n",
            "epoch 0, batch 6, loss=9.201590694414419\n",
            "epoch 1, batch 2, loss=9.200388543379937\n",
            "epoch 1, batch 7, loss=9.213365023069759\n",
            "epoch 2, batch 2, loss=9.195364092740697\n",
            "epoch 2, batch 7, loss=9.209113784598557\n",
            "epoch 3, batch 3, loss=9.189825993957939\n",
            "epoch 3, batch 9, loss=9.2222350834381\n",
            "epoch 4, batch 4, loss=9.246703906830174\n",
            "epoch 4, batch 9, loss=9.222059275146552\n",
            "epoch 5, batch 5, loss=9.18025445698861\n",
            "epoch 6, batch 0, loss=9.153390519550298\n",
            "epoch 6, batch 5, loss=9.172164645898151\n",
            "epoch 7, batch 0, loss=9.142362799220141\n",
            "epoch 7, batch 5, loss=9.163105322862503\n",
            "epoch 8, batch 0, loss=9.128399662148325\n",
            "epoch 8, batch 5, loss=9.153061038150408\n",
            "epoch 9, batch 0, loss=9.111605202597573\n",
            "epoch 9, batch 5, loss=9.141821583320704\n",
            "epoch 10, batch 0, loss=9.09167625390793\n",
            "epoch 10, batch 5, loss=9.128780780383003\n",
            "epoch 11, batch 0, loss=9.067599086081284\n",
            "epoch 11, batch 5, loss=9.112913807656906\n",
            "epoch 12, batch 0, loss=9.037443442394153\n",
            "epoch 12, batch 6, loss=9.190835625033802\n",
            "epoch 13, batch 2, loss=9.17508663771972\n",
            "epoch 13, batch 8, loss=8.852652066512789\n",
            "epoch 14, batch 3, loss=9.03636024357292\n",
            "epoch 14, batch 9, loss=9.181018192977554\n",
            "epoch 15, batch 4, loss=9.651538266797845\n",
            "epoch 15, batch 9, loss=9.182683717775603\n",
            "epoch 16, batch 4, loss=10.379636186926533\n",
            "epoch 16, batch 9, loss=9.241572643285435\n",
            "epoch 17, batch 5, loss=8.257657812717534\n",
            "epoch 18, batch 1, loss=9.094351007525916\n",
            "epoch 18, batch 6, loss=8.142593730253303\n",
            "epoch 19, batch 1, loss=8.592868164482717\n",
            "epoch 19, batch 6, loss=7.86047468340752\n",
            "epoch 20, batch 1, loss=8.371563619587917\n",
            "epoch 20, batch 7, loss=7.663079397999059\n",
            "epoch 21, batch 3, loss=7.6140526913155\n",
            "epoch 21, batch 8, loss=5.800217146411441\n",
            "epoch 22, batch 3, loss=7.647091735734731\n",
            "epoch 22, batch 9, loss=7.6875662952115675\n",
            "epoch 23, batch 5, loss=7.573604324014651\n",
            "epoch 24, batch 0, loss=6.056654097841652\n",
            "epoch 24, batch 5, loss=7.596857861412327\n",
            "epoch 25, batch 0, loss=6.047900802211504\n",
            "epoch 25, batch 5, loss=7.578014751829445\n",
            "epoch 26, batch 0, loss=6.039054138657358\n",
            "epoch 26, batch 6, loss=7.691232529190335\n",
            "epoch 27, batch 1, loss=7.688589740983572\n",
            "epoch 27, batch 6, loss=7.705024116289778\n",
            "epoch 28, batch 1, loss=7.702271596663557\n",
            "epoch 28, batch 6, loss=7.718509283961287\n",
            "epoch 29, batch 2, loss=7.4102663303331635\n",
            "epoch 29, batch 7, loss=7.4171766455531785\n",
            "epoch 30, batch 2, loss=7.352823299600425\n",
            "epoch 30, batch 7, loss=7.3528155349526365\n",
            "epoch 31, batch 3, loss=7.176896594120841\n",
            "epoch 31, batch 8, loss=5.719776356969479\n",
            "epoch 32, batch 3, loss=7.003849406280741\n",
            "epoch 32, batch 8, loss=5.617554854710922\n",
            "epoch 33, batch 3, loss=6.941065633304584\n",
            "epoch 33, batch 8, loss=5.574813291277974\n",
            "epoch 34, batch 4, loss=10.772239340408236\n",
            "epoch 35, batch 0, loss=5.993662845389993\n",
            "epoch 35, batch 6, loss=7.663157519303057\n",
            "epoch 36, batch 1, loss=7.796711332192505\n",
            "epoch 36, batch 7, loss=7.1338843538145\n",
            "epoch 37, batch 3, loss=6.933875347319164\n",
            "epoch 37, batch 9, loss=7.252539320301032\n",
            "epoch 38, batch 5, loss=7.633283112741852\n",
            "epoch 39, batch 1, loss=7.83864519759212\n",
            "epoch 39, batch 7, loss=6.919621612602221\n",
            "epoch 40, batch 3, loss=6.807100276969065\n",
            "epoch 40, batch 9, loss=7.054906747231055\n",
            "epoch 41, batch 5, loss=8.03117072513725\n",
            "epoch 42, batch 1, loss=7.849481100074938\n",
            "epoch 42, batch 7, loss=7.0371433749742325\n",
            "epoch 43, batch 3, loss=6.808696209212592\n",
            "epoch 43, batch 9, loss=7.02044710198646\n",
            "epoch 44, batch 5, loss=7.640624222194902\n",
            "epoch 45, batch 1, loss=7.805172237882651\n",
            "epoch 45, batch 7, loss=6.930308109173483\n",
            "epoch 46, batch 3, loss=6.775256495833737\n",
            "epoch 46, batch 9, loss=6.957438585438357\n",
            "epoch 47, batch 5, loss=7.545052975039851\n",
            "epoch 48, batch 1, loss=7.724265761703285\n",
            "epoch 48, batch 7, loss=6.788229380236184\n",
            "epoch 49, batch 2, loss=6.719413670266313\n",
            "epoch 49, batch 7, loss=6.775824141745533\n",
            "epoch 50, batch 2, loss=6.705388780918008\n",
            "epoch 50, batch 8, loss=5.118877854530587\n",
            "epoch 51, batch 3, loss=6.702751568888024\n",
            "epoch 51, batch 9, loss=6.949391851421969\n",
            "epoch 52, batch 5, loss=7.27903205649838\n",
            "epoch 53, batch 1, loss=7.541342760571679\n",
            "epoch 53, batch 7, loss=6.712305431896214\n",
            "epoch 54, batch 3, loss=6.668557448003164\n",
            "epoch 54, batch 9, loss=6.95150334446803\n",
            "epoch 55, batch 5, loss=7.1810310629520195\n",
            "epoch 56, batch 1, loss=7.469888047081105\n",
            "Invalid nan value encountered in the output of a C++-jit/pmap function. Calling the de-optimized version.\n"
          ]
        },
        {
          "ename": "FloatingPointError",
          "evalue": "invalid value (inf) encountered in jit(true_divide). Because jax_config.debug_nans.value and/or config.jax_debug_infs is set, the de-optimized function (i.e., the function as if the `jit` decorator were removed) was called in an attempt to get a more precise error message. However, the de-optimized function did not produce invalid values during its execution. This behavior can result from `jit` optimizations causing the invalid value to be produced. It may also arise from having nan/inf constants as outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. \n\nIt may be possible to avoid the invalid value by removing the `jit` decorator, at the cost of losing optimizations. \n\nIf you see this error, consider opening a bug report at https://github.com/jax-ml/jax.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFloatingPointError\u001b[0m                        Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py:327\u001b[0m, in \u001b[0;36mcheck_special\u001b[0;34m(name, bufs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m buf \u001b[38;5;129;01min\u001b[39;00m bufs:\n\u001b[0;32m--> 327\u001b[0m   \u001b[43m_check_special\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py:334\u001b[0m, in \u001b[0;36m_check_special\u001b[0;34m(name, dtype, buf)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdebug_infs\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misinf(np\u001b[38;5;241m.\u001b[39masarray(buf))):\n\u001b[0;32m--> 334\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFloatingPointError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid value (inf) encountered in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mFloatingPointError\u001b[0m: invalid value (inf) encountered in pjit",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFloatingPointError\u001b[0m                        Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/profiler.py:333\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1292\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arrays \u001b[38;5;129;01min\u001b[39;00m out_arrays:\n\u001b[0;32m-> 1292\u001b[0m   \u001b[43mdispatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_special\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_handler(out_arrays)\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py:327\u001b[0m, in \u001b[0;36mcheck_special\u001b[0;34m(name, bufs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m buf \u001b[38;5;129;01min\u001b[39;00m bufs:\n\u001b[0;32m--> 327\u001b[0m   \u001b[43m_check_special\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py:334\u001b[0m, in \u001b[0;36m_check_special\u001b[0;34m(name, dtype, buf)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdebug_infs\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misinf(np\u001b[38;5;241m.\u001b[39masarray(buf))):\n\u001b[0;32m--> 334\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFloatingPointError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid value (inf) encountered in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mFloatingPointError\u001b[0m: invalid value (inf) encountered in jit(true_divide)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mJaxStackTraceBeforeTransformation\u001b[0m         Traceback (most recent call last)",
            "File \u001b[0;32m/usr/lib/python3.10/runpy.py:196\u001b[0m, in \u001b[0;36m_run_module_as_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m mod_spec\u001b[38;5;241m.\u001b[39morigin\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _run_code(code, main_globals, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod_spec)\n",
            "File \u001b[0;32m/usr/lib/python3.10/runpy.py:86\u001b[0m, in \u001b[0;36m_run_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m run_globals\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m mod_name,\n\u001b[1;32m     80\u001b[0m                    \u001b[38;5;18m__file__\u001b[39m \u001b[38;5;241m=\u001b[39m fname,\n\u001b[1;32m     81\u001b[0m                    __cached__ \u001b[38;5;241m=\u001b[39m cached,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m                    __package__ \u001b[38;5;241m=\u001b[39m pkg_name,\n\u001b[1;32m     85\u001b[0m                    __spec__ \u001b[38;5;241m=\u001b[39m mod_spec)\n\u001b[0;32m---> 86\u001b[0m exec(code, run_globals)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_globals\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/ipykernel_launcher.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipykernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kernelapp \u001b[38;5;28;01mas\u001b[39;00m app\n\u001b[0;32m---> 18\u001b[0m app\u001b[38;5;241m.\u001b[39mlaunch_new_instance()\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/traitlets/config/application.py:1075\u001b[0m, in \u001b[0;36mlaunch_instance\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1074\u001b[0m app\u001b[38;5;241m.\u001b[39minitialize(argv)\n\u001b[0;32m-> 1075\u001b[0m app\u001b[38;5;241m.\u001b[39mstart()\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py:739\u001b[0m, in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_loop\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py:205\u001b[0m, in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masyncio_loop\u001b[38;5;241m.\u001b[39mrun_forever()\n",
            "File \u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py:603\u001b[0m, in \u001b[0;36mrun_forever\u001b[0;34m()\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n",
            "File \u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py:1909\u001b[0m, in \u001b[0;36m_run_once\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1908\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1909\u001b[0m         handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[1;32m   1910\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/asyncio/events.py:80\u001b[0m, in \u001b[0;36m_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:545\u001b[0m, in \u001b[0;36mdispatch_queue\u001b[0;34m()\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_one()\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:534\u001b[0m, in \u001b[0;36mprocess_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m dispatch(\u001b[38;5;241m*\u001b[39margs)\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:437\u001b[0m, in \u001b[0;36mdispatch_shell\u001b[0;34m()\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m--> 437\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m result\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py:362\u001b[0m, in \u001b[0;36mexecute_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_associate_new_top_level_threads_with(parent_header)\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mexecute_request(stream, ident, parent)\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:778\u001b[0m, in \u001b[0;36mexecute_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(reply_content):\n\u001b[0;32m--> 778\u001b[0m     reply_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m reply_content\n\u001b[1;32m    780\u001b[0m \u001b[38;5;66;03m# Flush output before sending the reply.\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py:449\u001b[0m, in \u001b[0;36mdo_execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 449\u001b[0m     res \u001b[38;5;241m=\u001b[39m shell\u001b[38;5;241m.\u001b[39mrun_cell(\n\u001b[1;32m    450\u001b[0m         code,\n\u001b[1;32m    451\u001b[0m         store_history\u001b[38;5;241m=\u001b[39mstore_history,\n\u001b[1;32m    452\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[1;32m    453\u001b[0m         cell_id\u001b[38;5;241m=\u001b[39mcell_id,\n\u001b[1;32m    454\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py:549\u001b[0m, in \u001b[0;36mrun_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrun_cell(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3075\u001b[0m, in \u001b[0;36mrun_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3075\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_cell(\n\u001b[1;32m   3076\u001b[0m         raw_cell, store_history, silent, shell_futures, cell_id\n\u001b[1;32m   3077\u001b[0m     )\n\u001b[1;32m   3078\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3130\u001b[0m, in \u001b[0;36m_run_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3130\u001b[0m     result \u001b[38;5;241m=\u001b[39m runner(coro)\n\u001b[1;32m   3131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334\u001b[0m, in \u001b[0;36mrun_cell_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3331\u001b[0m interactivity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mast_node_interactivity\n\u001b[0;32m-> 3334\u001b[0m has_raised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_ast_nodes(code_ast\u001b[38;5;241m.\u001b[39mbody, cell_name,\n\u001b[1;32m   3335\u001b[0m        interactivity\u001b[38;5;241m=\u001b[39minteractivity, compiler\u001b[38;5;241m=\u001b[39mcompiler, result\u001b[38;5;241m=\u001b[39mresult)\n\u001b[1;32m   3337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_execution_succeeded \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m has_raised\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517\u001b[0m, in \u001b[0;36mrun_ast_nodes\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3516\u001b[0m     asy \u001b[38;5;241m=\u001b[39m compare(code)\n\u001b[0;32m-> 3517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_code(code, result, async_\u001b[38;5;241m=\u001b[39masy):\n\u001b[1;32m   3518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m, in \u001b[0;36mrun_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3577\u001b[0m         exec(code_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_global_ns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3579\u001b[0m     \u001b[38;5;66;03m# Reset our crash handler in place\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[9], line 128\u001b[0m\n\u001b[1;32m    126\u001b[0m x_batch, y_batch \u001b[38;5;241m=\u001b[39m x_train[batch_indices], y_train[batch_indices]\n\u001b[0;32m--> 128\u001b[0m params, opt_state, losses, norms \u001b[38;5;241m=\u001b[39m train_step(params, x_batch, y_batch, optimizer, opt_state)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m last_log_time \u001b[38;5;241m>\u001b[39m time_between_logs:\n",
            "Cell \u001b[0;32mIn[9], line 98\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m losses \u001b[38;5;241m=\u001b[39m get_loss(params, x_batch, y_batch)\n\u001b[0;32m---> 98\u001b[0m grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(get_loss)(params, x_batch, y_batch)\n\u001b[1;32m     99\u001b[0m updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state)\n",
            "Cell \u001b[0;32mIn[9], line 78\u001b[0m, in \u001b[0;36mget_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# the reason for using jax.scipy.special.xlogy instead of\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# -jnp.log(y_pred_batch) * y_batch   is that it accounts for 0 in the\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# prediction batch. otherwise, 0 produces -inf and breaks the training\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m presum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mjax\u001b[38;5;241m.\u001b[39mscipy\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mxlogy(y_batch, y_pred_batch)\n\u001b[1;32m     79\u001b[0m crossentropyloss \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msum(presum)\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/scipy/special.py:504\u001b[0m, in \u001b[0;36m_xlogy_jvp\u001b[0;34m()\u001b[0m\n\u001b[1;32m    503\u001b[0m result \u001b[38;5;241m=\u001b[39m xlogy(x, y)\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, (x_dot \u001b[38;5;241m*\u001b[39m lax\u001b[38;5;241m.\u001b[39mlog(y) \u001b[38;5;241m+\u001b[39m y_dot \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m/\u001b[39m y)\u001b[38;5;241m.\u001b[39mastype(result\u001b[38;5;241m.\u001b[39mdtype)\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:1050\u001b[0m, in \u001b[0;36mop\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1050\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maval, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36mdeferring_binary_op\u001b[0;34m()\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m binary_op(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n",
            "\u001b[0;31mJaxStackTraceBeforeTransformation\u001b[0m: FloatingPointError: invalid value (inf) encountered in jit(true_divide). Because jax_config.debug_nans.value and/or config.jax_debug_infs is set, the de-optimized function (i.e., the function as if the `jit` decorator were removed) was called in an attempt to get a more precise error message. However, the de-optimized function did not produce invalid values during its execution. This behavior can result from `jit` optimizations causing the invalid value to be produced. It may also arise from having nan/inf constants as outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. \n\nIt may be possible to avoid the invalid value by removing the `jit` decorator, at the cost of losing optimizations. \n\nIf you see this error, consider opening a bug report at https://github.com/jax-ml/jax.\n\nThe preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mFloatingPointError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m indices[batch_start:batch_end]\n\u001b[1;32m    126\u001b[0m x_batch, y_batch \u001b[38;5;241m=\u001b[39m x_train[batch_indices], y_train[batch_indices]\n\u001b[0;32m--> 128\u001b[0m params, opt_state, losses, norms \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m last_log_time \u001b[38;5;241m>\u001b[39m time_between_logs:\n\u001b[1;32m    131\u001b[0m   last_log_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "Cell \u001b[0;32mIn[9], line 98\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(params, x_batch, y_batch, optimizer, opt_state)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(params, x_batch, y_batch, optimizer, opt_state):\n\u001b[1;32m     97\u001b[0m   losses \u001b[38;5;241m=\u001b[39m get_loss(params, x_batch, y_batch)\n\u001b[0;32m---> 98\u001b[0m   grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m   updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state)\n\u001b[1;32m    100\u001b[0m   params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
            "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1692\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;66;03m# If control reaches this line, we got a NaN on the output of `compiled`\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;66;03m# but not `fun.call_wrapped` on the same arguments. Let's tell the user.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1678\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax_config.debug_nans.value and/or config.jax_debug_infs is set, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1679\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde-optimized function (i.e., the function as if the `jit` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1690\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you see this error, consider opening a bug report at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1691\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/jax-ml/jax.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1692\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFloatingPointError\u001b[39;00m(msg)\n",
            "\u001b[0;31mFloatingPointError\u001b[0m: invalid value (inf) encountered in jit(true_divide). Because jax_config.debug_nans.value and/or config.jax_debug_infs is set, the de-optimized function (i.e., the function as if the `jit` decorator were removed) was called in an attempt to get a more precise error message. However, the de-optimized function did not produce invalid values during its execution. This behavior can result from `jit` optimizations causing the invalid value to be produced. It may also arise from having nan/inf constants as outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. \n\nIt may be possible to avoid the invalid value by removing the `jit` decorator, at the cost of losing optimizations. \n\nIf you see this error, consider opening a bug report at https://github.com/jax-ml/jax."
          ]
        }
      ],
      "source": [
        "## functions\n",
        "keys = random.split(random.PRNGKey(10298213), 10)\n",
        "neurons = [\n",
        "    28*28,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10,\n",
        "    10\n",
        "]\n",
        "\n",
        "def init_mlp_params(key, neurons):\n",
        "  # - HE weight initialization\n",
        "  # bias initializaiton as 0\n",
        "  wkey, bkey = random.split(key, 2)\n",
        "  mlp_params = {\n",
        "      # remember, its xW, not Wx, so W should be (in_vector_size, out_vector_size)\n",
        "      # so that (m,) @ (m,n) => (n,)\n",
        "      \"weights\" : [\n",
        "          # He initialization: norm(0,1) * (2/sqrt(weight.size))\n",
        "          random.normal(wkey, shape=(neurons[i], neurons[i+1])) * 2 / jnp.sqrt(neurons[i]*neurons[i+1])\n",
        "          for i in range(len(neurons) - 1)\n",
        "      ],\n",
        "      \"biases\" : [\n",
        "          # initialize biases as 0 vectors\n",
        "          jnp.zeros(shape=neurons[i+1])\n",
        "          for i in range(len(neurons) - 1)\n",
        "      ]\n",
        "  }\n",
        "  return mlp_params\n",
        "\n",
        "def mlp_forward(params, x_batch, y_batch):\n",
        "  # xW, not Wx\n",
        "  # x_batch y_batch\n",
        "  x = x_batch\n",
        "  for i in range(len(neurons)-1):\n",
        "    x = x @ params[\"weights\"][i]\n",
        "    x = x + params[\"biases\"][i]\n",
        "    if i < len(neurons)-2:\n",
        "      x = jax.nn.relu(x)\n",
        "    else:\n",
        "      x = jax.nn.softmax(x)\n",
        "  return x\n",
        "\n",
        "def get_loss(params, x_batch, y_batch):\n",
        "  y_pred_batch = mlp_forward(params, x_batch, y_batch)\n",
        "  # the reason for using jax.scipy.special.xlogy instead of\n",
        "  # -jnp.log(y_pred_batch) * y_batch   is that it accounts for 0 in the\n",
        "  # prediction batch. otherwise, 0 produces -inf and breaks the training\n",
        "  presum = -jax.scipy.special.xlogy(y_batch, y_pred_batch)\n",
        "  crossentropyloss = jnp.sum(presum)\n",
        "  return crossentropyloss\n",
        "\n",
        "\n",
        "def param_norms(params):\n",
        "  norms = {\n",
        "      \"weights\" : [jnp.log(jnp.linalg.norm(w)) for w in params['weights']],\n",
        "      'biases'  : [jnp.log(jnp.linalg.norm(b)) for b in params['biases']]\n",
        "  }\n",
        "  return norms\n",
        "\n",
        "params = init_mlp_params(keys[0], neurons)\n",
        "learning_rate = 0.001\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "# for MNIST? cross entropy sum(-log(prediction)*real)\n",
        "\n",
        "def train_step(params, x_batch, y_batch, optimizer, opt_state):\n",
        "  losses = get_loss(params, x_batch, y_batch)\n",
        "  grads = jax.grad(get_loss)(params, x_batch, y_batch)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  # ok so concepually, updates are different than grads. grads are used to calculate updates\n",
        "  # like in adam where the grads are used to calculate the moments, and then the moments\n",
        "  # combined with the learning rate are used to calculate the change to the params\n",
        "  # i.e. the updates to the params\n",
        "  return params, opt_state, losses, grads\n",
        "\n",
        "\n",
        "import time\n",
        "time_between_logs = 3\n",
        "from pprint import pprint\n",
        "\n",
        "batch_size = 4\n",
        "train_datapoints = len(x_train)\n",
        "batches = len(x_train)//batch_size\n",
        "indices = random.permutation(keys[1], train_datapoints)\n",
        "# first just overfit it on the first batch or something\n",
        "epochs = 1000\n",
        "last_log_time = time.time()\n",
        "\n",
        "record = []\n",
        "for epoch in range(epochs):\n",
        "  for batch in range(batches):\n",
        "    batch_start = batch*batch_size\n",
        "    batch_end = batch_start + batch_size\n",
        "    batch_indices = indices[batch_start:batch_end]\n",
        "    x_batch, y_batch = x_train[batch_indices], y_train[batch_indices]\n",
        "\n",
        "    params, opt_state, losses, norms = train_step(params, x_batch, y_batch, optimizer, opt_state)\n",
        "\n",
        "    if time.time() - last_log_time > time_between_logs:\n",
        "      last_log_time = time.time()\n",
        "      print(f\"epoch {epoch}, batch {batch}, loss={jnp.mean(losses)}\")\n",
        "      #pprint(norms)\n",
        "      record.append((epoch, jnp.mean(losses)))\n",
        "\n",
        "# optax adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "safe_map() argument 2 is shorter than argument 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m losses \u001b[38;5;241m=\u001b[39m get_loss(params, x_batch, y_batch)\n\u001b[1;32m     11\u001b[0m closed_jaxpr \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mmake_jaxpr(jax\u001b[38;5;241m.\u001b[39mgrad(get_loss))(params, x_batch, y_batch)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_jaxpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosed_jaxpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosed_jaxpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliterals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/core.py:500\u001b[0m, in \u001b[0;36meval_jaxpr\u001b[0;34m(jaxpr, consts, propagate_source_info, *args)\u001b[0m\n\u001b[1;32m    498\u001b[0m env: \u001b[38;5;28mdict\u001b[39m[Var, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28mmap\u001b[39m(write, jaxpr\u001b[38;5;241m.\u001b[39mconstvars, consts)\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m lu \u001b[38;5;241m=\u001b[39m last_used(jaxpr)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eqn \u001b[38;5;129;01min\u001b[39;00m jaxpr\u001b[38;5;241m.\u001b[39meqns:\n",
            "\u001b[0;31mValueError\u001b[0m: safe_map() argument 2 is shorter than argument 1"
          ]
        }
      ],
      "source": [
        "def param_norms(params):\n",
        "  norms = {\n",
        "      \"weights\" : [jnp.log(jnp.linalg.norm(w)) for w in params['weights']],\n",
        "      'biases'  : [jnp.log(jnp.linalg.norm(b)) for b in params['biases']]\n",
        "  }\n",
        "  return norms\n",
        "\n",
        "losses = get_loss(params, x_batch, y_batch)\n",
        "\n",
        "\n",
        "closed_jaxpr = jax.make_jaxpr(jax.grad(get_loss))(params, x_batch, y_batch)\n",
        "jax.core.eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, params, x_batch, y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p0zWPuWSh8IT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.6.1 (SDL 2.28.4, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "Predicted Probabilities: [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "Predicted Class: 0\n",
            "Model Prediction: 0\n",
            "Predicted Probabilities: [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "Predicted Class: 0\n",
            "Model Prediction: 0\n",
            "Predicted Probabilities: [[nan nan nan nan nan nan nan nan nan nan]]\n",
            "Predicted Class: 0\n",
            "Model Prediction: 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ChatGPT made this\n",
        "import pygame\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "# Initialize Pygame\n",
        "pygame.init()\n",
        "\n",
        "# Constants\n",
        "GRID_SIZE = 28\n",
        "CELL_SIZE = 20  # Size of each cell in the grid (pixels)\n",
        "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
        "\n",
        "# Colors\n",
        "WHITE = (255, 255, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "GRAY = (200, 200, 200)\n",
        "\n",
        "# Initialize the grid (28x28, like MNIST)\n",
        "grid = np.zeros((GRID_SIZE, GRID_SIZE), dtype=np.float32)\n",
        "\n",
        "# Initialize the window\n",
        "screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
        "pygame.display.set_caption(\"Draw and Predict\")\n",
        "\n",
        "def draw_grid():\n",
        "    \"\"\"Draw the grid on the Pygame window.\"\"\"\n",
        "    screen.fill(WHITE)\n",
        "    for y in range(GRID_SIZE):\n",
        "        for x in range(GRID_SIZE):\n",
        "            color = BLACK if grid[y, x] > 0 else WHITE\n",
        "            pygame.draw.rect(screen, color, (x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
        "            pygame.draw.rect(screen, GRAY, (x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE), 1)  # Grid lines\n",
        "\n",
        "def get_prediction(grid):\n",
        "    \"\"\"Predict the digit based on the current grid.\"\"\"\n",
        "    input_image = jnp.array(grid).reshape(1, -1)  # Flatten the grid\n",
        "    y_pred = mlp_forward(params, input_image, None)  # Predict using the MLP\n",
        "    predicted_class = jnp.argmax(y_pred)\n",
        "    print(f\"Predicted Probabilities: {y_pred}\")\n",
        "    print(f\"Predicted Class: {predicted_class}\")\n",
        "    return predicted_class\n",
        "\n",
        "def main():\n",
        "    running = True\n",
        "    drawing = False  # Track whether the mouse is pressed\n",
        "\n",
        "    while running:\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                running = False\n",
        "\n",
        "            # Handle mouse clicks\n",
        "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
        "                drawing = True\n",
        "            if event.type == pygame.MOUSEBUTTONUP:\n",
        "                drawing = False\n",
        "\n",
        "            # Clear the grid\n",
        "            if event.type == pygame.KEYDOWN:\n",
        "                if event.key == pygame.K_c:  # Press 'C' to clear\n",
        "                    grid.fill(0)\n",
        "                if event.key == pygame.K_p:  # Press 'P' to predict\n",
        "                    prediction = get_prediction(grid)\n",
        "                    print(f\"Model Prediction: {prediction}\")\n",
        "\n",
        "        # Draw on the grid when the mouse is pressed\n",
        "        if drawing:\n",
        "            x, y = pygame.mouse.get_pos()\n",
        "            grid_x, grid_y = x // CELL_SIZE, y // CELL_SIZE\n",
        "            if 0 <= grid_x < GRID_SIZE and 0 <= grid_y < GRID_SIZE:\n",
        "                grid[grid_y, grid_x] = 1  # Mark the cell as filled\n",
        "\n",
        "        # Update the display\n",
        "        draw_grid()\n",
        "        pygame.display.flip()\n",
        "\n",
        "    pygame.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
