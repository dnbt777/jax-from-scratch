{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o71b_stSBdOt"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "import jax\n",
        "import optax\n",
        "import torch\n",
        "import time\n",
        "from pprint import pprint\n",
        "#jax.config.update(\"jax_debug_nans\", True)\n",
        "#jax.config.update(\"jax_debug_infs\", True)\n",
        "#jax.config.update(\"jax_enable_x64\", True)\n",
        "#jax.disable_jit(disable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZVSVlAzesCU7",
        "outputId": "f75a5b00-3fb7-4ec2-8f6d-0078b836405b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([1.], dtype=float32)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check if GPU is working\n",
        "jax.default_backend()\n",
        "jax.device_put(jax.numpy.ones(1), device=jax.devices('gpu')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RaRSziO_K2z2"
      },
      "outputs": [],
      "source": [
        "# set up params\n",
        "batch_size = 4\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# first load the dataset\n",
        "train_data = torchvision.datasets.MNIST(root = './', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_data = torchvision.datasets.MNIST(root = './', train=False, download=True, transform=transforms.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3g5pPllZiydI"
      },
      "outputs": [],
      "source": [
        "# convert to jnp/np\n",
        "x_train, y_train = zip(*train_data)\n",
        "x_train, y_train = jnp.array(x_train), jnp.array(y_train)\n",
        "\n",
        "x_test, y_test = zip(*test_data)\n",
        "x_test, y_test = jnp.array(x_test), jnp.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dQAnlea1j3uu"
      },
      "outputs": [],
      "source": [
        "# flatten each x\n",
        "x_train = jnp.array([jnp.ravel(x) for x in x_train])\n",
        "x_test = jnp.array([jnp.ravel(x) for x in x_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxA0Ir2xi1FA",
        "outputId": "2c9b3b00-5892-416a-83dd-d7e3baa3aeea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        }
      ],
      "source": [
        "# convert ys to one-hot\n",
        "classes = len(set(y_train.tolist()))\n",
        "print(classes)\n",
        "y_train = jax.nn.one_hot(y_train, classes) # from n -> one-hot of n\n",
        "y_test = jax.nn.one_hot(y_test, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_YRTUPowslNm"
      },
      "outputs": [],
      "source": [
        "#jax.device_put(x_train, device=jax.devices('gpu')[0])\n",
        "#jax.device_put(y_train, device=jax.devices('gpu')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc14obpVIcWF",
        "outputId": "7fb72146-5901-4447-f3eb-2a26e64000da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784,) [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "(784,) [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "(784,) [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(784,) [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Array(0.13768007, dtype=float32)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train_data[idx][0] => x   (1, 28, 28)\n",
        "# train_data[idx][1] => y   int\n",
        "for idx in range(10):\n",
        "  print(x_train[idx].shape, y_train[idx])\n",
        "  # print(x_train[idx][0][14], train_data[idx][0][0][14])\n",
        "\n",
        "jnp.mean(jnp.array(x_train[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AikDkgdRFJff",
        "outputId": "a561d49b-4487-4f82-84ef-62ebb3c9feb6"
      },
      "outputs": [],
      "source": [
        "## functions\n",
        "keys = random.split(random.PRNGKey(10298213), 10)\n",
        "neurons = [\n",
        "    28*28,\n",
        "    28*28,\n",
        "    28*28,\n",
        "    28*28,\n",
        "    28*28,\n",
        "    10\n",
        "]\n",
        "\n",
        "def init_mlp_params(wkey, neurons):\n",
        "  # - HE weight initialization\n",
        "  # bias initializaiton as 0\n",
        "  mlp_params = {\n",
        "    f\"layer_{i}\" : {\n",
        "      # remember, its xW, not Wx, so W should be (in_vector_size, out_vector_size)\n",
        "      # so that (m,) @ (m,n) => (n,)\n",
        "      # He initialization: norm(0,1) * (2/sqrt(weight.size))\n",
        "      \"weight\" : random.normal(wkey, shape=(neurons[i], neurons[i+1])) * jnp.sqrt(2 / neurons[i]),\n",
        "      # initialize biases as 0 vectors\n",
        "      \"bias\" : jnp.zeros(shape=neurons[i+1])\n",
        "    } for i in range(len(neurons) - 1)\n",
        "  }\n",
        "  return mlp_params\n",
        "\n",
        "def mlp_forward(params, x_batch):\n",
        "  # returns LOGITS\n",
        "  # xW, not Wx\n",
        "  # x_batch y_batch\n",
        "  x = x_batch\n",
        "  for i in range(len(neurons)-1):\n",
        "    x = x @ params[f\"layer_{i}\"][\"weight\"] + params[f\"layer_{i}\"][\"bias\"]\n",
        "    if i < len(neurons)-2:\n",
        "      x = jax.nn.relu(x)\n",
        "    else:\n",
        "      pass\n",
        "      #x = jax.nn.softmax(x)\n",
        "      # no. return logits, so log_softmax can be used in get_loss\n",
        "  return x\n",
        "\n",
        "def accuracy(params, x_batch, y_batch):\n",
        "  logits = mlp_forward(params, x_batch)\n",
        "  predictions = jnp.argmax(logits, axis=1)\n",
        "  correct = jnp.argmax(y_batch, axis=1)\n",
        "  return jnp.mean(predictions == correct)\n",
        "\n",
        "\n",
        "def get_loss(params, x_batch, y_batch):\n",
        "  logits_batch = mlp_forward(params, x_batch)\n",
        "  # the reason for using jax.scipy.special.xlogy instead of\n",
        "  # -jnp.log(y_pred_batch) * y_batch   is that it accounts for 0 in the\n",
        "  # prediction batch. otherwise, 0 produces -inf and breaks the training\n",
        "  log_probs = jax.nn.log_softmax(logits_batch) # using this builtin prevents issues when calculating grads\n",
        "  crossentropyloss_batch = jnp.sum(y_batch * log_probs, axis=1)\n",
        "  batch_loss = -jnp.mean(crossentropyloss_batch)\n",
        "  return batch_loss\n",
        "\n",
        "\n",
        "def param_norms(params):\n",
        "  norms = {\n",
        "      \"weights\" : [jnp.log(jnp.linalg.norm(w)) for w in params['weights']],\n",
        "      'biases'  : [jnp.log(jnp.linalg.norm(b)) for b in params['biases']]\n",
        "  }\n",
        "  return norms\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = optax.adam(learning_rate)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, x_batch, y_batch, opt_state):\n",
        "  losses = get_loss(params, x_batch, y_batch)\n",
        "  grads = jax.grad(get_loss)(params, x_batch, y_batch)\n",
        "  updates, updated_opt_state = optimizer.update(grads, opt_state)\n",
        "  updated_params = optax.apply_updates(params, updates)\n",
        "  # ok so concepually, updates are different than grads. grads are used to calculate updates\n",
        "  # like in adam where the grads are used to calculate the moments, and then the moments\n",
        "  # combined with the learning rate are used to calculate the change to the params\n",
        "  # i.e. the updates to the params\n",
        "  return updated_params, updated_opt_state, losses, grads\n",
        "\n",
        "\n",
        "def train_loop():\n",
        "  record = []\n",
        "  time_limit = 30000000 #seconds\n",
        "\n",
        "  params = init_mlp_params(keys[0], neurons)\n",
        "  opt_state = optimizer.init(params)\n",
        "  # for MNIST? cross entropy sum(-log(prediction)*real)\n",
        "\n",
        "\n",
        "  batch_size = 8\n",
        "  train_datapoints = len(x_train)\n",
        "  batches = len(x_train)//batch_size\n",
        "  indices = random.permutation(keys[1], train_datapoints)\n",
        "  # first just overfit it on the first batch or something\n",
        "  epochs = 500000000\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    indices = random.permutation(random.PRNGKey(epoch), train_datapoints)\n",
        "    for batch in range(batches):\n",
        "      batch_start = batch*batch_size\n",
        "      batch_end = batch_start + batch_size\n",
        "      batch_indices = indices[batch_start:batch_end]\n",
        "      x_batch, y_batch = x_train[batch_indices], y_train[batch_indices]\n",
        "\n",
        "      acc = accuracy(params, x_batch, y_batch)\n",
        "      params, opt_state, losses, norms = train_step(params, x_batch, y_batch, opt_state)\n",
        "\n",
        "      print(f\"epoch {epoch}, batch {batch}, loss={jnp.mean(losses)}, acc={acc}\")\n",
        "      #pprint(norms)\n",
        "      #record.append((epoch, jnp.mean(losses)))\n",
        "\n",
        "      if jnp.mean(losses) == 0 or time.time() - start_time >= time_limit:\n",
        "        duration = (time.time() - start_time)\n",
        "        print(f\"DONE in {duration}s\")\n",
        "        steps_per_batch = 8\n",
        "        batches_per_epoch = batches\n",
        "        steps = epoch*batches_per_epoch*steps_per_batch + batch*steps_per_batch\n",
        "        steps_per_second = steps/duration\n",
        "        print(\"Samples trained on per second: \", steps_per_second)\n",
        "        return\n",
        "\n",
        "train_loop()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
