{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrand\n",
    "import numpy as np\n",
    "import optax\n",
    "import functools # for partial\n",
    "from functools import reduce\n",
    "import time\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "ipynb = True\n",
    "dataset_name = \"dictionary\"\n",
    "debug = False\n",
    "\n",
    "\n",
    "if debug:\n",
    "  jax.config.update(\"jax_disable_jit\", True)\n",
    "\n",
    "\n",
    "## LOAD DATA ##\n",
    "from transformer_utils import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "prefix = \"\"\n",
    "if ipynb == True:\n",
    "  prefix = \"../\"\n",
    "\n",
    "token_type = 'bpe' # wordlevel bpe charlevel\n",
    "tokenizer_vocab_size = 120\n",
    "if dataset_name == \"shakespeare\":\n",
    "  dataset_path = prefix + \"data/shakespeare.txt\"\n",
    "  vocab, train_data, test_data, encode, decode = load_dataset(dataset_path, split=0.9, vocab_size=tokenizer_vocab_size, prefix=prefix)\n",
    "if dataset_name == \"dnbt\":\n",
    "  dataset_path = prefix + \"data/dnbt_posts.txt\"\n",
    "  vocab, train_data, test_data, encode, decode = load_dataset(dataset_path, split=0.9, vocab_size=tokenizer_vocab_size, prefix=prefix)\n",
    "if dataset_name == \"dictionary\":\n",
    "  dataset_path = prefix + \"data/dictionary.txt\"\n",
    "  vocab, train_data, test_data, encode, decode = load_dataset(dataset_path, split=0.9, vocab_size=tokenizer_vocab_size, prefix=prefix)\n",
    "if dataset_name == \"wikipedia\":\n",
    "  dataset_path = prefix + \"data/dnbt_posts.txt\"\n",
    "  vocab, train_data, test_data, encode, decode = load_dataset(dataset_path, split=0.9, vocab_size=tokenizer_vocab_size, prefix=prefix)\n",
    "if dataset_name == \"tinystories\":\n",
    "  print(\"preparing dataset...\")\n",
    "  dataset_path = prefix + \"data/tinystories_combined.txt\" # concatenate test and val, and split ourselves. should be roughly the same at 0.9\n",
    "  vocab, train_data, test_data, encode, decode = load_dataset(dataset_path, split=0.9, vocab_size=tokenizer_vocab_size, prefix=prefix)\n",
    "  print(\"Done\")\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "\n",
    "## SET UP PARAM STRUCTS ##\n",
    "# https://github.com/xjdr-alt/simple_transformer/blob/main/simple_transformer.py\n",
    "class BlockParams(NamedTuple):\n",
    "  w_q : jax.Array\n",
    "  w_k : jax.Array\n",
    "  w_v : jax.Array\n",
    "  w_o : jax.Array\n",
    "  w1 : jax.Array\n",
    "  w2 : jax.Array\n",
    "  w3 : jax.Array\n",
    "  attn_norm_w : jax.Array\n",
    "  attn_norm_b : jax.Array\n",
    "  ffnorm_w : jax.Array\n",
    "  ffnorm_b : jax.Array\n",
    "\n",
    "\n",
    "class ModelParams(NamedTuple):\n",
    "  blocks : List[BlockParams]\n",
    "  embedding_projection : jax.Array\n",
    "  to_logits_w : jax.Array # after entire network\n",
    "  positional_embeddings : jax.Array\n",
    "  output_norm_w : jax.Array\n",
    "  output_norm_b : jax.Array\n",
    "\n",
    "\n",
    "def init_model_params(blocks, model_dim, d_k, qkv_dim, ff_hidden_size, vocab_size, block_size):\n",
    "  D = d_k\n",
    "  K = qkv_dim\n",
    "  attention_heads = model_dim // d_k\n",
    "  H = attention_heads\n",
    "  DH = attention_heads * d_k\n",
    "  scale = lambda s1, s2: 1 / jnp.sqrt(s1 + s2)\n",
    "  xavier_blocks = lambda n, m: np.random.uniform(size=(blocks, n, m)) * scale(n, m)\n",
    "  xavier_multihead_blocks = lambda n, m: np.random.uniform(size=(blocks, H, n, m))\n",
    "  xavier = lambda n, m: np.random.uniform(size=(n, m)) * scale(n, m)\n",
    "  \n",
    "  block_params = BlockParams(\n",
    "    # multi head attention\n",
    "    # C => D, H\n",
    "    w_q=xavier_multihead_blocks(D, K),\n",
    "    w_k=xavier_multihead_blocks(D, K),\n",
    "    w_v=xavier_multihead_blocks(D, K),\n",
    "    w_o=xavier_multihead_blocks(K, D),\n",
    "\n",
    "    w1=xavier_blocks(model_dim, ff_hidden_size),\n",
    "    w2=xavier_blocks(model_dim, ff_hidden_size),\n",
    "    w3=xavier_blocks(ff_hidden_size, model_dim),\n",
    "    # norm stuff\n",
    "    attn_norm_w=xavier_blocks(1, model_dim), # B, T, C (x) B, 1, C\n",
    "    attn_norm_b=jnp.zeros(shape=(blocks, model_dim,)),\n",
    "    ffnorm_w=xavier_blocks(1, model_dim),\n",
    "    ffnorm_b=jnp.zeros(shape=(blocks, model_dim,)),\n",
    "  )\n",
    "\n",
    "  model_params = ModelParams(\n",
    "    blocks=block_params,\n",
    "    embedding_projection=xavier(vocab_size, model_dim),\n",
    "    to_logits_w=xavier(model_dim, vocab_size),\n",
    "    positional_embeddings=xavier(block_size, model_dim), # bias: T, C\n",
    "    output_norm_w=xavier(1, model_dim),\n",
    "    output_norm_b=jnp.zeros(shape=(model_dim,)),\n",
    "  )\n",
    "\n",
    "  return model_params\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def attention(block_params : BlockParams, xBTC, dropout_key, dropout_rate=0.0):\n",
    "  # xBTC -> xBTHD via reshape | H = head, D = dim (like channel, but split into H segments)\n",
    "  B, T, C = xBTC.shape\n",
    "  H = block_params.w_q.shape[0] # attention head\n",
    "  D = block_params.w_q.shape[1] # model dim after split into heads\n",
    "  K = block_params.w_q.shape[2] # query dim\n",
    "  BTHD = (B, T, H, D)\n",
    "  xBTHD = jnp.reshape(xBTC, shape=BTHD)\n",
    "  # xBTHD -> xBHTD via transpose or axes swap\n",
    "  xBHTD = jnp.swapaxes(xBTHD, 1, 2)\n",
    "  # xBHTD @ (Wq, Wk, Wv) => (Q, K, V)   | (B, H, T, D) @ (D, K) => (B, H, T, K) | K = query/key size. also V size here, but does not have to be.\n",
    "  Q = jnp.einsum(\"BHTD,HDK->BHTK\", xBHTD, block_params.w_q) # (B, H, T, D) @ (H, D, K) => (B, H, T, K)\n",
    "  K = jnp.einsum(\"BHTD,HDK->BHTK\", xBHTD, block_params.w_k)\n",
    "  V = jnp.einsum(\"BHTD,HDK->BHTK\", xBHTD, block_params.w_v)\n",
    "  # Q @ K_transpose => QK               | (B, H, T, K) @ (B, H, K, T) => (B, H, T, T) | this may be the wrong transpose\n",
    "  QK = Q @ jnp.swapaxes(K, 2, 3)\n",
    "  # scale, mask, and softmax\n",
    "  mask = jnp.triu(jnp.ones_like(QK), k=1) #  mask=1 where jnp.-inf will be. k=1 preseves the middle diagonal for self attention\n",
    "  attention_scores = jnp.where(mask, -jnp.inf, QK/jnp.sqrt(d_k))\n",
    "  # QK @ V => Z                         | (B, H, T, T) @ (B, H, T, K) => (B, H, T, K)\n",
    "  Z = jax.nn.softmax(attention_scores, axis=-1) @ V # softmax in the T and K dims, for each B and H\n",
    "  # Z swap axes                         | (B, H, T, K) => (B, T, H, K)\n",
    "  Z = dropout(dropout_key, Z, dropout_rate)\n",
    "  Z = jnp.swapaxes(Z, 1, 2)\n",
    "  # Z @ w_out       | project to BTHD   | (B, T, H, K) @ (H, K, D) => (B, T, H, D)\n",
    "  xBTHD = jnp.einsum(\"BTHK,HKD->BTHD\", Z, block_params.w_o)\n",
    "  # reshape to BTC                      | (B, T, H, D) => (B, T, C)\n",
    "  xBTC = jnp.reshape(xBTHD, (B, T, C))\n",
    "  # return xBTC_out\n",
    "  xBTC = dropout(dropout_key, xBTC, dropout_rate)\n",
    "  return xBTC\n",
    "\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def forward(dropout_key, model_params : ModelParams, xBT, vocab_size=vocab_size, dropout_rate=0.0):\n",
    "  dropout_keys = jrand.split(dropout_key, 4)\n",
    "  # get embeddings via projection from token onehot to channel space\n",
    "  # xBT -> xBTC\n",
    "  xBTOH = jax.nn.one_hot(xBT, num_classes=vocab_size, axis=-1)\n",
    "  xBTC = xBTOH @ model_params.embedding_projection\n",
    "  # add learned positional embeddings. broadcast over B\n",
    "  B, T, C = xBTC.shape\n",
    "  xBTC = xBTC + model_params.positional_embeddings[None, :T, :] # B, T, C + 1, T, C\n",
    "  # scan through transformer blocks, updating xBTC\n",
    "  # block\n",
    "  #   attention\n",
    "  #   forward projection from Z space to channel space\n",
    "  # scan_fn :: (c, a) -> (c, b)\n",
    "  def ffw(block_params, xBTC):\n",
    "    #return jax.nn.silu(xBTC @ block_params.w1) * ((xBTC @ block_params.w2 + block_params.b2) @ block_params.w3 + block_params.b3)\n",
    "    return (jax.nn.silu(xBTC @ block_params.w1) * (xBTC @ block_params.w2)) @ block_params.w3\n",
    "  \n",
    "  def scan_fn(xBTC, block_params):\n",
    "    xBTC = xBTC + attention(block_params, layer_norm(xBTC, block_params.attn_norm_w, block_params.attn_norm_b), dropout_keys[0], dropout_rate)\n",
    "    xBTC = xBTC + dropout(dropout_keys[1], ffw(block_params, layer_norm(xBTC, block_params.ffnorm_w, block_params.ffnorm_b)), dropout_rate)\n",
    "    return xBTC, None # c, b\n",
    "  # ((c, a) -> (c, b)) -> c -> [a] -> (c, [b]) where b is None\n",
    "  xBTC = jax.lax.scan(scan_fn, xBTC, model_params.blocks)[0] # (xBTC, None) from the scan\n",
    "  # then project the embeddings to logit space\n",
    "  #xBTC = layer_norm(xBTC, model_params.output_norm_w, model_params.output_norm_b)\n",
    "  # logits = xBTC @ model_params.to_logits_w + model_params.to_logits_b\n",
    "  logits = xBTC @ model_params.to_logits_w\n",
    "  return dropout(key, logits, dropout_rate)\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def model_loss(dropout_key, model_params : ModelParams, xBT, yBT, dropout_rate):\n",
    "  # -sum(q*log(p))\n",
    "  # dont use one hot?\n",
    "  logits = forward(dropout_key, model_params, xBT, dropout_rate=dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  labels = jax.nn.one_hot(yBT, num_classes=vocab_size, axis=-1)\n",
    "  losses = -jnp.sum(labels * jax.nn.log_softmax(logits, axis=-1), axis=-1)\n",
    "  mean_loss = jnp.mean(losses)\n",
    "  return mean_loss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\", \"label_smoothing_epsilon\"])\n",
    "def smoothed_model_loss(dropout_key, model_params : ModelParams, xBT, yBT, dropout_rate, label_smoothing_epsilon):\n",
    "  # -sum(q*log(p))\n",
    "  # dont use one hot?\n",
    "  logits = forward(dropout_key, model_params, xBT, dropout_rate=dropout_rate)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  labels = jax.nn.one_hot(yBT, num_classes=vocab_size, axis=-1) * (1 - label_smoothing_epsilon) + label_smoothing_epsilon/vocab_size\n",
    "  losses = -jnp.sum(labels * jax.nn.log_softmax(logits, axis=-1), axis=-1)\n",
    "  mean_loss = jnp.mean(losses)\n",
    "  return mean_loss\n",
    "\n",
    "\n",
    "# for validation only\n",
    "@jax.jit\n",
    "def model_loss_and_accuracy(model_params : ModelParams, xBT, yBT):\n",
    "  # -sum(q*log(p))\n",
    "  # dont use one hot?\n",
    "  dropout_key = jrand.PRNGKey(0) # not used, but needed\n",
    "  logits = forward(dropout_key, model_params, xBT) # no key, no dropout\n",
    "  vocab_size = logits.shape[-1]\n",
    "  labels = jax.nn.one_hot(yBT, num_classes=vocab_size, axis=-1)\n",
    "  losses = -jnp.sum(labels * jax.nn.log_softmax(logits, axis=-1), axis=-1) # no label smoothing\n",
    "  mean_loss = jnp.mean(losses)\n",
    "  mean_accuracy = jnp.mean(yBT == jnp.argmax(logits, axis=-1))\n",
    "  return mean_loss, mean_accuracy\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['optimizer', \"dropout_rate\", \"label_smoothing_epsilon\"])\n",
    "def train_step(dropout_key,\n",
    "               model_params:ModelParams,\n",
    "               xBT, yBT,\n",
    "               opt_state, optimizer,\n",
    "               dropout_rate, label_smoothing_epsilon):\n",
    "  # jax value and grad\n",
    "  loss, grads = jax.value_and_grad(smoothed_model_loss, argnums=1)(\n",
    "    dropout_key, model_params, xBT, yBT, dropout_rate, label_smoothing_epsilon)\n",
    "  # update optimizer\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, model_params)\n",
    "  # update params\n",
    "  model_params = optax.apply_updates(updates, model_params)\n",
    "  # return opt state, loss, and new params\n",
    "  return model_params, opt_state, loss, grads\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def inference(model_params : ModelParams, xBT, temp=0.5):\n",
    "  dropout_key = jrand.PRNGKey(0) # not used\n",
    "  logits = forward(dropout_key, model_params, xBT, dropout_rate=0.0)[0, :, :] # the first Batch\n",
    "  probs = jax.nn.softmax(logits / temp, axis=-1)\n",
    "  get_token = lambda channel: jrand.choice(jrand.PRNGKey(int(1000*time.time())), channel.shape[-1], p=channel)\n",
    "  ts = jax.vmap(get_token, in_axes=0, out_axes=0)(probs) # just take the first T in the first batch ig\n",
    "  return ts\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=[\"dropout_rate\"])\n",
    "def dropout(dropout_key, tensor, dropout_rate):\n",
    "  if dropout_rate == 0:\n",
    "    return tensor\n",
    "  # generate dropout_mask = rand(x) < dropout_rate of size tensor.shape\n",
    "  dropout_mask = jrand.uniform(dropout_key, tensor.shape, tensor.dtype) <= dropout_rate\n",
    "  return jnp.where(dropout_mask, 0, tensor) * (1 / (1 - dropout_rate))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def layer_norm(BTC, w, b):\n",
    "  # norm over TxC\n",
    "  epsilon = 1e-7\n",
    "  mean = jnp.mean(BTC, axis=(1, 2), keepdims=True)\n",
    "  variance = jnp.var(BTC, axis=(1, 2), keepdims=True)\n",
    "  normalized = (BTC - mean) / jnp.sqrt(variance + epsilon)\n",
    "  return normalized * w + b\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def channel_norm(BTC, w, b):\n",
    "  # norm over C\n",
    "  epsilon = 1e-7\n",
    "  mean = jnp.mean(BTC, axis=-1, keepdims=True)\n",
    "  variance = jnp.var(BTC, axis=-1, keepdims=True)\n",
    "  normalized = (BTC - mean) / jnp.sqrt(variance + epsilon)\n",
    "  return normalized * w + b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## TRAIN\n",
    "# model params\n",
    "# model params\n",
    "transformer_blocks = 6\n",
    "attention_heads = 8\n",
    "\n",
    "model_dim = 512\n",
    "qkv_dim = 64\n",
    "ff_hidden_size = 2048 # 2048\n",
    "block_size = 64 # xlen\n",
    "\n",
    "\n",
    "# paper lr stuff\n",
    "warmup_steps = 4000\n",
    "get_lr = lambda step_num: (model_dim**-0.5) * min(step_num ** -0.5, step_num * warmup_steps**(-1.5))\n",
    "initial_lr = get_lr(1)\n",
    "label_smoothing_epsilon = 0.1\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# adam\n",
    "beta1 = 0.9\n",
    "beta2 = 0.98\n",
    "eps = 1e-9\n",
    "\n",
    "\n",
    "# set up training\n",
    "epochs = 10000\n",
    "max_shifts = 1000000000000000000\n",
    "train_batch_size = 64\n",
    "test_batch_size = 16\n",
    "print_every = 100\n",
    "stop_if_above = 1.5 # 150% of min val loss\n",
    "\n",
    "\n",
    "continue_training = False\n",
    "start_ep = 100\n",
    "start_step = 17600\n",
    "\n",
    "d_k = model_dim // attention_heads\n",
    "\n",
    "if not continue_training:\n",
    "  model_params = init_model_params(transformer_blocks, model_dim, d_k, qkv_dim, ff_hidden_size, vocab_size, block_size) # t blocks is hardcoded to 1 atm\n",
    "  optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=initial_lr, b1=beta1, b2=beta2, eps=eps)\n",
    "  opt_state = optimizer.init(model_params)\n",
    "  print(\"model params initialized.\\nstarting training\")\n",
    "\n",
    "\n",
    "\n",
    "loss_tracker = []\n",
    "global_step = 1\n",
    "min_val_loss = 10000 # init min val loss as extremely high\n",
    "stop_training = False # flag to stop training\n",
    "for epoch in range(epochs):\n",
    "  if stop_training:\n",
    "    break\n",
    "  if continue_training and epoch < start_ep:\n",
    "    continue\n",
    "  tokens_per_shift = (block_size + 1)*train_batch_size\n",
    "  test_tokens_per_shift = (block_size + 1)*test_batch_size\n",
    "  shifts = len(train_data) - tokens_per_shift\n",
    "  test_shifts = len(test_data) - test_tokens_per_shift\n",
    "\n",
    "  local_step = 1\n",
    "  losses = []\n",
    "\n",
    "  for shift in range(0, min(shifts, max_shifts), tokens_per_shift):\n",
    "    global_step += 1\n",
    "    local_step += 1\n",
    "    if continue_training and global_step < start_step:\n",
    "      continue\n",
    "    steps_start = time.time()\n",
    "\n",
    "    # update LR\n",
    "    opt_state.hyperparams['learning_rate'] = get_lr(global_step)\n",
    "\n",
    "    # gather data and train\n",
    "    train_tokens = train_data[shift:shift + tokens_per_shift].reshape(-1, block_size + 1) # (B, T) where T = block_size\n",
    "    xBT = train_tokens[:, :block_size] # get up to prompt_length\n",
    "    yBT = train_tokens[:, 1:block_size+1] # get the one after prompt_length\n",
    "    key = jrand.PRNGKey(epoch*shifts + shift)\n",
    "    model_params, opt_state, loss, grads = train_step(key, model_params, xBT, yBT, opt_state, optimizer, dropout_rate, label_smoothing_epsilon)\n",
    "\n",
    "    # tracking\n",
    "    losses.append(loss)\n",
    "    loss_tracker.append((\"train\", epoch, global_step, float(loss)))\n",
    "    if global_step % print_every == 0 or global_step == 0:\n",
    "      # validation\n",
    "      test_shift = shift % test_shifts\n",
    "      test_tokens = test_data[test_shift:test_shift + test_tokens_per_shift].reshape(-1, block_size + 1)\n",
    "      test_xBT = test_tokens[:, :block_size]\n",
    "      test_yBT = test_tokens[:, 1:block_size+1]\n",
    "      test_loss, test_accuracy = model_loss_and_accuracy(model_params, test_xBT, test_yBT)\n",
    "      loss_tracker.append((\"test\", epoch, global_step, float(test_loss)))\n",
    "      \n",
    "      # tracking\n",
    "      yhats = inference(model_params, xBT, temp=0.5)\n",
    "      in_chars = decode(test_xBT[0]).replace('\\n', '↵')\n",
    "      target_char = decode(test_yBT[0]).replace('\\n', '↵')\n",
    "      pred_char = decode(yhats).replace('\\n', '↵')\n",
    "      mean_step_loss = jnp.mean(jnp.array(losses))\n",
    "      steps_stop = time.time()\n",
    "      steps_per_second = local_step / (steps_stop - steps_start)\n",
    "      samples_per_second = steps_per_second\n",
    "      lr = opt_state.hyperparams['learning_rate'] \n",
    "      print(f\"e/s={epoch}/{global_step} samples/s={samples_per_second:0.0f} {lr=:0.5f} tloss={mean_step_loss:0.3f} vloss/acc={test_loss:0.3f}/{100*test_accuracy:0.1f}% \"\n",
    "            f\"|| val \\n'...{target_char}' =?> \\n'...{pred_char}'\")\n",
    "      losses = []\n",
    "      local_step = 0\n",
    "\n",
    "      # stopping\n",
    "      if test_loss > stop_if_above * min_val_loss:\n",
    "        stop_training = True\n",
    "        break\n",
    "      else:\n",
    "        min_val_loss = min(test_loss, min_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats and norms\n",
    "# run before graphing in next cell\n",
    "from pprint import pprint\n",
    "\n",
    "def pprint_namedtuple(nt):\n",
    "  nt = nt._replace(blocks=dict(nt.blocks._asdict()))\n",
    "  pprint(dict(nt._asdict()))\n",
    "  print()\n",
    "\n",
    "\n",
    "parameter_shapes_and_counts = jax.tree_util.tree_map(lambda t: f\"{t.shape} => {t.size}\", grads)\n",
    "parameter_count = jax.tree_util.tree_map(lambda t: t.size, grads)\n",
    "parameter_total = jax.tree.reduce(lambda a, b: a + b, parameter_count)\n",
    "print(f\"Parameters: {parameter_total:,}\")\n",
    "pprint_namedtuple(parameter_shapes_and_counts)\n",
    "\n",
    "\n",
    "print(\"Grad norms:\")\n",
    "grad_norms = jax.tree_util.tree_map(jnp.linalg.norm, grads)\n",
    "pprint_namedtuple(grad_norms)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphing\n",
    "# run cell above for this one to work\n",
    "from matplotlib import pyplot as plt\n",
    "loss_tracker = [item for item in loss_tracker if type(item) == type((2,))]\n",
    "steps_per_epoch = 0\n",
    "for item in loss_tracker:\n",
    "  if item[2] > steps_per_epoch:\n",
    "    steps_per_epoch = item[2]\n",
    "\n",
    "\n",
    "def moving_average(xs, ys, window):\n",
    "  queue = []\n",
    "  mays = []\n",
    "  for elem in ys:\n",
    "    if len(queue) < window:\n",
    "      queue.append(elem)\n",
    "    if len(queue) < window:\n",
    "      continue\n",
    "    elif len(queue) == window:\n",
    "      mays.append(sum(queue)/window)\n",
    "      queue.pop(0)\n",
    "      queue.append(elem)\n",
    "  \n",
    "  return xs[:-window + 1], mays\n",
    "\n",
    "\n",
    "trainx, trainy = zip(*[(step, loss) for losstype, epoch, step, loss in loss_tracker if losstype==\"train\"])\n",
    "window_size = 5 # steps\n",
    "trainx, trainy = moving_average(trainx, trainy, window_size)\n",
    "testx, testy = zip(*[(step, loss) for losstype, epoch, step, loss in loss_tracker if losstype==\"test\" ])\n",
    "testx, testy = moving_average(testx, testy, window_size)\n",
    "\n",
    "plt.title(f'vanilla transformer ({parameter_total/1_000_000:0.2f}M params)')\n",
    "plt.plot(trainx, jnp.log(jnp.array(trainy)), c='orange', label='train loss')\n",
    "plt.plot(testx, jnp.log(jnp.array(testy)), c='green', label='val loss')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Abbacy  n. (pl. -ies) office or jurisdiction of an abbot or abbess. [latin: related to *abbot]\n",
    "\n",
    "Meep  n. \"\"\"\n",
    "temp = 0.4\n",
    "\n",
    "@jax.jit\n",
    "def inference(model_params : ModelParams, xBT, temp=0.5):\n",
    "  key = jrand.PRNGKey(0)\n",
    "  logits = forward(key, model_params, xBT)[0, :, :] # the first Batch\n",
    "  probs = jax.nn.softmax(logits / temp, axis=-1)\n",
    "  get_token = lambda channel: jrand.choice(jrand.PRNGKey(int(1000*time.time())), channel.shape[-1], p=channel)\n",
    "  ts = jax.vmap(get_token, in_axes=0, out_axes=0)(probs) # just take the first T in the first batch ig\n",
    "  return ts\n",
    "\n",
    "\n",
    "print(len(prompt))\n",
    "import time\n",
    "key = jrand.PRNGKey(int(100*time.time()))\n",
    "completion = []\n",
    "\n",
    "token_prompt = encode(prompt)\n",
    "\n",
    "print(decode(token_prompt), end='')\n",
    "for i in range(1000):\n",
    "  token_prompt = encode(prompt)\n",
    "  context = token_prompt[-block_size:]\n",
    "  xBT = jnp.array(context)[None, :] # fake batch of 1\n",
    "  next_token = decode([inference(model_params, xBT, temp=temp)[-1]])\n",
    "  print(next_token, end='')\n",
    "  prompt += next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "\n",
    "import pickle\n",
    "\n",
    "model_path = 'weights.pickle'\n",
    "\n",
    "with open(model_path, 'wb') as file:\n",
    "  pickle.dump(model_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "with open(model_path, 'rb') as file:\n",
    "  model_params_pickle = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
