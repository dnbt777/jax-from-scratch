{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import optax\n",
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "\n",
    "gpu_device = jax.device_get('gpu')[0]\n",
    "cpu_device = jax.device_get('cpu')[0]\n",
    "# LSTM\n",
    "# xs = B, input_size = B, T, C\n",
    "# h = c = y = B, output_size = B, T, logits_size = B, T, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 155\n",
      "dog [66 77 69] dog\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "with open('data/dnbt_posts.txt', 'r') as file:\n",
    "  dataset = file.read()\n",
    "\n",
    "# tokenize\n",
    "vocab = sorted(list(set(dataset)))\n",
    "print(\"vocab length:\", len(vocab))\n",
    "\n",
    "token_to_char = dict(enumerate(vocab))\n",
    "char_to_token = dict([(v, k) for k, v in token_to_char.items()])\n",
    "decode = lambda tokens: \"\".join([token_to_char[int(token)] for token in tokens])\n",
    "encode = lambda chars: jnp.array([char_to_token[c] for c in chars])\n",
    "\n",
    "print(\"dog\", encode(\"dog\"), decode(encode(\"dog\")))\n",
    "\n",
    "dataset_tokens = encode(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tanh() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 170\u001b[0m\n\u001b[1;32m    167\u001b[0m xtokens_batch \u001b[38;5;241m=\u001b[39m xtokens\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length)\n\u001b[1;32m    168\u001b[0m ytokens_batch \u001b[38;5;241m=\u001b[39m ytokens\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length) \u001b[38;5;66;03m# B, T where T = sequence_length\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m xembeds_batch \u001b[38;5;241m=\u001b[39m \u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtokens_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# train example\u001b[39;00m\n\u001b[1;32m    173\u001b[0m logits_batch \u001b[38;5;241m=\u001b[39m lstm_forward(xembeds_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], lstm_params, xembeds_batch)\n",
      "Cell \u001b[0;32mIn[164], line 123\u001b[0m, in \u001b[0;36membed\u001b[0;34m(xtokens, lstm_params, vocab_size)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(xtokens, lstm_params, vocab_size):\n\u001b[1;32m    122\u001b[0m   xs_one_hot \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mone_hot(xtokens, vocab_size, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#B, T, vocab_size\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m   xembeds \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs_one_hot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlstm_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwEM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlstm_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbEM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B, T, C\u001b[39;00m\n\u001b[1;32m    124\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m xembeds\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ai_gym/.venv/lib/python3.10/site-packages/jax/_src/linear_util.py:193\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    198\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    199\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "\u001b[0;31mTypeError\u001b[0m: tanh() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "lstm_blocks = 1 # thanks karpathy\n",
    "sequence_length = 50 # thanks karpathy\n",
    "model_size = 128*2 # thanks karpathy\n",
    "\n",
    "input_size = len(vocab) # just do one-hot for now\n",
    "hidden_size = model_size\n",
    "output_size = len(vocab) # logits => one-hot => tokens\n",
    "\n",
    "\n",
    "# init LSTM params\n",
    "def init_LSTM_params(key, lstm_blocks, input_size, model_size, output_size):\n",
    "  layers = 8\n",
    "  keys = random.split(key, layers*lstm_blocks + 2)\n",
    "  hxconcat_size = model_size + model_size\n",
    "  he = lambda rkey, shape: random.normal(rkey, shape=shape) * jnp.sqrt(2 / shape[0])\n",
    "  params = [\n",
    "    {\n",
    "      \"wU\" : he(keys[layers*i + 0], (hxconcat_size, model_size)),\n",
    "      \"bU\" : jnp.zeros((model_size,)),\n",
    "      \"wC\" : he(keys[layers*i + 6], (hxconcat_size, model_size)),\n",
    "      \"bC\" : jnp.zeros((model_size,)),\n",
    "      \"wF1\": he(keys[layers*i + 1], (hxconcat_size, model_size)),\n",
    "      \"bF1\": jnp.zeros((model_size,)),\n",
    "      \"wF2\": he(keys[layers*i + 2], (hxconcat_size, model_size)),\n",
    "      \"bF2\": jnp.zeros((model_size,)),\n",
    "      \"wO\" : he(keys[layers*i + 3], (hxconcat_size, model_size)),\n",
    "      \"bO\" : jnp.zeros((model_size,)),\n",
    "      # this is for the y layer, which i am probably imlementing wrong.\n",
    "      \"wY1\" : he(keys[layers*i + 4], (model_size, output_size)),\n",
    "      \"bY1\" : jnp.zeros((output_size,)),\n",
    "      \"wY2\" : he(keys[layers*i + 5], (output_size, output_size)),\n",
    "      \"bY2\" : jnp.zeros((output_size,)),\n",
    "    }\n",
    "    for i in range(lstm_blocks)\n",
    "  ]\n",
    "  params[0].update(\n",
    "    {\n",
    "    \"h0\" : random.normal(keys[layers*(layers - 1) + 0], shape=(model_size, )) * jnp.sqrt(2 / model_size),\n",
    "    \"c0\" : random.normal(keys[layers*(layers - 1) + 1], shape=(model_size, )) * jnp.sqrt(2 / model_size),\n",
    "    # then embedding table weight and bias\n",
    "    \"wEM\" : he(keys[layers*(layers - 1) + 2], (input_size, model_size)),\n",
    "    \"bEM\" : jnp.zeros((model_size,)),\n",
    "\n",
    "  })\n",
    "  return params\n",
    "\n",
    "\n",
    "\n",
    "def lstm_step(lstm_params, xs, h, c, i):\n",
    "  hxconcat = jax.lax.concatenate([h, xs[:, i]], dimension=1) #B, h ++ B, C => B, h+c\n",
    "  # update gate\n",
    "  update = jax.nn.sigmoid(hxconcat @ lstm_params[i][\"wU\"] + lstm_params[i][\"bU\"])\n",
    "  candidate = jax.nn.tanh(hxconcat @ lstm_params[i][\"wC\"] + lstm_params[i][\"bC\"])\n",
    "  c = c + update * candidate # (batch, c) => (batch, c)\n",
    "  # forget gate\n",
    "  forget1 = jax.nn.sigmoid(hxconcat @ lstm_params[i][\"wF1\"] + lstm_params[i][\"bF1\"])\n",
    "  forget2 = jax.nn.tanh(hxconcat @ lstm_params[i][\"wF2\"] + lstm_params[i][\"bF2\"])\n",
    "  forget = forget1 * forget2\n",
    "  c = c + forget # (batch, c) => (batch, c)\n",
    "\n",
    "  # output\n",
    "  o = jax.nn.sigmoid(hxconcat @ lstm_params[i][\"wO\"] + lstm_params[i][\"bO\"])  # B, model_size\n",
    "  h = jax.nn.tanh(c) * o # (B, model_size)\n",
    "  return h, c\n",
    "\n",
    "\n",
    "# LSTM forward\n",
    "import functools\n",
    "@functools.partial(jax.jit, static_argnames=['batches'])\n",
    "def lstm_forward(batches, lstm_params, xs):\n",
    "  logits_ts = []\n",
    "  lstm_block = 0\n",
    "  steps = 0\n",
    "  # initialize h and c as random/learnable params\n",
    "  h = jnp.tile(lstm_params[0][\"h0\"], (batches, 1))\n",
    "  c = jnp.tile(lstm_params[0][\"c0\"], (batches, 1))\n",
    "  T = xs.shape[1]\n",
    "  # take xs and pass each xt through the same SINGULAR block. don't update the weight layer. there is only one layer.\n",
    "  while steps < T:\n",
    "    # iterate through all LSTM blocks, and get the output from the final one\n",
    "    # send h and c from one block to the next?\n",
    "    while lstm_block < len(lstm_params):\n",
    "      hxconcat = jax.lax.concatenate([h, xs[:, steps]], dimension=1) #B, h ++ B, C => B, h+c\n",
    "      # update gate\n",
    "      update = jax.nn.sigmoid(hxconcat @ lstm_params[lstm_block][\"wU\"] + lstm_params[lstm_block][\"bU\"])\n",
    "      candidate = jax.nn.tanh(hxconcat @ lstm_params[lstm_block][\"wC\"] + lstm_params[lstm_block][\"bC\"])\n",
    "      c = c + update * candidate # (batch, c) => (batch, c)\n",
    "      # forget gate\n",
    "      forget1 = jax.nn.sigmoid(hxconcat @ lstm_params[lstm_block][\"wF1\"] + lstm_params[lstm_block][\"bF1\"])\n",
    "      forget2 = jax.nn.tanh(hxconcat @ lstm_params[lstm_block][\"wF2\"] + lstm_params[lstm_block][\"bF2\"])\n",
    "      forget = forget1 * forget2\n",
    "      c = c + forget # (batch, c) => (batch, c)\n",
    "\n",
    "      # output\n",
    "      o = jax.nn.sigmoid(hxconcat @ lstm_params[lstm_block][\"wO\"] + lstm_params[lstm_block][\"bO\"])  # B, model_size\n",
    "      h = jax.nn.tanh(c) * o # (B, model_size)\n",
    "\n",
    "      lstm_block += 1\n",
    "    \n",
    "    lstm_block = len(lstm_params) - 1\n",
    "    y = h @ lstm_params[lstm_block]['wY1'] + lstm_params[lstm_block][\"bY1\"]\n",
    "    #y = y @ lstm_params[i]['wY2'] + lstm_params[i][\"bY2\"]\n",
    "\n",
    "    logits_ts.append(y)\n",
    "    steps += 1\n",
    "  logits = jnp.transpose(jnp.array(logits_ts), axes=(1, 0, 2)) # T, B, C => B, T, C\n",
    "  return logits\n",
    "\n",
    "@jax.jit\n",
    "def loss(lstm_params, xs, ys):\n",
    "  batches = xs.shape[0] # B, T, C\n",
    "  logits = lstm_forward(batches, lstm_params, xs)\n",
    "  vocab_size = logits.shape[-1]\n",
    "  ys_one_hot = jax.nn.one_hot(ys, vocab_size, axis=-1)\n",
    "  logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "  crossentropylosses = -jnp.sum(ys_one_hot * logprobs, axis=-1)\n",
    "  crossentropyloss = jnp.mean(crossentropylosses)\n",
    "  return crossentropyloss\n",
    "\n",
    "\n",
    "def embed(xtokens, lstm_params, vocab_size):\n",
    "  xs_one_hot = jax.nn.one_hot(xtokens, vocab_size, axis=-1) #B, T, vocab_size\n",
    "  xembeds = jax.nn.tanh(xs_one_hot @ lstm_params[-1][\"wEM\"] + lstm_params[-1][\"bEM\"]) # B, T, C\n",
    "  return xembeds\n",
    "\n",
    "\n",
    "lr = 2e-3 # thanks karpathy\n",
    "lr_decay = 0.97 # thanks karpathy\n",
    "decay_after = 10 # thanks karpathy\n",
    "optimizer = optax.inject_hyperparams(optax.adam)(learning_rate=lr)\n",
    "\n",
    "# make optimizer a static arg in jit or it breaks\n",
    "@jax.jit\n",
    "def train(lstm_params, xs, ys, opt_state):\n",
    "  step_loss, grads = jax.value_and_grad(loss)(lstm_params, xs, ys)\n",
    "  param_updates, updated_opt_state = optimizer.update(grads, opt_state, lstm_params)\n",
    "  updated_lstm_params = optax.apply_updates(lstm_params, param_updates)\n",
    "  return updated_lstm_params, updated_opt_state, step_loss, grads\n",
    "\n",
    "\n",
    "# set up lstm params\n",
    "keys = random.split(random.PRNGKey(123), 20)\n",
    "lstm_params = init_LSTM_params(keys[0], lstm_blocks, input_size, model_size, output_size)\n",
    "opt_state = optimizer.init(lstm_params)\n",
    "\n",
    "# train\n",
    "# for now just overfit on small sample idk lol\n",
    "dataset_tokens = jnp.array(dataset_tokens)\n",
    "train_tokens = dataset_tokens[:int(len(dataset_tokens)*0.9)]\n",
    "test_tokens = dataset_tokens[int(len(dataset_tokens)*0.9):]\n",
    "\n",
    "train_batch_size = 50 # thanks karpathy\n",
    "val_batch_size = 50 # nx3\n",
    "\n",
    "epochs = 1000\n",
    "decay_every = 5\n",
    "for epoch in range(epochs):\n",
    "  if epoch > decay_after:\n",
    "    if epoch % decay_every == 0:\n",
    "      lr *= lr_decay\n",
    "      opt_state.hyperparams['learning_rate'] = lr\n",
    "  samples = (len(train_tokens) - 1) // sequence_length\n",
    "  for i in range(0, len(train_tokens) - 1 - sequence_length*train_batch_size, sequence_length*train_batch_size):\n",
    "    xtokens = train_tokens[i:i+sequence_length*train_batch_size]\n",
    "    ytokens = train_tokens[i+1:i+sequence_length*train_batch_size+1]\n",
    "\n",
    "    xtokens_batch = xtokens.reshape(-1, sequence_length)\n",
    "    ytokens_batch = ytokens.reshape(-1, sequence_length) # B, T where T = sequence_length\n",
    "\n",
    "    xembeds_batch = embed(xtokens_batch, lstm_params, len(vocab))\n",
    "\n",
    "    # train example\n",
    "    logits_batch = lstm_forward(xembeds_batch.shape[0], lstm_params, xembeds_batch)\n",
    "    prediction_batch = jnp.argmax(logits_batch, axis=-1)\n",
    "\n",
    "    # val batch\n",
    "    \n",
    "    j = i % ((len(test_tokens) - 1)//((val_batch_size)*sequence_length))\n",
    "    idx = j*val_batch_size*sequence_length\n",
    "    xtokens_val_batch = test_tokens[idx:idx+sequence_length*val_batch_size].reshape(-1, sequence_length) # batches of sequences lstm block count size\n",
    "    ytokens_val_batch = test_tokens[idx+1:idx+sequence_length*val_batch_size+1].reshape(-1, sequence_length)\n",
    "    xembeds_val_batch = jax.nn.one_hot(xtokens_val_batch, len(vocab), axis=-1)\n",
    "    \n",
    "    logits_val_batch = lstm_forward(xembeds_val_batch.shape[0], lstm_params, xembeds_val_batch)\n",
    "    prediction_val_batch = jnp.argmax(logits_val_batch, axis=-1)\n",
    "    ys_onehot = jax.nn.one_hot(ytokens_val_batch, len(vocab), axis=-1)\n",
    "    logprobs = jax.nn.log_softmax(logits_val_batch, axis=-1)\n",
    "    crossentropies = -jnp.sum(ys_onehot*logprobs,axis=-1)\n",
    "    val_loss = jnp.mean(crossentropies) #lmao\n",
    "    val_accuracy = jnp.mean(prediction_val_batch == ytokens_val_batch)\n",
    "\n",
    "    lstm_params, opt_state, step_loss, grads = train(lstm_params, xembeds_batch, ytokens_batch, opt_state)\n",
    "    x = decode(xtokens_batch[0]).replace('\\n', ' ')\n",
    "    y = decode(ytokens_batch[0]).replace('\\n', ' ')\n",
    "    yhat = decode(prediction_batch[0]).replace('\\n', ' ')\n",
    "    #print(epoch, epoch * samples + i, f\"{step_loss:1.4f}\", \"pred:\", x, \"=>\", y, \"?=\", yhat)\n",
    "    print(f'INPUT | \"{x}\"')\n",
    "    print(f'PRED  | \"{yhat}\"')\n",
    "    print(f\"step {(epoch, epoch * samples + i)} || loss: {step_loss:1.4f} || val_loss: {val_loss:1.4f} val_acc: {val_accuracy:1.4f} || LR = \", opt_state.hyperparams['learning_rate'] )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reply: @teodor_io @ludwigABAP are the visualicalls"
     ]
    }
   ],
   "source": [
    "def inference(key, chars):\n",
    "  xtokens = encode(chars)\n",
    "  xembed = jax.nn.one_hot(xtokens, len(vocab))[None, :] # artificial single batch\n",
    "  logits = lstm_forward(xembed.shape[0], lstm_params, xembed)[0][-1] # logits of the first B and last T in the B T C. should be (C,)\n",
    "  yhattokens = random.choice(key, a=logits.shape[0], p=jax.nn.softmax(logits)) # no need for axis=-1 since logits are (C,)\n",
    "  sequence = yhattokens\n",
    "  return sequence\n",
    "\n",
    "steps = 1000\n",
    "import time\n",
    "seed = int(time.time())\n",
    "keys = random.split(random.PRNGKey(seed), steps)\n",
    "temperature = 0.5\n",
    "text = 'reply: @teodor_io @lud'\n",
    "print(text.replace('\\n\\n', ''), end='')\n",
    "for i in range(steps):\n",
    "  yseq = inference(keys[i], text[-sequence_length:])\n",
    "  next_char = decode([yseq])[-1]\n",
    "  if next_char == '🛑':\n",
    "    break\n",
    "  text += next_char\n",
    "  print(next_char, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bC': Array(0.00382806, dtype=float32),\n",
       "  'bF1': Array(0.00067422, dtype=float32),\n",
       "  'bF2': Array(0.00351771, dtype=float32),\n",
       "  'bO': Array(0.00102159, dtype=float32),\n",
       "  'bU': Array(0.00061966, dtype=float32),\n",
       "  'bY1': Array(0., dtype=float32),\n",
       "  'bY2': Array(0., dtype=float32),\n",
       "  'c0': Array(0.00768698, dtype=float32),\n",
       "  'h0': Array(0.00609007, dtype=float32),\n",
       "  'wC': Array(0.00712224, dtype=float32),\n",
       "  'wF1': Array(0.00125441, dtype=float32),\n",
       "  'wF2': Array(0.00654482, dtype=float32),\n",
       "  'wO': Array(0.0019007, dtype=float32),\n",
       "  'wU': Array(0.0011529, dtype=float32),\n",
       "  'wY1': Array(0., dtype=float32),\n",
       "  'wY2': Array(0., dtype=float32)},\n",
       " {'bC': Array(0.00296637, dtype=float32),\n",
       "  'bF1': Array(0.00067843, dtype=float32),\n",
       "  'bF2': Array(0.00304542, dtype=float32),\n",
       "  'bO': Array(0.06423178, dtype=float32),\n",
       "  'bU': Array(0.00065218, dtype=float32),\n",
       "  'bY1': Array(0.08539081, dtype=float32),\n",
       "  'bY2': Array(0., dtype=float32),\n",
       "  'wC': Array(0.0122483, dtype=float32),\n",
       "  'wF1': Array(0.00278899, dtype=float32),\n",
       "  'wF2': Array(0.01255963, dtype=float32),\n",
       "  'wO': Array(0.31585217, dtype=float32),\n",
       "  'wU': Array(0.00270591, dtype=float32),\n",
       "  'wY1': Array(0.42604887, dtype=float32),\n",
       "  'wY2': Array(0., dtype=float32)}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(jnp.linalg.norm, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_PARAMS: 1183262\n",
      "DTYPE: float32\n",
      "TOTAL_MEGABYTES: 4.733048\n"
     ]
    }
   ],
   "source": [
    "getsize = lambda s: s.size\n",
    "sizes = jax.tree_util.tree_map(getsize, grads)\n",
    "total_params = 0\n",
    "for layer in sizes:\n",
    "  for _, v in layer.items():\n",
    "    total_params += v\n",
    "\n",
    "print(f\"TOTAL_PARAMS: {total_params}\")\n",
    "print(f\"DTYPE: {grads[0]['bC'].dtype}\")\n",
    "print(f\"TOTAL_MEGABYTES: {total_params*4/1_000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
